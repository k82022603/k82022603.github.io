---
title: "ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ ìœ„í•œ ì—ì´ì „íŠ¸ ìŠ¤í‚¬ ê°œë°œì ê°€ì´ë“œ"
date: 2025-12-25
categories: [AI,  Context Engineering]
mermaid: [True]
tags: [AI,  context-engineering,  agent-skills,  anthropic-skills,  AIOptimization,  prompt-engineering,  Claude.write]
---


## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì´ë€?](#ì»¨í…ìŠ¤íŠ¸-ì—”ì§€ë‹ˆì–´ë§ì´ë€)
3. [Agent-Skills vs Anthropic Skill-Creator ì‹¬ì¸µ ë¹„êµ](#agent-skills-vs-anthropic-skill-creator-ì‹¬ì¸µ-ë¹„êµ)
4. [Agent-Skills 7ê°€ì§€ í•µì‹¬ ìŠ¤í‚¬](#agent-skills-7ê°€ì§€-í•µì‹¬-ìŠ¤í‚¬)
5. [Context Engineering 4ëŒ€ ì „ëµ (LangChain)](#context-engineering-4ëŒ€-ì „ëµ-langchain)
6. [Context Rot ì™„ì „ ì •ë³µ](#context-rot-ì™„ì „-ì •ë³µ)
7. [ì‹¤ì „ êµ¬í˜„ ê°€ì´ë“œ](#ì‹¤ì „-êµ¬í˜„-ê°€ì´ë“œ)
8. [ì»¨í…ìŠ¤íŠ¸ ìµœì í™” íŒ¨í„´](#ì»¨í…ìŠ¤íŠ¸-ìµœì í™”-íŒ¨í„´)
9. [í”„ë¡œë•ì…˜ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤](#í”„ë¡œë•ì…˜-ë² ìŠ¤íŠ¸-í”„ë™í‹°ìŠ¤)
10. [ì°¸ê³  ìë£Œ](#ì°¸ê³ -ìë£Œ)

---

## ê°œìš”

### Skill-Creatorì™€ Agent-Skills

2025ë…„ 10ì›”, AI ê°œë°œì ì»¤ë®¤ë‹ˆí‹°ì—ì„œ "Agent-Skills"ì™€ "Anthropic Skill-Creator"ë¥¼ í˜¼ë™í•˜ëŠ” ì¼ì´ ë¹ˆë²ˆí•´ì¡ŒìŠµë‹ˆë‹¤. ì´ë¦„ì€ ë¹„ìŠ·í•˜ì§€ë§Œ, ì´ ë‘ ë„êµ¬ëŠ” ê·¼ë³¸ì ìœ¼ë¡œ ë‹¤ë¥¸ ëª©ì ê³¼ ì ‘ê·¼ ë°©ì‹ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

**Anthropic Skill-Creator**ëŠ” ê³µì‹ í”„ë ˆì„ì›Œí¬ë¡œ, Claude í”Œë«í¼ì—ì„œ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ìŠ¤í‚¬ì„ ìƒì„±í•˜ëŠ” ë©”íƒ€ ë„êµ¬ì…ë‹ˆë‹¤. SKILL.mdë¼ëŠ” êµ¬ì¡°í™”ëœ íŒŒì¼ í˜•ì‹ì„ í†µí•´ ë„êµ¬ì˜ ë™ì‘ ë°©ì‹ì„ ì •ì˜í•˜ê³ , Progressive Disclosure íŒ¨í„´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì‹œì‘ ì‹œì ì—ëŠ” ê°„ë‹¨í•œ ë©”íƒ€ë°ì´í„°ë§Œ ì»¨í…ìŠ¤íŠ¸ì— ë¡œë“œë˜ê³ (ê° ìŠ¤í‚¬ë‹¹ ìˆ˜ì‹­ í† í°), ì‹¤ì œë¡œ ìŠ¤í‚¬ì´ í•„ìš”í•  ë•Œë§Œ ì „ì²´ ë‚´ìš©(ìˆ˜ì²œ~ìˆ˜ë§Œ í† í°)ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. Anthropicì˜ docx, pptx, xlsx, pdf ìƒì„± ê¸°ëŠ¥ì´ ëª¨ë‘ ì´ ì‹œìŠ¤í…œìœ¼ë¡œ êµ¬í˜„ë˜ì–´ ìˆìœ¼ë©°, APIë¥¼ í†µí•´ í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ í†µí•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë°˜ë©´ **Agent-Skills for Context Engineering**ì€ Muratcan Koylanì´ ë§Œë“  ì»¤ë®¤ë‹ˆí‹° í”„ë¡œì íŠ¸ë¡œ, ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ì›ì¹™ê³¼ íŒ¨í„´ì„ ë‹´ì€ ì§€ì‹ ì €ì¥ì†Œì…ë‹ˆë‹¤. 7ê°€ì§€ í•µì‹¬ ìŠ¤í‚¬(context-fundamentals, context-degradation, context-optimization, multi-agent-patterns, memory-systems, tool-design, evaluation)ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, AI ì—ì´ì „íŠ¸ê°€ ìì‹ ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ” ë°©ë²•ì„ ê°€ë¥´ì¹©ë‹ˆë‹¤. í”Œë«í¼ ë…ë¦½ì ì´ì–´ì„œ Claudeë¿ë§Œ ì•„ë‹ˆë¼ Cursor, ì»¤ìŠ¤í…€ ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬ ë“± ì–´ë””ì„œë‚˜ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë¹„ìœ í•˜ìë©´, Skill-CreatorëŠ” "ì„ ë°˜ì„ ë§Œë“œëŠ” ê³µêµ¬"ì´ê³  Agent-SkillsëŠ” "ì‘ì—…ì¥ ì •ë¦¬ ë§¤ë‰´ì–¼"ì…ë‹ˆë‹¤. í•˜ë‚˜ëŠ” ë„êµ¬ë¥¼ ë§Œë“œëŠ” ë°©ë²•ì„ ì œê³µí•˜ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” íš¨ìœ¨ì ìœ¼ë¡œ ì¼í•˜ëŠ” ì›ì¹™ì„ ê°€ë¥´ì¹©ë‹ˆë‹¤. í”„ë¡œê·¸ë˜ë° ì„¸ê³„ë¡œ ì¹˜ë©´ Skill-CreatorëŠ” Python ì–¸ì–´ ìì²´, Agent-SkillsëŠ” ë””ìì¸ íŒ¨í„´ ì±…ì¸ ì…ˆì…ë‹ˆë‹¤.

ì‹¤ë¬´ì—ì„œëŠ” ë‘ ê°€ì§€ë¥¼ ì¡°í•©í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ íš¨ê³¼ì ì…ë‹ˆë‹¤. Agent-Skillsë¡œ ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì›ì¹™ì„ í•™ìŠµí•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ Skill-Creatorë¥¼ ì‚¬ìš©í•´ ì‹¤ì œ ì‘ë™í•˜ëŠ” ìŠ¤í‚¬ì„ êµ¬í˜„í•œ í›„, í”„ë¡œë•ì…˜ í™˜ê²½ì— ë°°í¬í•˜ëŠ” ì›Œí¬í”Œë¡œìš°ê°€ ê¶Œì¥ë©ë‹ˆë‹¤. ê¸°ì—…ì˜ ë¸Œëœë“œ ê°€ì´ë“œë¼ì¸ ìë™í™”ëŠ” Skill-Creatorë¡œ, RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ ìµœì í™”ëŠ” Agent-Skillsë¡œ ì ‘ê·¼í•˜ëŠ” ì‹ìœ¼ë¡œ ê°ê°ì˜ ê°•ì ì„ ì‚´ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ¯ ì´ ê°€ì´ë“œì˜ ëª©ì 

ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì€ AI ì—ì´ì „íŠ¸ì˜ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•´ **ë¬´ì—‡ì„ ë³´ì—¬ì£¼ê³  ë³´ì—¬ì£¼ì§€ ì•Šì„ì§€**ë¥¼ ì „ëµì ìœ¼ë¡œ ê²°ì •í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ ê°€ì´ë“œëŠ” 2025ë…„ 12ì›” ê¸°ì¤€ ìµœì‹  ì—°êµ¬ì™€ ì‹¤ì „ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ:

- GitHubì—ì„œ í™”ì œì¸ [Agent-Skills for Context Engineering](https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering)
- Chroma Researchì˜ Context Rot ì—°êµ¬ (2025)
- LangChainì˜ Context Engineering ì „ëµ
- Anthropicì˜ ê³µì‹ Agent Skills ì‹œìŠ¤í…œ

ì„ ì¢…í•©ì ìœ¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤.

### í•µì‹¬ í†µì°°

> "Context engineering is effectively the #1 job of engineers building AI agents."  
> â€” Cognition AI

í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ê³¼ ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì˜ ì°¨ì´:

| êµ¬ë¶„ | í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ | ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ |
|------|-------------------|-------------------|
| **ì •ì˜** | íš¨ê³¼ì ì¸ ì§€ì‹œì‚¬í•­ ì‘ì„± ê¸°ìˆ  | AIì—ê²Œ ì œê³µí•  ì •ë³´ì˜ ì„ íƒê³¼ êµ¬ì¡°í™” ê¸°ìˆ  |
| **ì´ˆì ** | "ì–´ë–»ê²Œ ìš”ì²­í•  ê²ƒì¸ê°€" (How to ask) | "ë¬´ì—‡ì„ ë³´ì—¬ì¤„ ê²ƒì¸ê°€" (What to show) |
| **ë²”ìœ„** | ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ìµœì í™” | ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬ |
| **ëª©í‘œ** | ëª…í™•í•œ ì˜ë„ ì „ë‹¬ | ìµœì ì˜ ì •ë³´ ë°€ë„ + ì„±ëŠ¥ ìœ ì§€ |
| **ì˜í–¥** | ì‘ë‹µ í’ˆì§ˆ | ì‘ë‹µ í’ˆì§ˆ + ì„±ëŠ¥ + ë¹„ìš© + ì‹ ë¢°ì„± |

---

## ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì´ë€?

### ğŸ“Š í•µì‹¬ ì›ë¦¬

Andrej Karpathyì˜ ë¹„ìœ : **"ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ëŠ” RAMê³¼ ê°™ë‹¤. ê¸°ì–µì´ ì•„ë‹ˆë¼ í˜„ì¬ ë³´ê³  ìˆëŠ” ê²ƒë§Œ ë‹´ëŠ”ë‹¤."**

ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì˜ 3ëŒ€ ì›ì¹™:

```
1. ì •ë³´ ì„ ë³„ì„± (Selectivity)
   â””â”€ ëª¨ë“  ì •ë³´ ì œê³µ âŒ
   â””â”€ ì‘ì—… í•„ìˆ˜ ì •ë³´ë§Œ ì œê³µ âœ…
   â””â”€ ë…¸ì´ì¦ˆ ì œê±°ë¡œ ì‹ í˜¸ ê°•í™”

2. ì •ë³´ ë°€ë„ ìµœì í™” (Density Optimization)
   â””â”€ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸° ê³ ë ¤
   â””â”€ í† í° íš¨ìœ¨ì„± ê·¹ëŒ€í™”
   â””â”€ ë¹„ìš©-ì„±ëŠ¥ íŠ¸ë ˆì´ë“œì˜¤í”„ ê´€ë¦¬

3. êµ¬ì¡°ì  ì¡°ì§í™” (Structural Organization)
   â””â”€ ê³„ì¸µì  ì •ë³´ êµ¬ì„±
   â””â”€ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ë°°ì¹˜
   â””â”€ Progressive Disclosure ì ìš©
```

### ì™œ ì¤‘ìš”í•œê°€?

#### ë¬¸ì œ ìƒí™©

**ì—°êµ¬ ê²°ê³¼ (Chroma 2025):**
- GPT-4.1, Claude 4, Gemini 2.5 ë“± 18ê°œ SOTA ëª¨ë¸ í‰ê°€
- ê°„ë‹¨í•œ ì‘ì—…(ë¬¸ìì—´ ë°˜ë³µ)ì—ì„œë„ ì»¨í…ìŠ¤íŠ¸ ì¦ê°€ì‹œ ì„±ëŠ¥ ì €í•˜
- 2,500-5,000 ë‹¨ì–´ ì´ìƒ: ê±°ë¶€/ì ˆë‹¨/í™˜ê° ì¦ê°€

```
ë¬¸ì œ:
âŒ ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ë°©ëŒ€ â†’ Context Rot (ì„±ëŠ¥ ì €í•˜)
âŒ ë¶ˆí•„ìš”í•œ ì •ë³´ í¬í•¨ â†’ ì£¼ì˜ ë¶„ì‚° (Attention Dilution)
âŒ êµ¬ì¡°í™” ë¶€ì¡± â†’ í•µì‹¬ ì •ë³´ ëˆ„ë½ (Lost in the Middle)
âŒ ê´€ë ¨ ì—†ëŠ” ì •ë³´ â†’ Context Poisoning (ì˜¤ì—¼)

ë¹„ìš© ì˜í–¥:
ğŸ’° 100K í† í° ì»¨í…ìŠ¤íŠ¸ = ë” ë†’ì€ API ë¹„ìš©
â±ï¸ ê¸´ ì»¨í…ìŠ¤íŠ¸ = ì‘ë‹µ ì§€ì—° ì¦ê°€
ğŸ“‰ ì„±ëŠ¥ ì €í•˜ = ì‚¬ìš©ì ì´íƒˆ
```

#### í•´ê²°ì±…

```
âœ… ì ì ˆí•œ ì •ë³´ëŸ‰ ì¡°ì ˆ (Right-Sizing)
âœ… ëª…í™•í•œ ì •ë³´ êµ¬ì¡°í™” (Structuring)
âœ… ë™ì  ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ (Dynamic Management)
âœ… ê³„ì¸µì  ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ (Hierarchical Memory)
```

---

## Agent-Skills vs Anthropic Skill-Creator ì‹¬ì¸µ ë¹„êµ

### ğŸ” ì™„ì „íˆ ë‹¤ë¥¸ ë„êµ¬ì…ë‹ˆë‹¤

ë§ì€ ì‚¬ëŒë“¤ì´ "ê°™ì€ ê²ƒ ì•„ë‹Œê°€?"ë¼ê³  ë¬¼ì–´ë³´ì§€ë§Œ, **í° í‹€ì—ì„œë§Œ ìœ ì‚¬í•  ë¿ ë³¸ì§ˆì ìœ¼ë¡œ ë‹¤ë¦…ë‹ˆë‹¤.**

### Anthropic Skill-Creator (ê³µì‹ ì‹œìŠ¤í…œ)

**ì¶œì²˜**: Anthropic ê³µì‹ ë¦¬í¬ì§€í† ë¦¬  
**ìœ„ì¹˜**: `anthropics/skills` GitHub, `/mnt/skills/examples/skill-creator/`

#### í•µì‹¬ íŠ¹ì§•

**1. ë©”íƒ€ ë„êµ¬ (Meta-Tool)**
- ìŠ¤í‚¬ì„ ìƒì„±í•˜ëŠ” ë„êµ¬
- SKILL.md êµ¬ì¡° í…œí”Œë¦¿ ì œê³µ
- Anthropic í”Œë«í¼ ë„¤ì´í‹°ë¸Œ í†µí•©

**2. Progressive Disclosure ì•„í‚¤í…ì²˜**

```
ì‹œì‘ ì‹œì :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ System Prompt                       â”‚
â”‚ + Skill Metadata (ê° ìŠ¤í‚¬ë‹¹ ìˆ˜ì‹­ í† í°) â”‚
â”‚ + User Message                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ìŠ¤í‚¬ íŠ¸ë¦¬ê±° í›„:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ System Prompt                       â”‚
â”‚ + Skill Metadata                    â”‚
â”‚ + SKILL.md ì „ì²´ ë‚´ìš© (ìˆ˜ì²œ~ìˆ˜ë§Œ í† í°) â”‚
â”‚ + í•„ìš”ì‹œ ì¶”ê°€ íŒŒì¼ë“¤                  â”‚
â”‚ + User Message                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**3. ì‚¬ìš© ë°©ë²•**

```yaml
# SKILL.md êµ¬ì¡°
---
name: Brand Guidelines
description: Apply Acme Corp brand guidelines to documents
version: 1.0.0
dependencies:
  - python-docx
  - Pillow
---

# Overview
[ìŠ¤í‚¬ ì„¤ëª…]

# Instructions
[êµ¬ì²´ì  ì§€ì‹œì‚¬í•­]

# Examples
[ì˜ˆì œë“¤]
```

**4. ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€**

Anthropicì˜ ë¬¸ì„œ ìƒì„± ê¸°ëŠ¥ì€ ëª¨ë‘ ìŠ¤í‚¬ ê¸°ë°˜:
- `docx` - Word ë¬¸ì„œ ìƒì„±/í¸ì§‘
- `pptx` - PowerPoint í”„ë ˆì  í…Œì´ì…˜
- `xlsx` - Excel ìŠ¤í”„ë ˆë“œì‹œíŠ¸
- `pdf` - PDF ìƒì„±/ì–‘ì‹ ì‘ì„±

**5. API í†µí•©**

```python
import anthropic

client = anthropic.Anthropic()

response = client.beta.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=4096,
    betas=["code-execution-2025-08-25", "skills-2025-10-02"],
    container={
        "skills": [
            {
                "type": "anthropic",
                "skill_id": "pptx",
                "version": "latest"
            }
        ]
    },
    messages=[{
        "role": "user",
        "content": "Create a presentation about renewable energy"
    }],
    tools=[{
        "type": "code_execution_20250825",
        "name": "code_execution"
    }]
)
```

### Agent-Skills for Context Engineering (ì»¤ë®¤ë‹ˆí‹° í”„ë¡œì íŠ¸)

**ì¶œì²˜**: Muratcan Koylanì˜ GitHub ë¦¬í¬ì§€í† ë¦¬  
**URL**: https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering

#### í•µì‹¬ íŠ¹ì§•

**1. ì§€ì‹ ê¸°ë°˜ (Knowledge Base)**
- ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì›ì¹™ê³¼ íŒ¨í„´ ëª¨ìŒ
- "ë©”íƒ€ ì—ì´ì „íŠ¸" ì§€ì‹ ì œê³µ
- AIê°€ ìì‹ ì˜ ì¸ì§€ ìì›ì„ ê´€ë¦¬í•˜ëŠ” ë°©ë²• êµìœ¡

**2. 7ê°€ì§€ í•µì‹¬ ìŠ¤í‚¬** (ë‹¤ìŒ ì„¹ì…˜ì—ì„œ ìƒì„¸ ì„¤ëª…)

```
1. context-fundamentals     â†’ ì»¨í…ìŠ¤íŠ¸ ê¸°ì´ˆ
2. context-degradation      â†’ ì„±ëŠ¥ ì €í•˜ ì´í•´
3. context-optimization     â†’ ìµœì í™” ê¸°ë²•
4. multi-agent-patterns     â†’ ë©€í‹° ì—ì´ì „íŠ¸ íŒ¨í„´
5. memory-systems          â†’ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ
6. tool-design             â†’ ë„êµ¬ ì„¤ê³„
7. evaluation              â†’ í‰ê°€ ë° ì¸¡ì •
```

**3. ë²”ìš©ì„±**

```markdown
í”Œë«í¼ ë…ë¦½ì :
âœ… Claude Code
âœ… Cursor
âœ… ì»¤ìŠ¤í…€ ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬
âœ… ëª¨ë“  ìŠ¤í‚¬ ì§€ì› í”Œë«í¼

ì‹¤ì œë¡œ í…ŒìŠ¤íŠ¸:
$ cat context-fundamentals/SKILL.md | 
  your-llm-tool --prompt "ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬ ë°©ë²• ì„¤ëª…"
```

**4. ì‹¤ì „ ì¤‘ì‹¬ êµ¬ì„±**

ê° ìŠ¤í‚¬ì€ ë‹¤ìŒì„ í¬í•¨:
- ì´ë¡ ì  ë°°ê²½ (Why)
- ì‹¤ì „ íŒ¨í„´ (How)
- ì½”ë“œ ì˜ˆì œ (What)
- ì—°êµ¬ ì°¸ì¡° (Evidence)

### í•µì‹¬ ì°¨ì´ì  ìš”ì•½í‘œ

| ì¸¡ë©´ | Skill-Creator (Anthropic) | Agent-Skills (Community) |
|------|--------------------------|-------------------------|
| **ì„±ê²©** | ë„êµ¬ ìƒì„± í”„ë ˆì„ì›Œí¬ | ì‹¤ì „ ì§€ì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ |
| **ì¶”ìƒí™”** | ë©”íƒ€ ë ˆë²¨ (ìŠ¤í‚¬ ë§Œë“œëŠ” ë²•) | êµ¬í˜„ ë ˆë²¨ (ì‚¬ìš© ê°€ëŠ¥í•œ ì§€ì‹) |
| **ì£¼ìš” ê¸°ëŠ¥** | - SKILL.md í…œí”Œë¦¿ ì œê³µ<br>- ìŠ¤í‚¬ êµ¬ì¡°í™” ê°€ì´ë“œ<br>- API í†µí•© | - ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ì›ì¹™<br>- ì„±ëŠ¥ ìµœì í™” íŒ¨í„´<br>- ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì„¤ê³„ |
| **ì‚¬ìš© ë°©ë²•** | ìŠ¤í‚¬ íŒŒì¼ ìƒì„± â†’ ë“±ë¡ â†’ ìë™ ë¡œë“œ | ì§€ì‹ ìŠµë“ â†’ ì›ì¹™ ì ìš© â†’ ì»¤ìŠ¤í…€ êµ¬í˜„ |
| **í†µí•©** | Claude í”Œë«í¼ ë„¤ì´í‹°ë¸Œ | í”Œë«í¼ ë…ë¦½ì  |
| **í•™ìŠµ ê³¡ì„ ** | í”„ë ˆì„ì›Œí¬ êµ¬ì¡° ì´í•´ í•„ìš” | ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ì›ì¹™ |
| **ë²„ì „ ê´€ë¦¬** | API ë ˆë²¨ ë²„ì €ë‹ ì§€ì› | Git ê¸°ë°˜ ë²„ì „ ê´€ë¦¬ |
| **ë³´ì•ˆ** | ìƒŒë“œë°•ìŠ¤ ì‹¤í–‰ í™˜ê²½ | ì§€ì‹ ì „ë‹¬ (ì½”ë“œ ì‹¤í–‰ ì—†ìŒ) |

### ğŸ”§ ë¹„ìœ ë¡œ ì´í•´í•˜ê¸°

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ê³µêµ¬ ì œì‘ ë„êµ¬ vs ê³µêµ¬ ì‚¬ìš© ë§¤ë‰´ì–¼

Skill-Creator        Agent-Skills
     â†“                    â†“
  ì„ ë°˜, ë°€ë§           ì‘ì—… ì§€ì¹¨ì„œ
 (ë„êµ¬ ë§Œë“œëŠ” ê¸°ê³„)    (ë„êµ¬ ì‚¬ìš©ë²•)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

í”„ë¡œê·¸ë˜ë° ì–¸ì–´ vs ë””ìì¸ íŒ¨í„´

Skill-Creator        Agent-Skills
     â†“                    â†“
  Python ì–¸ì–´          GoF íŒ¨í„´
  (ë¬¸ë²•, êµ¬ì¡°)         (ì›ì¹™, íŒ¨í„´)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ChatGPTì™€ Geminiì˜ ê´€ê³„ì²˜ëŸ¼:
- í° ë²”ì£¼: ë‘˜ ë‹¤ "AI ìŠ¤í‚¬ ê´€ë ¨"
- ì„¸ë¶€ ê¸°ëŠ¥: ì™„ì „íˆ ë‹¤ë¥¸ ëª©ì ê³¼ ì‚¬ìš©ë²•
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### ì‹¤ë¬´ í™œìš© ì „ëµ

#### ì‹œë‚˜ë¦¬ì˜¤ 1: ê¸°ì—… ë‚´ë¶€ ì›Œí¬í”Œë¡œìš° ìë™í™”

```
ìƒí™©: ë¸Œëœë“œ ê°€ì´ë“œë¼ì¸ì— ë§ëŠ” ë¬¸ì„œ ìë™ ìƒì„±

âœ… Skill-Creator ì‚¬ìš©:
1. brand-guidelines.md ìŠ¤í‚¬ ìƒì„±
2. íšŒì‚¬ ë¡œê³ , ì»¬ëŸ¬, í°íŠ¸ í¬í•¨
3. Claudeì—ê²Œ ë“±ë¡
4. "Q3 ë¦¬í¬íŠ¸ ë§Œë“¤ì–´ì¤˜" â†’ ìë™ìœ¼ë¡œ ìŠ¤í‚¬ ì ìš©

âœ… Agent-Skills í™œìš©:
1. context-fundamentals í•™ìŠµ
2. ë¸Œëœë“œ ê°€ì´ë“œë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨í•˜ëŠ” ë°©ë²• ì´í•´
3. context-optimization íŒ¨í„´ ì ìš©
4. í† í° ì‚¬ìš©ëŸ‰ ìµœì†Œí™”í•˜ë©´ì„œ ì¼ê´€ì„± ìœ ì§€
```

#### ì‹œë‚˜ë¦¬ì˜¤ 2: RAG ì‹œìŠ¤í…œ ìµœì í™”

```
ìƒí™©: ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì„±ëŠ¥ ê°œì„ 

âœ… Skill-Creator:
- ì§ì ‘ì  í•´ê²°ì±… ì—†ìŒ (ë„êµ¬ ìƒì„± í”„ë ˆì„ì›Œí¬)

âœ… Agent-Skills:
1. context-degradation ìŠ¤í‚¬ë¡œ ë¬¸ì œ ì§„ë‹¨
2. context-optimizationìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ ì••ì¶•
3. memory-systemsë¡œ ê³„ì¸µì  ê²€ìƒ‰ êµ¬í˜„
4. evaluationìœ¼ë¡œ ì„±ëŠ¥ ì¸¡ì •
```

### ğŸ¯ ì„ íƒ ê°€ì´ë“œ

**Skill-Creatorë¥¼ ì„ íƒí•´ì•¼ í•  ë•Œ:**
- Claude í”Œë«í¼ì—ì„œ ë°˜ë³µì  ì‘ì—… ìë™í™”
- ë¬¸ì„œ ìƒì„±/í¸ì§‘ ê¸°ëŠ¥ í•„ìš”
- APIë¥¼ í†µí•œ ìŠ¤í‚¬ ê´€ë¦¬ í•„ìš”
- íŒ€ ì „ì²´ ìŠ¤í‚¬ ë°°í¬ í•„ìš”

**Agent-Skillsë¥¼ í™œìš©í•´ì•¼ í•  ë•Œ:**
- ì»¨í…ìŠ¤íŠ¸ ì„±ëŠ¥ ë¬¸ì œ í•´ê²°
- ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì„¤ê³„
- í”Œë«í¼ ë…ë¦½ì  ì†”ë£¨ì…˜ í•„ìš”
- ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì›ì¹™ í•™ìŠµ
- RAG/ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ìµœì í™”

**ë‘ ê°€ì§€ ëª¨ë‘ í™œìš©:**
```
Agent-Skillsë¡œ ì›ì¹™ í•™ìŠµ
      â†“
Skill-Creatorë¡œ êµ¬í˜„
      â†“
í”„ë¡œë•ì…˜ ë°°í¬
```

---

## Agent-Skills 7ê°€ì§€ í•µì‹¬ ìŠ¤í‚¬

GitHub ë¦¬í¬ì§€í† ë¦¬ì˜ `skills/` ë””ë ‰í† ë¦¬ êµ¬ì¡°:

```
skills/
â”œâ”€â”€ context-fundamentals/      # ì»¨í…ìŠ¤íŠ¸ ê¸°ì´ˆ
â”œâ”€â”€ context-degradation/       # ì„±ëŠ¥ ì €í•˜
â”œâ”€â”€ context-optimization/      # ìµœì í™”
â”œâ”€â”€ multi-agent-patterns/      # ë©€í‹° ì—ì´ì „íŠ¸
â”œâ”€â”€ memory-systems/           # ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ
â”œâ”€â”€ tool-design/              # ë„êµ¬ ì„¤ê³„
â””â”€â”€ evaluation/               # í‰ê°€
```

### 1ï¸âƒ£ context-fundamentals (ì»¨í…ìŠ¤íŠ¸ ê¸°ì´ˆ)

**ì–¸ì œ ì‚¬ìš©**: ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜ ì„¤ê³„, ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ ì‹¤íŒ¨ ë””ë²„ê¹…, ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© ìµœì í™”

#### í•µì‹¬ ê°œë…

**ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ìš”ì†Œ:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Complete Context Window            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. System Prompts                           â”‚
â”‚    - ì—ì´ì „íŠ¸ ì •ì²´ì„± ì •ì˜                      â”‚
â”‚    - í–‰ë™ ì œì•½ì‚¬í•­                            â”‚
â”‚    - ì„¸ì…˜ ì‹œì‘ì‹œ 1íšŒ ë¡œë“œ                     â”‚
â”‚                                             â”‚
â”‚ 2. Tool Definitions                         â”‚
â”‚    - ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ì„¤ëª…                     â”‚
â”‚    - íŒŒë¼ë¯¸í„° ìŠ¤í‚¤ë§ˆ                          â”‚
â”‚    - ì‹¤í–‰ ê°€ì´ë“œë¼ì¸                          â”‚
â”‚                                             â”‚
â”‚ 3. Retrieved Documents                      â”‚
â”‚    - RAGë¡œ ê°€ì ¸ì˜¨ ë¬¸ì„œ                        â”‚
â”‚    - ê´€ë ¨ì„± ì ìˆ˜ ê¸°ë°˜ í•„í„°ë§                   â”‚
â”‚    - ë™ì  ë¡œë“œ/ì–¸ë¡œë“œ                         â”‚
â”‚                                             â”‚
â”‚ 4. Message History                          â”‚
â”‚    - ëŒ€í™” ì´ë ¥                               â”‚
â”‚    - ìµœê·¼ Nê°œ í„´ ìœ ì§€                         â”‚
â”‚    - ìš”ì•½ or ì••ì¶•                            â”‚
â”‚                                             â”‚
â”‚ 5. Tool Outputs                             â”‚
â”‚    - ë„êµ¬ ì‹¤í–‰ ê²°ê³¼                           â”‚
â”‚    - ì—ëŸ¬ ë©”ì‹œì§€                             â”‚
â”‚    - ìƒíƒœ ì •ë³´                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**System Prompt ìµœì í™”:**

```markdown
âŒ ì˜ëª»ëœ ì˜ˆ (ë„ˆë¬´ ë†’ì€ ìˆ˜ì¤€):
"Be helpful and answer questions accurately."

âŒ ì˜ëª»ëœ ì˜ˆ (ë„ˆë¬´ ë‚®ì€ ìˆ˜ì¤€):
"When user asks about weather:
  1. Call get_weather(city, date)
  2. If error code 404, say 'City not found'
  3. If error code 500, retry 3 times
  ... (50ì¤„ì˜ ì„¸ë¶€ ë¡œì§)"

âœ… ì˜¬ë°”ë¥¸ ì˜ˆ (ì ì ˆí•œ ìˆ˜ì¤€):
"""
You are a data analysis assistant for financial teams.

Core Capabilities:
- Extract insights from financial reports
- Generate visualizations using provided tools
- Explain calculations in business terms

Guidelines:
- Always verify data sources before analysis
- Use clear, jargon-free language for non-technical users
- When uncertain, acknowledge limitations

Tools Available:
- spreadsheet_analyzer: For Excel/CSV processing
- chart_generator: For data visualization
- calculation_engine: For financial metrics
"""
```

**Progressive Disclosure ì›ì¹™:**

```python
# ë‚˜ìœ ë°©ë²•: ëª¨ë“  ë¬¸ì„œë¥¼ í•œë²ˆì— ë¡œë“œ
def bad_approach(query, all_documents):
    context = "\n".join(all_documents)  # ìˆ˜ë°± MB
    return f"Query: {query}\n\nDocuments:\n{context}"

# ì¢‹ì€ ë°©ë²•: í•„ìš”í•œ ì •ë³´ë§Œ ì ì§„ì  ë¡œë“œ
def good_approach(query, all_documents):
    # 1ë‹¨ê³„: ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ 5ê°œë§Œ ë¡œë“œ
    relevant = semantic_search(query, all_documents, top_k=5)
    
    # 2ë‹¨ê³„: ìš”ì•½ë³¸ ë¨¼ì € ì œê³µ
    summaries = [summarize(doc, max_tokens=200) for doc in relevant]
    
    # 3ë‹¨ê³„: í•„ìš”ì‹œ ì „ì²´ ë‚´ìš© ë¡œë“œ
    # (ì—ì´ì „íŠ¸ê°€ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­í•  ë•Œë§Œ)
    
    return {
        "summaries": summaries,
        "full_docs_available": [doc.id for doc in relevant]
    }
```

#### ì‹¤ì „ ì ìš©

```python
class ContextFundamentalsManager:
    """ì»¨í…ìŠ¤íŠ¸ ê¸°ì´ˆ ì›ì¹™ì„ ì ìš©í•œ ê´€ë¦¬ì"""
    
    def __init__(self, max_context_tokens=128000):
        self.max_tokens = max_context_tokens
        self.components = {
            'system_prompt': {'tokens': 0, 'content': '', 'priority': 10},
            'tool_definitions': {'tokens': 0, 'content': '', 'priority': 9},
            'recent_messages': {'tokens': 0, 'content': [], 'priority': 8},
            'retrieved_docs': {'tokens': 0, 'content': [], 'priority': 6},
            'tool_outputs': {'tokens': 0, 'content': [], 'priority': 7},
            'background_info': {'tokens': 0, 'content': '', 'priority': 3}
        }
    
    def build_context(self):
        """ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_components = sorted(
            self.components.items(),
            key=lambda x: x[1]['priority'],
            reverse=True
        )
        
        context_parts = []
        used_tokens = 0
        
        for name, component in sorted_components:
            if used_tokens + component['tokens'] > self.max_tokens:
                # ë‚¨ì€ ê³µê°„ì— ë§ê²Œ ì˜ë¼ë‚´ê¸°
                remaining = self.max_tokens - used_tokens
                truncated = self._truncate(component['content'], remaining)
                context_parts.append(truncated)
                break
            
            context_parts.append(component['content'])
            used_tokens += component['tokens']
        
        return '\n\n'.join(context_parts)
```

---

### 2ï¸âƒ£ context-degradation (ì»¨í…ìŠ¤íŠ¸ ì„±ëŠ¥ ì €í•˜)

**ì–¸ì œ ì‚¬ìš©**: ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ ì‹¤íŒ¨ ì§„ë‹¨, ì„±ëŠ¥ ì €í•˜ ë¬¸ì œ í•´ê²°

#### í•µì‹¬ ì‹¤íŒ¨ íŒ¨í„´

**1. Lost in the Middle (ì¤‘ê°„ ì •ë³´ ì†ì‹¤)**

```
ì‹¤í—˜ ê²°ê³¼ (Stanford, 2023):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 20ê°œ ë¬¸ì„œ, ì´ 4,000 í† í°               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ìœ„ì¹˜  1: ì •í™•ë„ 70-75% âœ…              â”‚
â”‚ ìœ„ì¹˜ 10: ì •í™•ë„ 55-60% âš ï¸              â”‚
â”‚ ìœ„ì¹˜ 20: ì •í™•ë„ 70-75% âœ…              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Uì í˜•íƒœ ì£¼ì˜ ê³¡ì„ :
   ë†’ìŒ â–²               
        â”‚ â–ˆ            â–ˆ
        â”‚ â–ˆ            â–ˆ
        â”‚ â–ˆ      â–„     â–ˆ
   ë‚®ìŒ â”‚ â–ˆ  â–„â–„â–„â–ˆ â–ˆâ–„â–„  â–ˆ
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶
          ì‹œì‘  ì¤‘ê°„   ë
```

**í•´ê²°ì±…:**

```python
def place_critical_info_strategically(documents, critical_info):
    """í•µì‹¬ ì •ë³´ë¥¼ ì‹œì‘ì´ë‚˜ ëì— ë°°ì¹˜"""
    return [
        critical_info,          # ì‹œì‘ì— ë°°ì¹˜
        *documents[:-1],        # ì¤‘ê°„ ë¬¸ì„œë“¤
        documents[-1],          # ë§ˆì§€ë§‰ ë¬¸ì„œ (ë‘ ë²ˆì§¸ë¡œ ì¤‘ìš”í•œ ì •ë³´)
    ]
```

**2. Context Poisoning (ì»¨í…ìŠ¤íŠ¸ ì˜¤ì—¼)**

```python
# ì˜ëª»ëœ ì •ë³´ê°€ ì»¨í…ìŠ¤íŠ¸ì— ë“¤ì–´ê°€ë©´ ì—°ì‡„ ì‹¤íŒ¨ ë°œìƒ

def prevent_context_poisoning():
    """
    ì»¨í…ìŠ¤íŠ¸ ì˜¤ì—¼ ë°©ì§€ ì „ëµ:
    1. í™˜ê° ê°ì§€ ë° ì œê±°
    2. ì‚¬ì‹¤ ê²€ì¦
    3. ì¶œì²˜ ì¶”ì 
    """
    
    # ì˜ˆì‹œ: ì´ì „ ì‘ë‹µ ê²€ì¦
    if response_contains_hallucination(prev_response):
        # ì˜¤ì—¼ëœ ì‘ë‹µì„ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì œê±°
        context.remove(prev_response)
        # ìƒˆë¡œìš´ ê²€ì¦ëœ ì •ë³´ë¡œ êµì²´
        context.add(verified_information)
```

**3. Context Confusion (ì»¨í…ìŠ¤íŠ¸ í˜¼ë€)**

```
ë¬¸ì œ: ìœ ì‚¬í•˜ì§€ë§Œ ê´€ë ¨ ì—†ëŠ” ì •ë³´ê°€ ë§ì„ ë•Œ

ì˜ˆì‹œ:
Query: "Python ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°"

Contextì— í¬í•¨ëœ ë¬¸ì„œ:
- Python ê¸°ì´ˆ ë¬¸ë²• âš ï¸ (ìœ ì‚¬í•˜ì§€ë§Œ ê´€ë ¨ì„± ë‚®ìŒ)
- JavaScript async/await âš ï¸ (ë¹„ë™ê¸°ì§€ë§Œ ë‹¤ë¥¸ ì–¸ì–´)
- Python asyncio ë¬¸ì„œ âœ… (ì •í™•íˆ ê´€ë ¨ë¨)

â†’ ëª¨ë¸ì´ í˜¼ë€ìŠ¤ëŸ¬ì›Œ Pythonê³¼ JavaScript ì„ì–´ì„œ ì„¤ëª…
```

**í•´ê²°ì±…: ì˜ë¯¸ë¡ ì  ê±°ë¦¬ ê³„ì‚°**

```python
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def filter_confusing_context(query, documents, threshold=0.5):
    """ìœ ì‚¬ë„ ê¸°ë°˜ í˜¼ë€ ë°©ì§€ í•„í„°ë§"""
    
    # ì„ë² ë”© ìƒì„±
    query_embedding = get_embedding(query)
    doc_embeddings = [get_embedding(doc) for doc in documents]
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarities = cosine_similarity(
        [query_embedding],
        doc_embeddings
    )[0]
    
    # ë„ˆë¬´ ìœ ì‚¬í•œ (ì¤‘ë³µ) ë¬¸ì„œ ì œê±°
    # ë„ˆë¬´ ë‹¤ë¥¸ (ê´€ë ¨ ì—†ëŠ”) ë¬¸ì„œ ì œê±°
    filtered = []
    for doc, sim in zip(documents, similarities):
        if threshold < sim < 0.95:  # ì ì ˆí•œ ë²”ìœ„ë§Œ
            filtered.append(doc)
    
    return filtered
```

**4. Attention Scarcity (ì£¼ì˜ë ¥ ë¶€ì¡±)**

```
ì»¨í…ìŠ¤íŠ¸ í¬ê¸°ì™€ ì£¼ì˜ë ¥ ê´€ê³„:

1K í† í°:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% ì£¼ì˜ë ¥
10K í† í°:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          60% ì£¼ì˜ë ¥
100K í† í°: â–ˆâ–ˆâ–ˆâ–ˆ                  20% ì£¼ì˜ë ¥
1M í† í°:   â–ˆ                      5% ì£¼ì˜ë ¥

â†’ í° ì»¨í…ìŠ¤íŠ¸ = ê° í† í°ì— ëŒ€í•œ ì£¼ì˜ë ¥ ê°ì†Œ
```

---

### 3ï¸âƒ£ context-optimization (ì»¨í…ìŠ¤íŠ¸ ìµœì í™”)

**ì–¸ì œ ì‚¬ìš©**: í† í° ì‚¬ìš©ëŸ‰ ê°ì†Œ, ì‘ë‹µ ì†ë„ ê°œì„ , ë¹„ìš© ì ˆê°

#### 3ê°€ì§€ í•µì‹¬ ê¸°ë²•

**1. Compaction (ì••ì¶•)**

```python
class ContextCompactor:
    """ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ê¸°ë²•"""
    
    def extractive_summarization(self, text, ratio=0.3):
        """í•µì‹¬ ë¬¸ì¥ë§Œ ì¶”ì¶œ"""
        sentences = sent_tokenize(text)
        
        # TF-IDFë¡œ ì¤‘ìš”ë„ ê³„ì‚°
        vectorizer = TfidfVectorizer()
        tfidf = vectorizer.fit_transform(sentences)
        
        # ìƒìœ„ N% ë¬¸ì¥ ì„ íƒ
        scores = tfidf.sum(axis=1).A1
        top_indices = np.argsort(scores)[-int(len(sentences) * ratio):]
        
        # ì›ë˜ ìˆœì„œëŒ€ë¡œ ì¬ì¡°ë¦½
        top_indices.sort()
        return ' '.join([sentences[i] for i in top_indices])
    
    def abstractive_summarization(self, text, max_length=200):
        """LLM ê¸°ë°˜ ìš”ì•½"""
        prompt = f"""
        ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {max_length} ë‹¨ì–´ ì´ë‚´ë¡œ ìš”ì•½:
        
        {text}
        
        í•µì‹¬ë§Œ í¬í•¨í•˜ê³  ì˜ˆì‹œëŠ” ì œì™¸í•˜ì„¸ìš”.
        """
        return call_llm(prompt)
    
    def chunk_and_compress(self, long_document, chunk_size=2000):
        """ì²­í¬ ë‹¨ìœ„ë¡œ ì••ì¶• í›„ ë³‘í•©"""
        chunks = split_into_chunks(long_document, chunk_size)
        summaries = [self.extractive_summarization(c) for c in chunks]
        
        # ìš”ì•½ë“¤ì„ ë‹¤ì‹œ ìš”ì•½ (ê³„ì¸µì  ì••ì¶•)
        combined = '\n'.join(summaries)
        if len(combined.split()) > 1000:
            return self.abstractive_summarization(combined)
        return combined
```

**2. Masking (ë§ˆìŠ¤í‚¹)**

```python
def mask_irrelevant_information(context, query):
    """
    ì¿¼ë¦¬ì™€ ê´€ë ¨ ì—†ëŠ” ì •ë³´ë¥¼ ë§ˆìŠ¤í‚¹
    (ì œê±° ëŒ€ì‹  [REDACTED]ë¡œ í‘œì‹œ)
    """
    
    doc_sections = split_into_sections(context)
    query_keywords = extract_keywords(query)
    
    masked_sections = []
    for section in doc_sections:
        section_keywords = extract_keywords(section)
        overlap = len(query_keywords & section_keywords)
        
        if overlap == 0:
            # ê´€ë ¨ì„± ì—†ìŒ â†’ ë§ˆìŠ¤í‚¹
            masked_sections.append("[SECTION OMITTED - Not relevant to query]")
        else:
            # ê´€ë ¨ì„± ìˆìŒ â†’ ìœ ì§€
            masked_sections.append(section)
    
    return '\n\n'.join(masked_sections)
```

**3. Caching (ìºì‹±)**

```python
import hashlib
import time

class SmartContextCache:
    """Anthropicì˜ Prompt Caching í™œìš©"""
    
    def __init__(self):
        self.cache = {}
        self.cache_ttl = 300  # 5ë¶„
    
    def get_cache_key(self, content):
        """ì»¨í…ì¸  í•´ì‹œë¡œ ìºì‹œ í‚¤ ìƒì„±"""
        return hashlib.sha256(content.encode()).hexdigest()
    
    def should_cache(self, content):
        """ìºì‹± ê°€ì¹˜ íŒë‹¨"""
        # 1. ì¶©ë¶„íˆ í° ì»¨í…ì¸  (1024 í† í° ì´ìƒ)
        if count_tokens(content) < 1024:
            return False
        
        # 2. ì¬ì‚¬ìš© ê°€ëŠ¥ì„± ë†’ì€ ì»¨í…ì¸ 
        # (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸, ë„êµ¬ ì •ì˜, ìì£¼ ì“°ëŠ” ë¬¸ì„œ ë“±)
        if is_static_content(content):
            return True
        
        return False
    
    def build_cached_context(self, static_parts, dynamic_parts):
        """
        Anthropic Prompt Caching í™œìš©:
        - static_parts: ìºì‹œë¨ (90% í• ì¸)
        - dynamic_parts: ë§¤ë²ˆ ì²˜ë¦¬
        """
        
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": static_parts['system_prompt'],
                        "cache_control": {"type": "ephemeral"}
                    },
                    {
                        "type": "text",
                        "text": static_parts['tool_definitions'],
                        "cache_control": {"type": "ephemeral"}
                    },
                    {
                        "type": "text",
                        "text": dynamic_parts['user_query']
                    }
                ]
            }
        ]
        
        return messages
```

#### ì‹¤ì „ ìµœì í™” íŒŒì´í”„ë¼ì¸

```python
class OptimizationPipeline:
    """ì „ì²´ ìµœì í™” íŒŒì´í”„ë¼ì¸"""
    
    def optimize(self, query, documents, max_tokens=10000):
        """
        ë‹¨ê³„ë³„ ìµœì í™”:
        1. ê´€ë ¨ì„± í•„í„°ë§
        2. ì»¨í…ìŠ¤íŠ¸ ì••ì¶•
        3. í† í° ì œí•œ ì ìš©
        4. ìºì‹± ì „ëµ
        """
        
        # Phase 1: ê´€ë ¨ì„± í•„í„° (50% ê°ì†Œ)
        relevant_docs = self.relevance_filter(query, documents)
        print(f"Phase 1: {len(documents)} â†’ {len(relevant_docs)} docs")
        
        # Phase 2: ì••ì¶• (70% ê°ì†Œ)
        compressed_docs = [
            self.compactor.extractive_summarization(doc, ratio=0.3)
            for doc in relevant_docs
        ]
        print(f"Phase 2: Compressed to 30% of original size")
        
        # Phase 3: í† í° ì œí•œ
        final_context = self.fit_to_budget(
            query, compressed_docs, max_tokens
        )
        
        # Phase 4: ìºì‹±
        if self.should_cache(final_context):
            cache_key = self.cache.get_cache_key(final_context)
            self.cache.set(cache_key, final_context)
        
        return final_context
```

---

### 4ï¸âƒ£ multi-agent-patterns (ë©€í‹° ì—ì´ì „íŠ¸ íŒ¨í„´)

**ì–¸ì œ ì‚¬ìš©**: ë³µì¡í•œ ì‘ì—…ì„ ì—¬ëŸ¬ ì—ì´ì „íŠ¸ë¡œ ë¶„ì‚°, ì „ë¬¸í™”ëœ ì—ì´ì „íŠ¸ í™œìš©

#### 3ê°€ì§€ í•µì‹¬ íŒ¨í„´

**1. Orchestrator Pattern (ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°)**

```python
class OrchestratorAgent:
    """ì¤‘ì•™ í†µì œ ì—ì´ì „íŠ¸ê°€ ì‘ì—… ë¶„ë°°"""
    
    def __init__(self):
        self.specialist_agents = {
            'researcher': ResearchAgent(),
            'coder': CodingAgent(),
            'reviewer': ReviewAgent(),
            'writer': WritingAgent()
        }
    
    def execute_task(self, task):
        """ì‘ì—…ì„ ë¶„ì„í•˜ê³  ì ì ˆí•œ ì—ì´ì „íŠ¸ì— í• ë‹¹"""
        
        # ì‘ì—… ë¶„ì„
        task_plan = self.analyze_task(task)
        
        results = []
        for subtask in task_plan['subtasks']:
            # ì ì ˆí•œ ì „ë¬¸ ì—ì´ì „íŠ¸ ì„ íƒ
            agent_type = subtask['agent_type']
            agent = self.specialist_agents[agent_type]
            
            # ìµœì†Œí•œì˜ ì»¨í…ìŠ¤íŠ¸ë§Œ ì „ë‹¬
            minimal_context = self.extract_relevant_context(
                subtask, 
                previous_results=results
            )
            
            # ì‹¤í–‰
            result = agent.execute(subtask, minimal_context)
            results.append(result)
        
        # ìµœì¢… ê²°ê³¼ í†µí•©
        return self.merge_results(results)
    
    def extract_relevant_context(self, subtask, previous_results):
        """ê° ì—ì´ì „íŠ¸ì— í•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ"""
        # ì´ì „ ê²°ê³¼ì—ì„œ ê´€ë ¨ ë¶€ë¶„ë§Œ ì„ íƒ
        relevant = [r for r in previous_results 
                   if self.is_relevant(r, subtask)]
        
        # í† í° ì˜ˆì‚° ë‚´ë¡œ ì••ì¶•
        return self.compress_to_budget(relevant, max_tokens=5000)
```

**ì»¨í…ìŠ¤íŠ¸ ê²©ë¦¬ íš¨ê³¼:**

```
ë‹¨ì¼ ì—ì´ì „íŠ¸ ë°©ì‹:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Context: 100K tokens               â”‚
â”‚ - Task description                 â”‚
â”‚ - All documents                    â”‚
â”‚ - All tool outputs                 â”‚
â”‚ - Full conversation history        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†’ ì„±ëŠ¥ ì €í•˜, ë†’ì€ ë¹„ìš©

ë©€í‹° ì—ì´ì „íŠ¸ (Orchestrator) ë°©ì‹:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Researcher   â”‚ â”‚ Coder        â”‚ â”‚ Reviewer     â”‚
â”‚ 10K context  â”‚ â”‚ 15K context  â”‚ â”‚ 8K context   â”‚
â”‚ (ì—°êµ¬ ë¬¸ì„œë§Œ) â”‚ â”‚ (ì½”ë“œ ê´€ë ¨ë§Œ) â”‚ â”‚ (ë¦¬ë·° ëŒ€ìƒë§Œ) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†’ ê° ì—ì´ì „íŠ¸ ìµœì  ì„±ëŠ¥, ë¹„ìš© ì ˆê°
```

**2. Swarm Pattern (ìŠ¤ì›œ)**

```python
class SwarmCoordinator:
    """ë…ë¦½ì  ì—ì´ì „íŠ¸ë“¤ì´ í˜‘ë ¥"""
    
    def __init__(self, num_agents=5):
        self.agents = [GeneralistAgent(id=i) for i in range(num_agents)]
        self.shared_memory = SharedMemory()
    
    def solve_problem(self, problem):
        """
        ê° ì—ì´ì „íŠ¸ê°€ ë…ë¦½ì ìœ¼ë¡œ ì‘ì—…í•˜ë˜
        ê³µìœ  ë©”ëª¨ë¦¬ë¥¼ í†µí•´ í˜‘ë ¥
        """
        
        # ê° ì—ì´ì „íŠ¸ì— ê°™ì€ ë¬¸ì œ í• ë‹¹
        futures = []
        for agent in self.agents:
            # ê³µìœ  ë©”ëª¨ë¦¬ ì ‘ê·¼ ê¶Œí•œ ë¶€ì—¬
            agent.connect_to_shared_memory(self.shared_memory)
            
            # ë¹„ë™ê¸° ì‹¤í–‰
            future = agent.solve_async(problem)
            futures.append(future)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        solutions = [f.result() for f in futures]
        
        # ìµœì„ ì˜ í•´ê²°ì±… ì„ íƒ
        return self.select_best_solution(solutions)
    
    def select_best_solution(self, solutions):
        """ì—¬ëŸ¬ í•´ê²°ì±… ì¤‘ ìµœì„  ì„ íƒ"""
        # íˆ¬í‘œ, ì ìˆ˜, ê²€ì¦ ë“± ë‹¤ì–‘í•œ ë°©ë²• ê°€ëŠ¥
        scores = [self.evaluate(s) for s in solutions]
        best_idx = max(range(len(scores)), key=lambda i: scores[i])
        return solutions[best_idx]
```

**ì»¨í…ìŠ¤íŠ¸ ê³µìœ  ì „ëµ:**

```python
class SharedMemory:
    """ì—ì´ì „íŠ¸ ê°„ íš¨ìœ¨ì  ì •ë³´ ê³µìœ """
    
    def __init__(self):
        self.storage = {
            'facts': {},           # ê²€ì¦ëœ ì‚¬ì‹¤
            'hypotheses': {},      # ê°€ì„¤ë“¤
            'observations': [],    # ê´€ì°° ê²°ê³¼
        }
    
    def add_fact(self, fact, confidence=1.0):
        """ê²€ì¦ëœ ì •ë³´ë§Œ ê³µìœ """
        if confidence > 0.8:
            self.storage['facts'][fact.id] = fact
    
    def get_relevant_context(self, query, max_tokens=5000):
        """ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ì •ë³´ë§Œ ì¶”ì¶œ"""
        # ì „ì²´ ê³µìœ  ë©”ëª¨ë¦¬ê°€ ì•„ë‹Œ ê´€ë ¨ ë¶€ë¶„ë§Œ
        relevant_facts = self.search_facts(query)
        return self.compress_to_budget(relevant_facts, max_tokens)
```

**3. Hierarchical Pattern (ê³„ì¸µì )**

```python
class HierarchicalSystem:
    """ê³„ì¸µì  ì—ì´ì „íŠ¸ êµ¬ì¡°"""
    
    def __init__(self):
        # ìƒìœ„ ë ˆë²¨: ì „ëµì  ì˜ì‚¬ê²°ì •
        self.strategic_agent = StrategicAgent(
            context_budget=50000  # í° ì»¨í…ìŠ¤íŠ¸
        )
        
        # ì¤‘ê°„ ë ˆë²¨: ì „ìˆ ì  ê³„íš
        self.tactical_agents = [
            TacticalAgent(context_budget=20000)
            for _ in range(3)
        ]
        
        # í•˜ìœ„ ë ˆë²¨: ì‹¤í–‰
        self.operational_agents = [
            OperationalAgent(context_budget=5000)
            for _ in range(10)
        ]
    
    def execute(self, mission):
        """ê³„ì¸µì  ì‹¤í–‰"""
        # 1. ì „ëµ ìˆ˜ë¦½ (ìƒìœ„)
        strategy = self.strategic_agent.plan(mission)
        
        # 2. ì „ìˆ  ê³„íš (ì¤‘ê°„)
        tactical_plans = []
        for tactical_agent in self.tactical_agents:
            # ì „ëµì˜ ì¼ë¶€ë§Œ ì „ë‹¬
            relevant_strategy = extract_relevant_part(
                strategy, 
                tactical_agent.specialty
            )
            plan = tactical_agent.create_plan(relevant_strategy)
            tactical_plans.append(plan)
        
        # 3. ì‹¤í–‰ (í•˜ìœ„)
        results = []
        for plan in tactical_plans:
            for task in plan.tasks:
                agent = self.assign_operational_agent(task)
                # ìµœì†Œ ì»¨í…ìŠ¤íŠ¸ë§Œ ì „ë‹¬
                result = agent.execute(task, minimal_context=True)
                results.append(result)
        
        return results
```

**ê³„ì¸µë³„ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬:**

```
Level 1 (Strategic): 50K tokens
â”œâ”€ Mission overview
â”œâ”€ Domain knowledge
â””â”€ High-level constraints

Level 2 (Tactical): 20K tokens
â”œâ”€ Assigned portion of strategy
â”œâ”€ Specialized domain info
â””â”€ Team coordination

Level 3 (Operational): 5K tokens
â”œâ”€ Specific task description
â”œâ”€ Immediate context only
â””â”€ Tool documentation
```

---

### 5ï¸âƒ£ memory-systems (ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ)

**ì–¸ì œ ì‚¬ìš©**: ì„¸ì…˜ ê°„ ì—°ì†ì„± ìœ ì§€, ì§€ì‹ ì¶•ì , ì¥ê¸° ì—ì´ì „íŠ¸ ìš´ì˜

#### ë©”ëª¨ë¦¬ ê³„ì¸µ êµ¬ì¡°

```python
class HierarchicalMemory:
    """
    3ê³„ì¸µ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ:
    - Working Memory (ì‘ì—… ë©”ëª¨ë¦¬)
    - Short-term Memory (ë‹¨ê¸° ë©”ëª¨ë¦¬)  
    - Long-term Memory (ì¥ê¸° ë©”ëª¨ë¦¬)
    """
    
    def __init__(self):
        # Layer 1: Working Memory (ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ë‚´)
        self.working = {
            'capacity': 8000,      # í† í°
            'latency': 0,          # ì¦‰ì‹œ ì ‘ê·¼
            'persistence': 'volatile',  # ì„¸ì…˜ ì¢…ë£Œì‹œ ì‚­ì œ
            'content': []
        }
        
        # Layer 2: Short-term Memory (ì„¸ì…˜ ë‚´ ìœ ì§€)
        self.short_term = {
            'capacity': 50000,
            'latency': 'low',      # ë¹ ë¥¸ ê²€ìƒ‰
            'persistence': 'session',
            'content': []
        }
        
        # Layer 3: Long-term Memory (ì˜êµ¬ ë³´ê´€)
        self.long_term = {
            'capacity': float('inf'),
            'latency': 'medium',   # ê²€ìƒ‰ í•„ìš”
            'persistence': 'permanent',
            'storage': VectorDatabase()
        }
    
    def add(self, information, importance='medium'):
        """ì¤‘ìš”ë„ì— ë”°ë¼ ì ì ˆí•œ ê³„ì¸µì— ì €ì¥"""
        
        if importance == 'critical':
            # ì‘ì—… ë©”ëª¨ë¦¬ì— ì¶”ê°€ (ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥)
            self.working['content'].append(information)
            self._enforce_working_limit()
        
        elif importance == 'high':
            # ë‹¨ê¸° ë©”ëª¨ë¦¬ì— ì¶”ê°€
            self.short_term['content'].append(information)
        
        else:
            # ì¥ê¸° ë©”ëª¨ë¦¬ì— ì €ì¥
            self.long_term['storage'].add(information)
    
    def _enforce_working_limit(self):
        """ì‘ì—… ë©”ëª¨ë¦¬ ìš©ëŸ‰ ì œí•œ ì ìš©"""
        while self.get_working_tokens() > self.working['capacity']:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª©ì„ ë‹¨ê¸°ë¡œ ì´ë™
            moved = self.working['content'].pop(0)
            self.short_term['content'].append(moved)
    
    def retrieve(self, query, max_items=5):
        """ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë©”ëª¨ë¦¬ ê²€ìƒ‰"""
        results = []
        
        # 1. ì‘ì—… ë©”ëª¨ë¦¬ì—ì„œ ê²€ìƒ‰ (ê°€ì¥ ë¹ ë¦„)
        for item in self.working['content']:
            if self.is_relevant(query, item):
                results.append(('working', item))
        
        # 2. ë‹¨ê¸° ë©”ëª¨ë¦¬ì—ì„œ ê²€ìƒ‰
        if len(results) < max_items:
            for item in self.short_term['content'][-20:]:  # ìµœê·¼ 20ê°œë§Œ
                if self.is_relevant(query, item):
                    results.append(('short_term', item))
        
        # 3. ì¥ê¸° ë©”ëª¨ë¦¬ì—ì„œ ê²€ìƒ‰
        if len(results) < max_items:
            long_term_results = self.long_term['storage'].search(
                query, 
                limit=max_items - len(results)
            )
            results.extend([('long_term', r) for r in long_term_results])
        
        return results[:max_items]
```

#### Vector Storeì˜ í•œê³„ì™€ í•´ê²°ì±…

**ë¬¸ì œ:**

```
Vector Store (RAG)ì˜ ì œì•½:
âŒ ê´€ê³„ ì •ë³´ ì†ì‹¤
   "ê³ ê° Xê°€ ì œí’ˆ Yë¥¼ ë‚ ì§œ Zì— êµ¬ë§¤" â†’ ë²¡í„°í™” ì‹œ ê´€ê³„ êµ¬ì¡° ì‚¬ë¼ì§

âŒ ë³µì¡í•œ ì¿¼ë¦¬ ë¶ˆê°€
   "ì œí’ˆ Yë¥¼ êµ¬ë§¤í•œ ê³ ê°ë“¤ì´ ë˜ êµ¬ë§¤í•œ ì œí’ˆì€?"
   â†’ Vector Storeë¡œ ë‹µë³€ ë¶ˆê°€ëŠ¥

âŒ ì‹œê°„ ì •ë³´ ë¶€ì¬
   ìµœì‹ ì„± íŒë‹¨ ì–´ë ¤ì›€
```

**í•´ê²°: Knowledge Graph + Temporal ì •ë³´**

```python
class TemporalKnowledgeGraph:
    """ì‹œê°„ ì •ë³´ë¥¼ í¬í•¨í•œ ì§€ì‹ ê·¸ë˜í”„"""
    
    def __init__(self):
        self.graph = nx.DiGraph()
        self.temporal_index = {}
    
    def add_fact(self, subject, predicate, object, timestamp):
        """
        ì‚¬ì‹¤ì„ ê·¸ë˜í”„ì— ì¶”ê°€
        ì˜ˆ: (Customer_X, purchased, Product_Y, 2024-01-15)
        """
        # ë…¸ë“œ ì¶”ê°€
        self.graph.add_node(subject, type='customer')
        self.graph.add_node(object, type='product')
        
        # ì—£ì§€ ì¶”ê°€ (ê´€ê³„)
        self.graph.add_edge(
            subject, object,
            relation=predicate,
            timestamp=timestamp
        )
        
        # ì‹œê°„ ì¸ë±ìŠ¤
        if timestamp not in self.temporal_index:
            self.temporal_index[timestamp] = []
        self.temporal_index[timestamp].append((subject, predicate, object))
    
    def query_relationship(self, query):
        """ê´€ê³„ ê¸°ë°˜ ì¿¼ë¦¬"""
        # "Product Yë¥¼ êµ¬ë§¤í•œ ê³ ê°ë“¤"
        customers = [
            node for node in self.graph.nodes()
            if self.graph.has_edge(node, 'Product_Y')
        ]
        
        # "ê·¸ ê³ ê°ë“¤ì´ êµ¬ë§¤í•œ ë‹¤ë¥¸ ì œí’ˆë“¤"
        other_products = set()
        for customer in customers:
            products = self.graph.successors(customer)
            other_products.update(products)
        
        other_products.discard('Product_Y')  # ì›ë˜ ì œí’ˆ ì œì™¸
        return list(other_products)
    
    def query_temporal(self, start_date, end_date):
        """ì‹œê°„ ë²”ìœ„ ì¿¼ë¦¬"""
        facts = []
        for timestamp, fact_list in self.temporal_index.items():
            if start_date <= timestamp <= end_date:
                facts.extend(fact_list)
        return facts
```

#### Hybrid ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ

```python
class HybridMemorySystem:
    """Vector Store + Knowledge Graph ê²°í•©"""
    
    def __init__(self):
        self.vector_store = ChromaDB()
        self.knowledge_graph = TemporalKnowledgeGraph()
        self.hierarchical_memory = HierarchicalMemory()
    
    def store_information(self, info):
        """ì •ë³´ë¥¼ ì ì ˆí•œ ì €ì¥ì†Œì— ë¶„ì‚°"""
        
        # 1. í…ìŠ¤íŠ¸ ì •ë³´ â†’ Vector Store (ì˜ë¯¸ë¡ ì  ê²€ìƒ‰)
        if info['type'] == 'document':
            self.vector_store.add(
                text=info['content'],
                metadata=info['metadata']
            )
        
        # 2. ê´€ê³„ ì •ë³´ â†’ Knowledge Graph (êµ¬ì¡°ì  ì¿¼ë¦¬)
        elif info['type'] == 'relationship':
            self.knowledge_graph.add_fact(
                subject=info['subject'],
                predicate=info['predicate'],
                object=info['object'],
                timestamp=info['timestamp']
            )
        
        # 3. ì¦‰ì‹œ ì‚¬ìš© ì •ë³´ â†’ Hierarchical Memory
        if info.get('immediate_use', False):
            self.hierarchical_memory.add(
                info['content'],
                importance='critical'
            )
    
    def retrieve(self, query, query_type='hybrid'):
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        results = {}
        
        # Semantic search
        if query_type in ['semantic', 'hybrid']:
            results['semantic'] = self.vector_store.search(query, top_k=5)
        
        # Structural query
        if query_type in ['structural', 'hybrid']:
            results['structural'] = self.knowledge_graph.query_relationship(query)
        
        # Working memory
        if query_type in ['immediate', 'hybrid']:
            results['immediate'] = self.hierarchical_memory.retrieve(query)
        
        # ê²°ê³¼ í†µí•© ë° ìˆœìœ„ ì§€ì •
        return self.merge_and_rank(results)
```

---

### 6ï¸âƒ£ tool-design (ë„êµ¬ ì„¤ê³„)

**ì–¸ì œ ì‚¬ìš©**: ì—ì´ì „íŠ¸ê°€ ì‚¬ìš©í•  ë„êµ¬ ì„¤ê³„, ë„êµ¬ ì„¤ëª… ìµœì í™”

#### ë„êµ¬ ì„¤ê³„ ì›ì¹™

**1. ëª…í™•í•œ ë„êµ¬ ì„¤ëª…**

```python
# âŒ ë‚˜ìœ ì˜ˆ: ëª¨í˜¸í•œ ì„¤ëª…
{
    "name": "process_data",
    "description": "Processes data",
    "parameters": {
        "data": {"type": "string"}
    }
}

# âœ… ì¢‹ì€ ì˜ˆ: êµ¬ì²´ì ì¸ ì„¤ëª…
{
    "name": "analyze_customer_sentiment",
    "description": """
    Analyzes customer feedback to determine sentiment (positive/negative/neutral).
    
    Use when:
    - You have customer reviews, support tickets, or survey responses
    - You need to understand overall customer satisfaction
    
    Don't use when:
    - Data is not customer-related
    - You need numerical analysis (use data_analyzer instead)
    
    Returns: Sentiment score (-1.0 to 1.0) and classification
    """,
    "parameters": {
        "feedback_text": {
            "type": "string",
            "description": "Customer feedback text to analyze. Max 5000 characters."
        },
        "language": {
            "type": "string",
            "enum": ["en", "ko", "ja"],
            "description": "Language of the feedback. Defaults to 'en'."
        }
    }
}
```

**2. ë„êµ¬ ê°œìˆ˜ ìµœì í™” (RAG for Tools)**

```python
class ToolRAG:
    """ë„êµ¬ ì„¤ëª…ì— RAG ì ìš©"""
    
    def __init__(self, all_tools):
        self.all_tools = all_tools
        self.tool_embeddings = self._embed_tools()
    
    def _embed_tools(self):
        """ê° ë„êµ¬ì˜ ì„¤ëª…ì„ ì„ë² ë”©"""
        embeddings = {}
        for tool in self.all_tools:
            description = f"{tool['name']}: {tool['description']}"
            embeddings[tool['name']] = get_embedding(description)
        return embeddings
    
    def select_relevant_tools(self, task_description, max_tools=5):
        """ì‘ì—…ê³¼ ê´€ë ¨ëœ ë„êµ¬ë§Œ ì„ íƒ"""
        
        task_embedding = get_embedding(task_description)
        
        # ê° ë„êµ¬ì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = {}
        for tool_name, tool_embedding in self.tool_embeddings.items():
            sim = cosine_similarity([task_embedding], [tool_embedding])[0][0]
            similarities[tool_name] = sim
        
        # ìƒìœ„ Nê°œ ì„ íƒ
        top_tools = sorted(
            similarities.items(),
            key=lambda x: x[1],
            reverse=True
        )[:max_tools]
        
        return [
            next(t for t in self.all_tools if t['name'] == name)
            for name, _ in top_tools
        ]
```

**ì„±ëŠ¥ í–¥ìƒ (ì—°êµ¬ ê²°ê³¼):**

```
ì „ì²´ ë„êµ¬ ì œê³µ:
â”œâ”€ 100ê°œ ë„êµ¬ â†’ ì»¨í…ìŠ¤íŠ¸ 20K í† í°
â”œâ”€ ë„êµ¬ ì„ íƒ ì •í™•ë„: 35%
â””â”€ ì‘ë‹µ ì‹œê°„: 8ì´ˆ

RAG ê¸°ë°˜ ë„êµ¬ ì„ íƒ:
â”œâ”€ 5ê°œ ê´€ë ¨ ë„êµ¬ë§Œ â†’ ì»¨í…ìŠ¤íŠ¸ 1K í† í°
â”œâ”€ ë„êµ¬ ì„ íƒ ì •í™•ë„: 85% (â†‘ 3ë°°)
â””â”€ ì‘ë‹µ ì‹œê°„: 2ì´ˆ (â†“ 75%)
```

---

### 7ï¸âƒ£ evaluation (í‰ê°€)

**ì–¸ì œ ì‚¬ìš©**: ì—ì´ì „íŠ¸ ì„±ëŠ¥ ì¸¡ì •, A/B í…ŒìŠ¤íŠ¸, í’ˆì§ˆ ë³´ì¥

#### LLM-as-a-Judge ê¸°ë²•

```python
class LLMJudge:
    """LLMì„ í‰ê°€ìë¡œ í™œìš©"""
    
    def direct_scoring(self, response, criteria):
        """ì§ì ‘ ì ìˆ˜ ë§¤ê¸°ê¸°"""
        judge_prompt = f"""
        ë‹¤ìŒ ì‘ë‹µì„ í‰ê°€í•˜ì„¸ìš”.
        
        ì‘ë‹µ: {response}
        
        í‰ê°€ ê¸°ì¤€:
        1. ì •í™•ì„± (0-10ì )
        2. ì™„ì„±ë„ (0-10ì )
        3. ëª…í™•ì„± (0-10ì )
        
        ê° ê¸°ì¤€ì— ëŒ€í•´ ì ìˆ˜ì™€ ì´ìœ ë¥¼ ì œì‹œí•˜ì„¸ìš”.
        """
        
        return call_llm(judge_prompt)
    
    def pairwise_comparison(self, response_a, response_b, question):
        """ìŒ ë¹„êµ"""
        judge_prompt = f"""
        ì§ˆë¬¸: {question}
        
        ì‘ë‹µ A: {response_a}
        ì‘ë‹µ B: {response_b}
        
        ì–´ë–¤ ì‘ë‹µì´ ë” ë‚˜ì€ê°€ìš”? A, B, ë˜ëŠ” Tieë¥¼ ì„ íƒí•˜ê³  ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.
        """
        
        return call_llm(judge_prompt)
    
    def rubric_based_evaluation(self, response, rubric):
        """ë£¨ë¸Œë¦­ ê¸°ë°˜ í‰ê°€"""
        judge_prompt = f"""
        ë£¨ë¸Œë¦­:
        {rubric}
        
        ì‘ë‹µ:
        {response}
        
        ë£¨ë¸Œë¦­ì˜ ê° í•­ëª©ì— ëŒ€í•´ í‰ê°€í•˜ì„¸ìš”.
        """
        
        return call_llm(judge_prompt)
```

#### ì»¨í…ìŠ¤íŠ¸ íš¨ìœ¨ì„± ë©”íŠ¸ë¦­

```python
class ContextEfficiencyMetrics:
    """ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© íš¨ìœ¨ì„± ì¸¡ì •"""
    
    def __init__(self):
        self.monitor = TokenMonitor()
    
    def calculate_metrics(self, context, response, task_success):
        """ì¢…í•© ë©”íŠ¸ë¦­ ê³„ì‚°"""
        
        # 1. í† í° íš¨ìœ¨ì„±
        context_tokens = self.monitor.count_tokens(context)
        response_tokens = self.monitor.count_tokens(response)
        total_tokens = context_tokens + response_tokens
        
        # 2. ì •ë³´ ë°€ë„
        # ì‹¤ì œ ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸ vs ì „ì²´ ì»¨í…ìŠ¤íŠ¸
        used_context = self.extract_used_context(context, response)
        used_tokens = self.monitor.count_tokens(used_context)
        information_density = used_tokens / context_tokens
        
        # 3. ì„±ê³µë¥  ëŒ€ë¹„ ë¹„ìš©
        cost_per_success = total_tokens if task_success else float('inf')
        
        return {
            'total_tokens': total_tokens,
            'context_tokens': context_tokens,
            'response_tokens': response_tokens,
            'information_density': information_density,
            'cost_per_success': cost_per_success,
            'success': task_success
        }
    
    def extract_used_context(self, context, response):
        """ì‘ë‹µì—ì„œ ì‹¤ì œ ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸ ë¶€ë¶„ ì¶”ì¶œ"""
        # ì‘ë‹µì— ë‚˜íƒ€ë‚œ ì»¨í…ìŠ¤íŠ¸ì˜ êµ¬ì ˆë“¤ì„ ì°¾ìŒ
        context_sentences = sent_tokenize(context)
        response_lower = response.lower()
        
        used = []
        for sent in context_sentences:
            # ë¬¸ì¥ì˜ ì£¼ìš” ë‹¨ì–´ë“¤ì´ ì‘ë‹µì— ë‚˜íƒ€ë‚˜ëŠ”ì§€ í™•ì¸
            keywords = extract_keywords(sent)
            if any(kw.lower() in response_lower for kw in keywords):
                used.append(sent)
        
        return ' '.join(used)
```

---

## Context Engineering 4ëŒ€ ì „ëµ (LangChain)

LangChain íŒ€ì´ ì •ë¦¬í•œ ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì˜ 4ê°€ì§€ í•µì‹¬ ì „ëµ:

### 1. Write (ì‘ì„±) - ì»¨í…ìŠ¤íŠ¸ ì™¸ë¶€ì— ì €ì¥

**ê°œë…**: ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ë°–ì— ì •ë³´ë¥¼ ì €ì¥í•˜ì—¬ í•„ìš”í•  ë•Œ ë¡œë“œ

**êµ¬í˜„ ì˜ˆì‹œ:**

```python
class ScratchpadMemory:
    """ìŠ¤í¬ë˜ì¹˜íŒ¨ë“œ íŒ¨í„´"""
    
    def __init__(self):
        self.scratchpad = {}  # ì»¨í…ìŠ¤íŠ¸ ì™¸ë¶€ ì €ì¥ì†Œ
    
    def write_note(self, key, content):
        """ë©”ëª¨ ì‘ì„± (ì»¨í…ìŠ¤íŠ¸ì— ë„£ì§€ ì•ŠìŒ)"""
        self.scratchpad[key] = {
            'content': content,
            'timestamp': datetime.now(),
            'access_count': 0
        }
    
    def read_note(self, key):
        """í•„ìš”í•  ë•Œë§Œ ì½ê¸°"""
        if key in self.scratchpad:
            self.scratchpad[key]['access_count'] += 1
            return self.scratchpad[key]['content']
        return None

# Anthropicì˜ Multi-Agent Researcher ì˜ˆì‹œ
class LeadResearcher:
    def plan_research(self, topic):
        # ê³„íšì„ ë©”ëª¨ë¦¬ì— ì €ì¥ (ì»¨í…ìŠ¤íŠ¸ ì ˆì•½)
        plan = self.create_research_plan(topic)
        self.memory.write_note('research_plan', plan)
        
        # ì»¨í…ìŠ¤íŠ¸ì—ëŠ” ìš”ì•½ë§Œ í¬í•¨
        return f"ì—°êµ¬ ê³„íš ìˆ˜ë¦½ ì™„ë£Œ (ì„¸ë¶€ì‚¬í•­ì€ ë©”ëª¨ë¦¬ì— ì €ì¥)"
```

### 2. Select (ì„ íƒ) - ê´€ë ¨ ì •ë³´ë§Œ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨

**ê°œë…**: ì „ì²´ê°€ ì•„ë‹Œ ì‘ì—… ê´€ë ¨ ì •ë³´ë§Œ ì„ íƒì ìœ¼ë¡œ ë¡œë“œ

```python
class SelectiveContextLoader:
    """ì„ íƒì  ì»¨í…ìŠ¤íŠ¸ ë¡œë”©"""
    
    def __init__(self):
        self.full_state = {}
        self.vector_db = ChromaDB()
    
    def add_to_state(self, key, value, metadata=None):
        """ìƒíƒœì— ì¶”ê°€"""
        self.full_state[key] = value
        
        # ë²¡í„° DBì—ë„ ì €ì¥ (ê²€ìƒ‰ìš©)
        self.vector_db.add(
            text=str(value),
            metadata={'key': key, **(metadata or {})}
        )
    
    def get_relevant_context(self, query, max_items=3):
        """ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ìƒíƒœë§Œ ë¡œë“œ"""
        # ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ í•­ëª© ì°¾ê¸°
        results = self.vector_db.search(query, top_k=max_items)
        
        # ì „ì²´ ìƒíƒœì—ì„œ í•´ë‹¹ í•­ëª©ë“¤ë§Œ ê°€ì ¸ì˜¤ê¸°
        context = {}
        for result in results:
            key = result.metadata['key']
            if key in self.full_state:
                context[key] = self.full_state[key]
        
        return context

# ì‚¬ìš© ì˜ˆ
loader = SelectiveContextLoader()

# ë§ì€ ì •ë³´ ì €ì¥
for i in range(100):
    loader.add_to_state(
        f'doc_{i}',
        f'Document {i} content...',
        metadata={'category': 'research'}
    )

# ì¿¼ë¦¬ì— ê´€ë ¨ëœ ê²ƒë§Œ ë¡œë“œ
relevant = loader.get_relevant_context(
    "Python programming tips",
    max_items=3
)
# â†’ 100ê°œ ì¤‘ 3ê°œë§Œ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨!
```

### 3. Compress (ì••ì¶•) - í•„ìš”í•œ í† í°ë§Œ ìœ ì§€

**ê°œë…**: ì»¨í…ìŠ¤íŠ¸ë¥¼ ì••ì¶•í•˜ì—¬ í† í° ìˆ˜ ê°ì†Œ

#### 3-1. ëŒ€í™” ìš”ì•½

```python
class ConversationCompressor:
    """ëŒ€í™” ì´ë ¥ ì••ì¶•"""
    
    def compress_long_conversation(self, messages):
        """ê¸´ ëŒ€í™”ë¥¼ ìš”ì•½ìœ¼ë¡œ ì••ì¶•"""
        
        if len(messages) < 10:
            return messages  # ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ
        
        # ìµœê·¼ 3ê°œëŠ” ìœ ì§€
        recent = messages[-3:]
        
        # ë‚˜ë¨¸ì§€ëŠ” ìš”ì•½
        old_messages = messages[:-3]
        summary = self.summarize_messages(old_messages)
        
        return [
            {"role": "system", "content": f"ì´ì „ ëŒ€í™” ìš”ì•½: {summary}"},
            *recent
        ]
    
    def summarize_messages(self, messages):
        """ë©”ì‹œì§€ ë°°ì¹˜ë¥¼ ìš”ì•½"""
        combined = "\n".join([
            f"{m['role']}: {m['content']}" 
            for m in messages
        ])
        
        summary_prompt = f"""
        ë‹¤ìŒ ëŒ€í™”ë¥¼ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½:
        
        {combined}
        
        í•µì‹¬ ê²°ì •ì‚¬í•­ê³¼ ì¤‘ìš” ì •ë³´ë§Œ í¬í•¨í•˜ì„¸ìš”.
        """
        
        return call_llm(summary_prompt)

# ì‹¤ì „ íš¨ê³¼ (LangChain í…ŒìŠ¤íŠ¸):
# ì „: 115,000 í† í°
# í›„:  60,000 í† í° (48% ê°ì†Œ)
```

#### 3-2. ë„êµ¬ ì¶œë ¥ ì••ì¶•

```python
class ToolOutputCompressor:
    """ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ ì••ì¶•"""
    
    def compress_large_output(self, tool_output, max_tokens=1000):
        """í° ë„êµ¬ ì¶œë ¥ì„ ì••ì¶•"""
        
        current_tokens = count_tokens(tool_output)
        
        if current_tokens <= max_tokens:
            return tool_output
        
        # JSONì¸ ê²½ìš°: ì¤‘ìš” í•„ë“œë§Œ ì¶”ì¶œ
        if is_json(tool_output):
            return self.extract_important_fields(tool_output)
        
        # í…ìŠ¤íŠ¸ì¸ ê²½ìš°: ìš”ì•½
        return self.summarize_text(tool_output, max_tokens)
    
    def extract_important_fields(self, json_output):
        """JSONì—ì„œ ì¤‘ìš” í•„ë“œë§Œ"""
        data = json.loads(json_output)
        
        # ì˜ˆ: API ì‘ë‹µì—ì„œ 'results' í•„ë“œë§Œ
        if 'results' in data:
            return json.dumps({'results': data['results'][:5]})  # ìƒìœ„ 5ê°œë§Œ
        
        return json_output
```

### 4. Isolate (ê²©ë¦¬) - ì»¨í…ìŠ¤íŠ¸ ë¶„ë¦¬

**ê°œë…**: ê´€ë ¨ ì—†ëŠ” ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¦¬í•˜ì—¬ í˜¼ë€ ë°©ì§€

```python
class ContextIsolation:
    """ì»¨í…ìŠ¤íŠ¸ ê²©ë¦¬ ì „ëµ"""
    
    def create_isolated_sessions(self, tasks):
        """ê° ì‘ì—…ë§ˆë‹¤ ë…ë¦½ì  ì„¸ì…˜"""
        results = []
        
        for task in tasks:
            # ìƒˆë¡œìš´ ë…ë¦½ ì„¸ì…˜
            session = self.create_new_session()
            
            # ì´ ì‘ì—…ì—ë§Œ í•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸
            relevant_context = self.get_task_context(task)
            session.set_context(relevant_context)
            
            # ì‹¤í–‰
            result = session.execute(task)
            results.append(result)
            
            # ì„¸ì…˜ ì¢…ë£Œ (ì»¨í…ìŠ¤íŠ¸ ì •ë¦¬)
            session.close()
        
        return results
    
    def parallel_isolated_execution(self, subtasks):
        """ë³‘ë ¬ ê²©ë¦¬ ì‹¤í–‰"""
        from concurrent.futures import ThreadPoolExecutor
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = [
                executor.submit(self.execute_isolated, task)
                for task in subtasks
            ]
            
            return [f.result() for f in futures]
    
    def execute_isolated(self, task):
        """ì™„ì „íˆ ê²©ë¦¬ëœ í™˜ê²½ì—ì„œ ì‹¤í–‰"""
        # 1. ë…ë¦½ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = {
            'system_prompt': self.get_task_specific_prompt(task),
            'tools': self.get_task_specific_tools(task),
            'documents': self.get_task_specific_docs(task)
        }
        
        # 2. ê²©ë¦¬ ì‹¤í–‰
        return self.execute_with_context(task, context)
```

**ê²©ë¦¬ì˜ ì¥ì :**

```
ë¬¸ì œ: ì‘ì—… Aì˜ ì»¨í…ìŠ¤íŠ¸ê°€ ì‘ì—… Bì— ì˜í–¥
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Task A: "ë¶„ì„" â†’ Context: í†µê³„ ë¬¸ì„œ  â”‚
â”‚ Task B: "ê¸€ì“°ê¸°" â†’ Context: ì—¬ì „íˆ í†µê³„â”‚
â”‚ â†’ ê²°ê³¼: ê¸€ì´ ë„ˆë¬´ ê¸°ìˆ ì ìœ¼ë¡œ ì‘ì„±ë¨   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

í•´ê²°: ê²©ë¦¬ëœ ì‹¤í–‰
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Session A    â”‚    â”‚ Session B    â”‚
â”‚ í†µê³„ ë¬¸ì„œ     â”‚    â”‚ ê¸€ì“°ê¸° ê°€ì´ë“œâ”‚
â”‚ ë¶„ì„ ë„êµ¬     â”‚    â”‚ ìŠ¤íƒ€ì¼ ê°€ì´ë“œâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†’ ê° ì‘ì—…ì´ ë…ë¦½ì ìœ¼ë¡œ ìµœì í™”ë¨
```

---

## Context Rot ì™„ì „ ì •ë³µ

### ğŸ”¬ ìµœì‹  ì—°êµ¬ (Chroma Research, 2025)

#### ì‹¤í—˜ ì„¤ì •

**í‰ê°€ ëŒ€ìƒ:** 18ê°œ SOTA ëª¨ë¸
- GPT-4.1
- Claude 4  
- Gemini 2.5
- Qwen 3
- ê¸°íƒ€ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸

**í…ŒìŠ¤íŠ¸ ê³¼ì œ:**
1. Needle-in-Haystack (ì •ë³´ ê²€ìƒ‰)
2. ë°˜ë³µ ë‹¨ì–´ ìƒì„±
3. ëŒ€í™”í˜• Q&A
4. ë‹¤ë‹¨ê³„ ì¶”ë¡ 

#### í•µì‹¬ ë°œê²¬ì‚¬í•­

**1. ë¹„ê· ì¼í•œ ì„±ëŠ¥ ì €í•˜**

```
ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ì •í™•ë„:

ë‹¨ìˆœ ì‘ì—… (ë°˜ë³µ ë‹¨ì–´):
1K í† í°:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 95% ì •í™•ë„
10K í† í°:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     80% ì •í™•ë„
50K í† í°:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               30% ì •í™•ë„
100K í† í°: â–ˆâ–ˆ                   10% ì •í™•ë„

ë³µì¡í•œ ì‘ì—… (ë‹¤ë‹¨ê³„ ì¶”ë¡ ):
1K í† í°:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     80% ì •í™•ë„
10K í† í°:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           50% ì •í™•ë„
50K í† í°:  â–ˆâ–ˆâ–ˆâ–ˆ                 20% ì •í™•ë„
100K í† í°: â–ˆ                     5% ì •í™•ë„

â†’ ë³µì¡í•œ ì‘ì—…ì¼ìˆ˜ë¡ ì„±ëŠ¥ ì €í•˜ ë” ì‹¬ê°
```

**2. ìœ„ì¹˜ í¸í–¥ (Positional Bias)**

```
ì •ë³´ ìœ„ì¹˜ì— ë”°ë¥¸ ì •í™•ë„:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Document (4K tokens)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [Start] Needle here â†’ 75% ì •í™•ë„   â”‚
â”‚                                    â”‚
â”‚ [Middle] Needle here â†’ 55% ì •í™•ë„  â”‚
â”‚                                    â”‚
â”‚ [End] Needle here â†’ 72% ì •í™•ë„     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

U-Shaped Attention Curve í™•ì¸
```

**3. ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„ì˜ ì˜í–¥**

```
Needleê³¼ Haystackì˜ ìœ ì‚¬ë„:

ë‚®ì€ ìœ ì‚¬ë„ (ì™„ì „íˆ ë‹¤ë¥¸ ì£¼ì œ):
â””â”€ ì •í™•ë„: 70%

ì¤‘ê°„ ìœ ì‚¬ë„ (ê´€ë ¨ ì£¼ì œ):  
â””â”€ ì •í™•ë„: 55% âš ï¸ (ê°€ì¥ ì–´ë ¤ì›€)

ë†’ì€ ìœ ì‚¬ë„ (ê°™ì€ ì£¼ì œ):
â””â”€ ì •í™•ë„: 65%

â†’ ë¹„ìŠ·í•˜ì§€ë§Œ ë‹¤ë¥¸ ì •ë³´ê°€ ê°€ì¥ í˜¼ë€ìŠ¤ëŸ¬ì›€
```

**4. Distractorì˜ ì˜í–¥**

```
ë°©í•´ ìš”ì†Œ(Distractor) ê°œìˆ˜:

0ê°œ: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 95%
1ê°œ: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     80%
2ê°œ: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         60%
4ê°œ: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               30%

â†’ Distractorê°€ ë§ì„ìˆ˜ë¡ ê¸‰ê²©í•œ ì„±ëŠ¥ ì €í•˜
```

**5. ë¬¸ì„œ êµ¬ì¡°ì˜ ì˜í–¥**

```
ë†€ë¼ìš´ ë°œê²¬:

êµ¬ì¡°í™”ëœ ì—ì„¸ì´ (coherent):
â””â”€ ì •í™•ë„: 55%

ë¬´ì‘ìœ„ ë¬¸ì¥ (shuffled):
â””â”€ ì •í™•ë„: 70%

â†’ êµ¬ì¡°í™”ëœ ë¬¸ì„œê°€ ì˜¤íˆë ¤ ë” ì–´ë ¤ì›€!
   (ëª¨ë¸ì´ ë‚´ëŸ¬í‹°ë¸Œë¥¼ ë”°ë¼ê°€ë‹¤ ì •ë³´ ë†“ì¹¨)
```

**6. ëª¨ë¸ë³„ ì°¨ì´**

```
ê±°ë¶€/í™˜ê° íŒ¨í„´:

GPT ì‹œë¦¬ì¦ˆ:
â””â”€ í™•ì‹  ìˆê²Œ ì˜ëª»ëœ ë‹µë³€ (í™˜ê°)

Claude ì‹œë¦¬ì¦ˆ:
â””â”€ ë¶ˆí™•ì‹¤í•  ë•Œ ë‹µë³€ ê±°ë¶€

â†’ ì•ˆì „ì„± vs ìœ ìš©ì„± íŠ¸ë ˆì´ë“œì˜¤í”„
```

### ì‹¤ì „ í•´ê²° ë°©ì•ˆ

#### 1. ì ì‘í˜• ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°

```python
class AdaptiveContextWindow:
    """ì‘ì—… ë³µì¡ë„ì— ë”°ë¥¸ ë™ì  ìœˆë„ìš° ì¡°ì ˆ"""
    
    def __init__(self):
        self.base_limits = {
            'simple': 4000,      # ê°„ë‹¨í•œ ê²€ìƒ‰
            'moderate': 16000,   # ì¼ë°˜ì  ì‘ì—…
            'complex': 64000,    # ë³µì¡í•œ ë¶„ì„
            'max': 128000        # ìµœëŒ€ ìš©ëŸ‰
        }
    
    def determine_optimal_window(self, task):
        """ìµœì  ìœˆë„ìš° í¬ê¸° ê²°ì •"""
        
        # ì‘ì—… ë³µì¡ë„ ë¶„ì„
        complexity = self.analyze_task_complexity(task)
        
        # ê¸°ë³¸ í¬ê¸° ì„ íƒ
        base_size = self.base_limits[complexity]
        
        # ì‹¤ì‹œê°„ ì¡°ì •
        # Context Rot ì§•í›„ê°€ ë³´ì´ë©´ ì¶•ì†Œ
        if self.detect_context_rot(task):
            return base_size * 0.7  # 30% ì¶•ì†Œ
        
        return base_size
    
    def detect_context_rot(self, task):
        """Context Rot ì§•í›„ ê°ì§€"""
        indicators = {
            'hallucination': self.check_hallucination(),
            'refusal': self.check_refusal_rate(),
            'latency': self.check_response_time(),
            'quality': self.check_output_quality()
        }
        
        # 2ê°œ ì´ìƒ ì§•í›„ â†’ Context Rot
        return sum(indicators.values()) >= 2
```

#### 2. ì „ëµì  ì •ë³´ ë°°ì¹˜

```python
class StrategicPlacement:
    """ì •ë³´ë¥¼ ì „ëµì  ìœ„ì¹˜ì— ë°°ì¹˜"""
    
    def optimize_placement(self, critical_info, supporting_docs):
        """
        U-Shaped Attentionì„ ê³ ë ¤í•œ ë°°ì¹˜
        """
        
        # í•µì‹¬ ì •ë³´: ì‹œì‘ê³¼ ë
        context = [
            "=== CRITICAL INFORMATION ===",
            critical_info,
            "=== CRITICAL INFORMATION END ===",
            "",
        ]
        
        # ë³´ì¡° ë¬¸ì„œ: ì¤‘ê°„
        context.extend(supporting_docs)
        
        # í•µì‹¬ ì •ë³´ ìš”ì•½: ë‹¤ì‹œ ëì—
        context.extend([
            "",
            "=== KEY POINTS RECAP ===",
            self.summarize(critical_info),
            "=== END ==="
        ])
        
        return "\n".join(context)
```

#### 3. Distractor í•„í„°ë§

```python
class DistractorFilter:
    """ë°©í•´ ìš”ì†Œ ì œê±°"""
    
    def remove_distractors(self, query, documents):
        """
        ì¿¼ë¦¬ì™€ ë¹„ìŠ·í•˜ì§€ë§Œ ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œ ì œê±°
        (ê°€ì¥ ìœ„í—˜í•œ ì¤‘ê°„ ìœ ì‚¬ë„ ë²”ìœ„)
        """
        
        query_embedding = get_embedding(query)
        
        filtered = []
        for doc in documents:
            doc_embedding = get_embedding(doc)
            similarity = cosine_similarity([query_embedding], [doc_embedding])[0][0]
            
            # ìœ„í—˜ êµ¬ê°„ (0.3-0.6) ì œì™¸
            if similarity < 0.3 or similarity > 0.6:
                filtered.append(doc)
            else:
                # ì¤‘ê°„ ìœ ì‚¬ë„: ì¶”ê°€ ê²€ì¦
                if self.verify_relevance(query, doc):
                    filtered.append(doc)
        
        return filtered
    
    def verify_relevance(self, query, document):
        """ì¶”ê°€ ê²€ì¦ (í‚¤ì›Œë“œ ê¸°ë°˜)"""
        query_keywords = extract_keywords(query)
        doc_keywords = extract_keywords(document)
        
        overlap = len(query_keywords & doc_keywords)
        return overlap >= 3  # ìµœì†Œ 3ê°œ í‚¤ì›Œë“œ ê³µí†µ
```

#### 4. ë¬¸ì„œ ì „ì²˜ë¦¬

```python
def preprocess_for_retrieval(document):
    """
    ì—°êµ¬ ê²°ê³¼: êµ¬ì¡°í™”ëœ ë¬¸ì„œê°€ ë” ì–´ë ¤ì›€
    â†’ ê²€ìƒ‰ ìµœì í™”ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬
    """
    
    # ê¸´ ë‚´ëŸ¬í‹°ë¸Œë¥¼ ë…ë¦½ì  ì²­í¬ë¡œ ë¶„í• 
    chunks = split_into_independent_chunks(document)
    
    # ê° ì²­í¬ì— ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    enriched_chunks = []
    for chunk in chunks:
        enriched = f"""
        [Document: {document.title}]
        [Section: {chunk.section}]
        
        {chunk.content}
        
        [End of section]
        """
        enriched_chunks.append(enriched)
    
    return enriched_chunks
```

#### 5. ê³„ì¸µì  ê²€ìƒ‰

```python
class HierarchicalRetrieval:
    """Context Rot ë°©ì§€ ê³„ì¸µì  ê²€ìƒ‰"""
    
    def retrieve(self, query, max_tokens=10000):
        """
        1ì°¨: ë¬¸ì„œ ë ˆë²¨ ê²€ìƒ‰ â†’ ìš”ì•½
        2ì°¨: ê´€ë ¨ ë¬¸ì„œ ë‚´ ì„¸ë¶€ ê²€ìƒ‰
        """
        
        # Phase 1: ë¬¸ì„œ ë ˆë²¨ (ìš”ì•½ë§Œ)
        doc_summaries = self.get_document_summaries(query, top_k=10)
        
        current_tokens = count_tokens('\n'.join(doc_summaries))
        
        if current_tokens > max_tokens:
            return doc_summaries[:5]  # ìš”ì•½ë§Œ ë°˜í™˜
        
        # Phase 2: ì„¸ë¶€ ë‚´ìš© (í•„ìš”ì‹œ)
        detailed_sections = []
        remaining_tokens = max_tokens - current_tokens
        
        for doc_summary in doc_summaries:
            if remaining_tokens < 1000:
                break
            
            # í•´ë‹¹ ë¬¸ì„œì˜ ê´€ë ¨ ì„¹ì…˜ë§Œ ê°€ì ¸ì˜¤ê¸°
            sections = self.get_relevant_sections(
                query, 
                doc_summary.doc_id,
                max_tokens=remaining_tokens
            )
            
            detailed_sections.extend(sections)
            remaining_tokens -= count_tokens('\n'.join(sections))
        
        return doc_summaries + detailed_sections
```

#### 6. ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

```python
class ContextRotMonitor:
    """Context Rot ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        self.metrics_history = []
    
    def monitor_session(self, context_size, response_quality, latency):
        """ì„¸ì…˜ ëª¨ë‹ˆí„°ë§"""
        
        metrics = {
            'timestamp': datetime.now(),
            'context_tokens': context_size,
            'quality_score': response_quality,
            'latency_ms': latency
        }
        
        self.metrics_history.append(metrics)
        
        # Context Rot ê°ì§€
        if self.is_rot_detected():
            self.trigger_alert()
    
    def is_rot_detected(self):
        """íŒ¨í„´ ë¶„ì„ìœ¼ë¡œ ê°ì§€"""
        
        if len(self.metrics_history) < 5:
            return False
        
        recent = self.metrics_history[-5:]
        
        # 1. í’ˆì§ˆ ì €í•˜ ì¶”ì„¸
        quality_trend = [m['quality_score'] for m in recent]
        if self.is_declining(quality_trend):
            return True
        
        # 2. ë ˆì´í„´ì‹œ ì¦ê°€
        latency_trend = [m['latency_ms'] for m in recent]
        if np.mean(latency_trend) > 5000:  # 5ì´ˆ ì´ìƒ
            return True
        
        # 3. ì»¨í…ìŠ¤íŠ¸ í¬ê¸° vs í’ˆì§ˆ ì—­ìƒê´€
        sizes = [m['context_tokens'] for m in recent]
        qualities = [m['quality_score'] for m in recent]
        correlation = np.corrcoef(sizes, qualities)[0, 1]
        
        if correlation < -0.7:  # ê°•í•œ ìŒì˜ ìƒê´€ê´€ê³„
            return True
        
        return False
    
    def trigger_alert(self):
        """ì•Œë¦¼ ë° ìë™ ì¡°ì¹˜"""
        logger.warning("Context Rot detected!")
        
        # ìë™ ì¡°ì¹˜
        self.reduce_context_size()
        self.clear_cache()
        self.switch_to_summarization_mode()
```

### ğŸ“Š ì‹¤ì „ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼

```
í…ŒìŠ¤íŠ¸: ê³ ê° ì§€ì› ì±—ë´‡ (1000ê°œ ëŒ€í™”)

Before ìµœì í™”:
â”œâ”€ í‰ê·  ì»¨í…ìŠ¤íŠ¸: 85K í† í°
â”œâ”€ ì •í™•ë„: 62%
â”œâ”€ í‰ê·  ì‘ë‹µ ì‹œê°„: 8.5ì´ˆ
â””â”€ ì›” ë¹„ìš©: $2,400

After ìµœì í™” (Context Engineering ì ìš©):
â”œâ”€ í‰ê·  ì»¨í…ìŠ¤íŠ¸: 12K í† í° (â†“ 86%)
â”œâ”€ ì •í™•ë„: 89% (â†‘ 27%p)
â”œâ”€ í‰ê·  ì‘ë‹µ ì‹œê°„: 2.1ì´ˆ (â†“ 75%)
â””â”€ ì›” ë¹„ìš©: $380 (â†“ 84%)

ì ìš© ê¸°ë²•:
âœ… Hierarchical Memory (3ê³„ì¸µ)
âœ… Adaptive Window Sizing
âœ… Distractor Filtering  
âœ… Strategic Placement
âœ… Real-time Monitoring
```

---

## ì‹¤ì „ êµ¬í˜„ ê°€ì´ë“œ

### ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```python
"""
í”„ë¡œë•ì…˜ê¸‰ ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì‹œìŠ¤í…œ
"""

import anthropic
from typing import List, Dict, Optional
import numpy as np
from datetime import datetime

class ProductionContextEngine:
    """
    Agent-Skills ì›ì¹™ì„ ì ìš©í•œ 
    í”„ë¡œë•ì…˜ ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì—”ì§„
    """
    
    def __init__(self, api_key: str):
        # Core components
        self.client = anthropic.Anthropic(api_key=api_key)
        
        # Memory system (Agent-Skills: memory-systems)
        self.memory = HierarchicalMemory()
        
        # Context optimization (Agent-Skills: context-optimization)
        self.optimizer = ContextOptimizationPipeline()
        
        # Context monitoring (Agent-Skills: context-degradation)
        self.monitor = ContextRotMonitor()
        
        # Evaluation (Agent-Skills: evaluation)
        self.evaluator = LLMJudge()
        
        # Configuration
        self.config = {
            'max_context_tokens': 100000,
            'target_context_tokens': 10000,
            'enable_caching': True,
            'enable_monitoring': True
        }
    
    async def process_query(
        self,
        user_query: str,
        documents: Optional[List[str]] = None,
        conversation_history: Optional[List[Dict]] = None,
        task_complexity: str = 'moderate'
    ) -> Dict:
        """
        ì¿¼ë¦¬ ì²˜ë¦¬ ë©”ì¸ í”Œë¡œìš°
        
        Args:
            user_query: ì‚¬ìš©ì ì§ˆì˜
            documents: ì°¸ì¡° ë¬¸ì„œë“¤ (optional)
            conversation_history: ëŒ€í™” ì´ë ¥ (optional)
            task_complexity: ì‘ì—… ë³µì¡ë„ ('simple'|'moderate'|'complex')
        
        Returns:
            {
                'response': ì‘ë‹µ í…ìŠ¤íŠ¸,
                'metrics': ì„±ëŠ¥ ë©”íŠ¸ë¦­,
                'context_used': ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸
            }
        """
        
        start_time = datetime.now()
        
        # Phase 1: Context ì¤€ë¹„
        print("ğŸ“Š Phase 1: Context Preparation")
        optimized_context = await self._prepare_context(
            user_query,
            documents or [],
            conversation_history or [],
            task_complexity
        )
        
        # Phase 2: API í˜¸ì¶œ
        print("ğŸ¤– Phase 2: Claude API Call")
        response = await self._call_claude_api(optimized_context)
        
        # Phase 3: í‰ê°€ ë° ëª¨ë‹ˆí„°ë§
        print("ğŸ“ˆ Phase 3: Evaluation & Monitoring")
        metrics = self._collect_metrics(
            user_query,
            optimized_context,
            response,
            start_time
        )
        
        # Phase 4: ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸
        print("ğŸ’¾ Phase 4: Memory Update")
        self._update_memory(user_query, response)
        
        return {
            'response': response,
            'metrics': metrics,
            'context_used': optimized_context
        }
    
    async def _prepare_context(
        self,
        query: str,
        documents: List[str],
        history: List[Dict],
        complexity: str
    ) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (ìµœì í™” ì ìš©)"""
        
        # 1. ì ì‘í˜• ìœˆë„ìš° í¬ê¸° ê²°ì •
        window_manager = AdaptiveContextWindow()
        max_tokens = window_manager.determine_optimal_window({
            'query': query,
            'complexity': complexity
        })
        
        print(f"  â””â”€ Target context size: {max_tokens:,} tokens")
        
        # 2. ê´€ë ¨ ë©”ëª¨ë¦¬ ê²€ìƒ‰
        memory_context = self.memory.retrieve(query, max_items=3)
        
        # 3. ë¬¸ì„œ ìµœì í™”
        if documents:
            documents = self.optimizer.optimize(
                query=query,
                raw_documents=documents,
                max_tokens=int(max_tokens * 0.6)  # 60% for documents
            )
        
        # 4. ëŒ€í™” ì´ë ¥ ì••ì¶•
        if history:
            compressor = ConversationCompressor()
            history = compressor.compress_long_conversation(history)
        
        # 5. ê³„ì¸µì  ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self._build_hierarchical_context(
            query=query,
            documents=documents,
            memory=memory_context,
            history=history
        )
        
        # 6. ìµœì¢… í† í° ê²€ì¦
        actual_tokens = count_tokens(context)
        if actual_tokens > max_tokens:
            print(f"  âš ï¸  Context too large ({actual_tokens:,} > {max_tokens:,}), compressing...")
            context = self._emergency_compression(context, max_tokens)
        
        print(f"  âœ… Final context: {count_tokens(context):,} tokens")
        
        return context
    
    def _build_hierarchical_context(
        self,
        query: str,
        documents: List[str],
        memory: List[tuple],
        history: List[Dict]
    ) -> str:
        """
        ê³„ì¸µì  ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        (Agent-Skills: context-fundamentals ì›ì¹™ ì ìš©)
        """
        
        sections = []
        
        # Level 1: í•µì‹¬ ì‘ì—… ì •ë³´ (ìµœìš°ì„ )
        sections.append(f"""
# PRIMARY OBJECTIVE
User Query: {query}

Task: Provide accurate, helpful response based on available context.
""")
        
        # Level 2: ê´€ë ¨ ë©”ëª¨ë¦¬ (ë†’ì€ ìš°ì„ ìˆœìœ„)
        if memory:
            memory_text = "\n".join([
                f"- [{source}] {content}"
                for source, content in memory
            ])
            sections.append(f"""
# RELEVANT MEMORY
{memory_text}
""")
        
        # Level 3: ì°¸ì¡° ë¬¸ì„œ (ì¤‘ê°„ ìš°ì„ ìˆœìœ„)
        if documents:
            # Strategic Placement: í•µì‹¬ ë¬¸ì„œë¥¼ ì‹œì‘ê³¼ ëì—
            doc_text = "\n\n".join([
                f"## Document {i+1}\n{doc}"
                for i, doc in enumerate(documents)
            ])
            sections.append(f"""
# REFERENCE DOCUMENTS
{doc_text}
""")
        
        # Level 4: ëŒ€í™” ì´ë ¥ (ë³´ì¡° ì •ë³´)
        if history:
            history_text = "\n".join([
                f"{msg['role']}: {msg['content'][:200]}..."
                if len(msg['content']) > 200
                else f"{msg['role']}: {msg['content']}"
                for msg in history
            ])
            sections.append(f"""
# CONVERSATION HISTORY
{history_text}
""")
        
        return "\n\n".join(sections)
    
    async def _call_claude_api(self, context: str) -> str:
        """Claude API í˜¸ì¶œ (ìºì‹± ì§€ì›)"""
        
        try:
            if self.config['enable_caching']:
                # Prompt Caching í™œìš©
                message = self.client.messages.create(
                    model="claude-sonnet-4-20250514",
                    max_tokens=4096,
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": context,
                                    "cache_control": {"type": "ephemeral"}
                                }
                            ]
                        }
                    ]
                )
            else:
                message = self.client.messages.create(
                    model="claude-sonnet-4-20250514",
                    max_tokens=4096,
                    messages=[{"role": "user", "content": context}]
                )
            
            return message.content[0].text
            
        except Exception as e:
            print(f"âŒ API Error: {e}")
            return f"Error occurred: {str(e)}"
    
    def _collect_metrics(
        self,
        query: str,
        context: str,
        response: str,
        start_time: datetime
    ) -> Dict:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        
        metrics = {
            'query_tokens': count_tokens(query),
            'context_tokens': count_tokens(context),
            'response_tokens': count_tokens(response),
            'total_tokens': count_tokens(query + context + response),
            'latency_ms': (datetime.now() - start_time).total_seconds() * 1000
        }
        
        # Context Rot ëª¨ë‹ˆí„°ë§
        if self.config['enable_monitoring']:
            self.monitor.monitor_session(
                context_size=metrics['context_tokens'],
                response_quality=0.8,  # TODO: ì‹¤ì œ í’ˆì§ˆ í‰ê°€
                latency=metrics['latency_ms']
            )
        
        return metrics
    
    def _update_memory(self, query: str, response: str):
        """ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸"""
        
        # ì¤‘ìš”í•œ ì •ë³´ë§Œ ë©”ëª¨ë¦¬ì— ì €ì¥
        if self._is_important_exchange(query, response):
            self.memory.add(
                f"Q: {query}\nA: {response[:500]}...",
                importance='high'
            )
    
    def _is_important_exchange(self, query: str, response: str) -> bool:
        """ì¤‘ìš”í•œ êµí™˜ì¸ì§€ íŒë‹¨"""
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: ì¶©ë¶„íˆ ê¸´ ì‘ë‹µ
        return len(response) > 200
    
    def _emergency_compression(self, context: str, max_tokens: int) -> str:
        """ê¸´ê¸‰ ì••ì¶•"""
        compressor = ContextCompactor()
        return compressor.compress_to_fit(context, max_tokens)


# ============================================
# ì‚¬ìš© ì˜ˆì‹œ
# ============================================

async def main():
    """ì‹¤ì „ ì‚¬ìš© ì˜ˆì œ"""
    
    # ì—”ì§„ ì´ˆê¸°í™”
    engine = ProductionContextEngine(api_key="your-api-key")
    
    # ì‹œë‚˜ë¦¬ì˜¤: ê¸°ìˆ  ë¬¸ì„œ ë¶„ì„
    documents = [
        "Python asyncioëŠ” ë¹„ë™ê¸° I/O í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤...",
        "FastAPIëŠ” Python ì›¹ í”„ë ˆì„ì›Œí¬ë¡œ...",
        # ... 100ê°œì˜ ë¬¸ì„œ
    ]
    
    # ì¿¼ë¦¬ ì²˜ë¦¬
    result = await engine.process_query(
        user_query="Pythonìœ¼ë¡œ ë¹„ë™ê¸° ì›¹ APIë¥¼ ë§Œë“œëŠ” ë°©ë²•ì€?",
        documents=documents,
        task_complexity='moderate'
    )
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ¯ Response:")
    print("="*60)
    print(result['response'])
    
    print("\n" + "="*60)
    print("ğŸ“Š Metrics:")
    print("="*60)
    for key, value in result['metrics'].items():
        print(f"  {key}: {value:,}" if isinstance(value, int) else f"  {key}: {value}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

---

## ì»¨í…ìŠ¤íŠ¸ ìµœì í™” íŒ¨í„´

### íŒ¨í„´ 1: Progressive Disclosure (ì ì§„ì  ë…¸ì¶œ)

```python
class ProgressiveDisclosure:
    """í•„ìš”í•  ë•Œë§Œ ì •ë³´ ë¡œë“œ"""
    
    def __init__(self):
        self.skill_metadata = self.load_all_metadata()
        self.loaded_skills = {}
    
    def load_all_metadata(self):
        """ì‹œì‘ì‹œ ë©”íƒ€ë°ì´í„°ë§Œ ë¡œë“œ"""
        # ê° ìŠ¤í‚¬ë‹¹ ìˆ˜ì‹­ í† í°ë§Œ
        return {
            'pptx': {
                'name': 'PowerPoint Creation',
                'description': 'Create professional presentations',
                'tokens': 45
            },
            'xlsx': {
                'name': 'Excel Manipulation',
                'description': 'Work with spreadsheets',
                'tokens': 38
            },
            # ... 100ê°œ ìŠ¤í‚¬
        }
    
    def activate_skill(self, skill_name):
        """í•„ìš”í•  ë•Œ ì „ì²´ ìŠ¤í‚¬ ë¡œë“œ"""
        if skill_name not in self.loaded_skills:
            # ì „ì²´ ë‚´ìš© ë¡œë“œ (ìˆ˜ì²œ~ìˆ˜ë§Œ í† í°)
            self.loaded_skills[skill_name] = self.load_full_skill(skill_name)
        
        return self.loaded_skills[skill_name]

# íš¨ê³¼:
# 100ê°œ ìŠ¤í‚¬ Ã— 45 í† í° = 4,500 í† í° (ë©”íƒ€ë°ì´í„°)
# vs
# 100ê°œ ìŠ¤í‚¬ Ã— 5,000 í† í° = 500,000 í† í° (ì „ì²´)
# â†’ 99% í† í° ì ˆê°!
```

### íŒ¨í„´ 2: Semantic Chunking (ì˜ë¯¸ë¡ ì  ì²­í‚¹)

```python
def semantic_chunking(document, max_chunk_tokens=1000):
    """ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í¬ ë¶„í• """
    
    # 1. ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í• 
    paragraphs = document.split('\n\n')
    
    # 2. ì˜ë¯¸ì ìœ¼ë¡œ ì—°ê´€ëœ ë¬¸ë‹¨ ê·¸ë£¹í™”
    chunks = []
    current_chunk = []
    current_tokens = 0
    
    for para in paragraphs:
        para_tokens = count_tokens(para)
        
        if current_tokens + para_tokens > max_chunk_tokens:
            # í˜„ì¬ ì²­í¬ ì™„ì„±
            chunks.append('\n\n'.join(current_chunk))
            current_chunk = [para]
            current_tokens = para_tokens
        else:
            # í˜„ì¬ ì²­í¬ì— ì¶”ê°€
            current_chunk.append(para)
            current_tokens += para_tokens
    
    # ë§ˆì§€ë§‰ ì²­í¬
    if current_chunk:
        chunks.append('\n\n'.join(current_chunk))
    
    return chunks
```

### íŒ¨í„´ 3: Dynamic Context Pruning (ë™ì  ê°€ì§€ì¹˜ê¸°)

```python
class DynamicPruner:
    """ì‹¤ì‹œê°„ ì»¨í…ìŠ¤íŠ¸ ê°€ì§€ì¹˜ê¸°"""
    
    def prune_during_generation(self, context, current_response):
        """
        ì‘ë‹µ ìƒì„± ì¤‘ ë¶ˆí•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ ì œê±°
        """
        
        # ì´ë¯¸ ì‚¬ìš©ëœ ì •ë³´ ì‹ë³„
        used_info = self.extract_used_information(context, current_response)
        
        # ì‚¬ìš©ë˜ì§€ ì•Šì€ ë¶€ë¶„ ì œê±°
        pruned = self.remove_unused_sections(context, used_info)
        
        # í† í° ì˜ˆì‚°ì— ë§ê²Œ ì¡°ì •
        return self.fit_to_budget(pruned, max_tokens=10000)
```

---

## í”„ë¡œë•ì…˜ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### ğŸ¯ ì²´í¬ë¦¬ìŠ¤íŠ¸

```markdown
âœ… ì„¤ê³„ ë‹¨ê³„
[ ] ì‘ì—… ë³µì¡ë„ë³„ ì»¨í…ìŠ¤íŠ¸ ì „ëµ ìˆ˜ë¦½
[ ] ë©”ëª¨ë¦¬ ê³„ì¸µ êµ¬ì¡° ì„¤ê³„
[ ] ìºì‹± ì „ëµ ê³„íš
[ ] ëª¨ë‹ˆí„°ë§ ì§€í‘œ ì •ì˜

âœ… êµ¬í˜„ ë‹¨ê³„
[ ] Progressive Disclosure ì ìš©
[ ] Adaptive Window Sizing êµ¬í˜„
[ ] Distractor Filtering ì ìš©
[ ] Strategic Placement êµ¬í˜„
[ ] Emergency Compression ì¤€ë¹„

âœ… í…ŒìŠ¤íŠ¸ ë‹¨ê³„
[ ] ë‹¤ì–‘í•œ ì»¨í…ìŠ¤íŠ¸ í¬ê¸° í…ŒìŠ¤íŠ¸
[ ] Context Rot ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
[ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìˆ˜í–‰
[ ] ë¹„ìš© ë¶„ì„

âœ… ëª¨ë‹ˆí„°ë§ ë‹¨ê³„
[ ] í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
[ ] í’ˆì§ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
[ ] Context Rot ê°ì§€ ì‹œìŠ¤í…œ
[ ] ì•Œë¦¼ ì„¤ì •

âœ… ìµœì í™” ë‹¨ê³„
[ ] A/B í…ŒìŠ¤íŠ¸
[ ] ì»¨í…ìŠ¤íŠ¸ ì „ëµ ì¡°ì •
[ ] ìºì‹œ íˆíŠ¸ìœ¨ ê°œì„ 
[ ] ë¹„ìš© ìµœì í™”
```

### ğŸ“Š ì„±ëŠ¥ ëª©í‘œ

```
ê¸°ì¤€ ë©”íŠ¸ë¦­:

í† í° íš¨ìœ¨ì„±:
â”œâ”€ ëª©í‘œ: ì´ í† í° < 20K
â”œâ”€ ìš°ìˆ˜: < 10K
â””â”€ íƒì›”: < 5K

ì‘ë‹µ í’ˆì§ˆ:
â”œâ”€ ëª©í‘œ: ì •í™•ë„ > 80%
â”œâ”€ ìš°ìˆ˜: > 90%
â””â”€ íƒì›”: > 95%

ì‘ë‹µ ì†ë„:
â”œâ”€ ëª©í‘œ: < 3ì´ˆ
â”œâ”€ ìš°ìˆ˜: < 2ì´ˆ
â””â”€ íƒì›”: < 1ì´ˆ

ë¹„ìš© íš¨ìœ¨:
â”œâ”€ ëª©í‘œ: ì›” ë¹„ìš© < $1,000
â”œâ”€ ìš°ìˆ˜: < $500
â””â”€ íƒì›”: < $200
```

### ğŸ”§ ë””ë²„ê¹… ê°€ì´ë“œ

```python
class ContextDebugger:
    """ì»¨í…ìŠ¤íŠ¸ ë¬¸ì œ ë””ë²„ê¹…"""
    
    def diagnose(self, context, response):
        """ì¢…í•© ì§„ë‹¨"""
        
        issues = []
        
        # 1. í¬ê¸° í™•ì¸
        tokens = count_tokens(context)
        if tokens > 50000:
            issues.append({
                'type': 'SIZE_WARNING',
                'severity': 'high',
                'message': f'Context too large: {tokens:,} tokens',
                'suggestion': 'Apply compression or filtering'
            })
        
        # 2. ì •ë³´ ë°€ë„ í™•ì¸
        density = self.calculate_information_density(context, response)
        if density < 0.3:
            issues.append({
                'type': 'LOW_DENSITY',
                'severity': 'medium',
                'message': f'Low information density: {density:.2%}',
                'suggestion': 'Remove irrelevant context'
            })
        
        # 3. ì¤‘ë³µ í™•ì¸
        duplicates = self.find_duplicates(context)
        if duplicates:
            issues.append({
                'type': 'DUPLICATES',
                'severity': 'low',
                'message': f'Found {len(duplicates)} duplicate sections',
                'suggestion': 'Deduplicate content'
            })
        
        return issues
```

---

## ì°¸ê³  ìë£Œ

### ğŸ”— GitHub ë¦¬í¬ì§€í† ë¦¬

1. **Agent-Skills for Context Engineering**
   - https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering
   - 7ê°€ì§€ í•µì‹¬ ìŠ¤í‚¬ + ì‹¤ì „ ì˜ˆì œ

2. **Anthropic Skills (ê³µì‹)**
   - https://github.com/anthropics/skills
   - ê³µì‹ ìŠ¤í‚¬ ì˜ˆì œ ë° ë¬¸ì„œ

3. **LangChain Context Engineering**
   - https://github.com/langchain-ai/context_engineering
   - 4ëŒ€ ì „ëµ ë…¸íŠ¸ë¶

### ğŸ“š ì—°êµ¬ ë…¼ë¬¸

1. **Context Rot (Chroma Research, 2025)**
   - https://research.trychroma.com/context-rot
   - 18ê°œ ëª¨ë¸ í‰ê°€, ì‹¤í—˜ ì½”ë“œ ê³µê°œ

2. **Lost in the Middle (Liu et al., 2023)**
   - ìœ„ì¹˜ í¸í–¥ í˜„ìƒ ìµœì´ˆ ë³´ê³ 
   - U-Shaped Attention Curve

3. **Large Language Models Can Be Easily Distracted (Shi et al., 2023)**
   - Distractor íš¨ê³¼ ì—°êµ¬

### ğŸ“ ê³µì‹ ë¬¸ì„œ

1. **Anthropic Claude Documentation**
   - https://docs.anthropic.com/
   - Agent Skills ê°€ì´ë“œ

2. **Anthropic Prompt Engineering**
   - https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview

3. **Agent Skills í‘œì¤€ (Open Standard)**
   - https://agentskills.io/
   - í¬ë¡œìŠ¤ í”Œë«í¼ í˜¸í™˜ ëª…ì„¸

### ğŸ› ï¸ ë„êµ¬ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬

```python
# í† í° ì¹´ìš´íŒ…
pip install tiktoken

# ì„ë² ë”© & ë²¡í„° ê²€ìƒ‰
pip install chromadb
pip install pinecone-client

# LangChain
pip install langchain
pip install langchain-anthropic

# ë°ì´í„° ì²˜ë¦¬
pip install numpy pandas scikit-learn

# ëª¨ë‹ˆí„°ë§
pip install prometheus-client
```

### ğŸ“– ì¶”ì²œ í•™ìŠµ ê²½ë¡œ

```
ğŸŒ± ì´ˆê¸‰ (1-2ì£¼)
â”œâ”€ Agent-Skills README ì½ê¸°
â”œâ”€ Context Fundamentals í•™ìŠµ
â”œâ”€ Anthropic ê³µì‹ ë¬¸ì„œ í›‘ê¸°
â””â”€ ê°„ë‹¨í•œ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ êµ¬í˜„

ğŸŒ¿ ì¤‘ê¸‰ (2-4ì£¼)
â”œâ”€ Context Rot ë…¼ë¬¸ ì½ê¸°
â”œâ”€ 4ëŒ€ ì „ëµ (Write/Select/Compress/Isolate) ì ìš©
â”œâ”€ ê³„ì¸µì  ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ êµ¬í˜„
â””â”€ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìˆ˜í–‰

ğŸŒ³ ê³ ê¸‰ (1-3ê°œì›”)
â”œâ”€ Knowledge Graph í†µí•©
â”œâ”€ ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ êµ¬ì¶•
â”œâ”€ í”„ë¡œë•ì…˜ ìµœì í™”
â””â”€ ì»¤ìŠ¤í…€ í‰ê°€ ì‹œìŠ¤í…œ ê°œë°œ
```

### ğŸ’¬ ì»¤ë®¤ë‹ˆí‹°

- **Reddit**: r/ClaudeAI
- **Discord**: Anthropic Community
- **Twitter/X**: #ContextEngineering
- **Hacker News**: agent skills ê²€ìƒ‰

### ğŸ“º ë™ì˜ìƒ ìë£Œ

- Andrej Karpathy: "Software Is Changing" (Y Combinator)
- Anthropic: "Agent Skills Introduction"
- LangChain: "Context Engineering Strategies"

---

## ë§ˆì¹˜ë©°

### ğŸ¯ í•µì‹¬ ìš”ì•½

**1. ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ = AI ì—ì´ì „íŠ¸ì˜ í•µì‹¬ ì—­ëŸ‰**

```
"Context engineering is effectively the #1 job 
of engineers building AI agents."
```

**2. Agent-Skills â‰  Skill-Creator**

```
Agent-Skills:     ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ì›ì¹™ê³¼ íŒ¨í„´
Skill-Creator:    Claudeìš© ìŠ¤í‚¬ ìƒì„± í”„ë ˆì„ì›Œí¬

â†’ ìƒí˜¸ ë³´ì™„ì , í•¨ê»˜ ì‚¬ìš© ê¶Œì¥
```

**3. Context RotëŠ” ì‹¤ì¬í•˜ëŠ” ë¬¸ì œ**

```
ì—°êµ¬ ê²°ê³¼ (2025):
- 18ê°œ SOTA ëª¨ë¸ ëª¨ë‘ ì˜í–¥ ë°›ìŒ
- ê°„ë‹¨í•œ ì‘ì—…ì—ì„œë„ ì„±ëŠ¥ ì €í•˜
- ì»¨í…ìŠ¤íŠ¸ í¬ê¸° â†‘ â‰  ì„±ëŠ¥ â†‘
```

**4. 4ëŒ€ ì „ëµ (Write, Select, Compress, Isolate)**

```
Write:      ì»¨í…ìŠ¤íŠ¸ ì™¸ë¶€ ì €ì¥
Select:     ê´€ë ¨ ì •ë³´ë§Œ ë¡œë“œ
Compress:   í† í° íš¨ìœ¨ ê·¹ëŒ€í™”
Isolate:    ì»¨í…ìŠ¤íŠ¸ ë¶„ë¦¬
```

**5. ì‹¤ì „ ì ìš©ì´ í•µì‹¬**

```
ì´ë¡ ë§Œìœ¼ë¡œëŠ” ë¶€ì¡±
â†’ ë²¤ì¹˜ë§ˆí¬, ì¸¡ì •, ìµœì í™”
â†’ ì§€ì†ì  ëª¨ë‹ˆí„°ë§
â†’ í”¼ë“œë°± ê¸°ë°˜ ê°œì„ 
```

### ğŸš€ ë‹¤ìŒ ë‹¨ê³„

```markdown
ğŸ“… 1ì£¼ì°¨: ê¸°ì´ˆ ë‹¤ì§€ê¸°
- [ ] context-fundamentals ìŠ¤í‚¬ ì •ë…
- [ ] ê°„ë‹¨í•œ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì êµ¬í˜„
- [ ] í† í° ì¹´ìš´íŒ… ì‹œìŠ¤í…œ êµ¬ì¶•

ğŸ“… 2ì£¼ì°¨: ìµœì í™” ì ìš©
- [ ] Adaptive Window Sizing êµ¬í˜„
- [ ] Distractor Filtering í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìˆ˜í–‰

ğŸ“… 3ì£¼ì°¨: ê³ ê¸‰ ê¸°ëŠ¥
- [ ] Hierarchical Memory êµ¬í˜„
- [ ] Multi-Agent íŒ¨í„´ ì‹¤í—˜
- [ ] Context Rot ëª¨ë‹ˆí„°ë§

ğŸ“… 4ì£¼ì°¨: í”„ë¡œë•ì…˜ ì¤€ë¹„
- [ ] ì „ì²´ ì‹œìŠ¤í…œ í†µí•©
- [ ] A/B í…ŒìŠ¤íŠ¸
- [ ] ë¹„ìš© ìµœì í™”
- [ ] ë¬¸ì„œí™”
```

### ğŸ’¡ ë§ˆì§€ë§‰ ì¡°ì–¸

> "ë” í° ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ê°€ í•´ê²°ì±…ì´ ì•„ë‹™ë‹ˆë‹¤.  
> ë” ìŠ¤ë§ˆíŠ¸í•œ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ê°€ í•´ê²°ì±…ì…ë‹ˆë‹¤."

**ì„±ê³µì˜ ì—´ì‡ :**
1. ì¸¡ì •í•˜ì§€ ì•Šìœ¼ë©´ ê°œì„ í•  ìˆ˜ ì—†ë‹¤
2. ì‘ê²Œ ì‹œì‘í•´ì„œ ì ì§„ì ìœ¼ë¡œ í™•ì¥
3. ì‹¤í—˜í•˜ê³ , ì‹¤íŒ¨í•˜ê³ , ë°°ìš°ê³ , ë°˜ë³µ
4. ì»¤ë®¤ë‹ˆí‹°ì™€ ê³µìœ í•˜ê³  ë°°ìš°ê¸°

í–‰ìš´ì„ ë¹•ë‹ˆë‹¤! ğŸ‰

---

**ë¬¸ì„œ ì •ë³´**
- ì‘ì„±ì: Agent Skills ê°œë°œì ê°€ì´ë“œ
- ìµœì¢… ì—…ë°ì´íŠ¸: 2025-12-25
- ë²„ì „: 1.0.0
- ë¼ì´ì„ ìŠ¤: MIT

**ì°¸ê³ í•œ ì£¼ìš” ìë£Œ:**
- Agent-Skills for Context Engineering (Muratcan Koylan)
- Context Rot Research (Chroma, 2025)
- Anthropic Agent Skills Documentation
- LangChain Context Engineering Strategies

---

**ì‘ì„± ì¼ì**: 2025-12-25
