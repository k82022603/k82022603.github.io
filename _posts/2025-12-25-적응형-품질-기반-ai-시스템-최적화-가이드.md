---
title: "ì ì‘í˜• í’ˆì§ˆ ê¸°ë°˜ AI ì‹œìŠ¤í…œ ìµœì í™” ê°€ì´ë“œ"
date: 2025-12-25
categories: [AI,  Context Engineering]
mermaid: [True]
tags: [AI,  context-engineering,  Claude.write]
---


## ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [í•µì‹¬ ê°œë…: ì ì‘í˜• í’ˆì§ˆì´ë€?](#í•µì‹¬-ê°œë…-ì ì‘í˜•-í’ˆì§ˆì´ë€)
3. [ì¿¼ë¦¬ ë¶„ë¥˜ ì „ëµ](#ì¿¼ë¦¬-ë¶„ë¥˜-ì „ëµ)
4. [3ë‹¨ê³„ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸](#3ë‹¨ê³„-ì²˜ë¦¬-íŒŒì´í”„ë¼ì¸)
5. [ì‹¤ì „ êµ¬í˜„ ê°€ì´ë“œ](#ì‹¤ì „-êµ¬í˜„-ê°€ì´ë“œ)
6. [ì„±ëŠ¥ ìµœì í™” ë° ë¹„ìš© ì ˆê°](#ì„±ëŠ¥-ìµœì í™”-ë°-ë¹„ìš©-ì ˆê°)
7. [ëª¨ë‹ˆí„°ë§ ë° ê°œì„ ](#ëª¨ë‹ˆí„°ë§-ë°-ê°œì„ )
8. [ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤](#ë² ìŠ¤íŠ¸-í”„ë™í‹°ìŠ¤)

---

## ê°œìš”

### ë¬¸ì œ ìƒí™©

ë§ì€ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ëª¨ë“  ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤. ê°„ë‹¨í•œ "ì•ˆë…•í•˜ì„¸ìš”"ë¶€í„° ë³µì¡í•œ ê¸°ìˆ  ì§€ì› ìš”ì²­ê¹Œì§€, ëª¨ë‘ ê°™ì€ LLM APIë¥¼ í˜¸ì¶œí•˜ê³ , ê°™ì€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•˜ê³ , ê°™ì€ ë¹„ìš©ì„ ì§€ë¶ˆí•©ë‹ˆë‹¤.

**ì´ê²ƒì€ ë¹„íš¨ìœ¨ì ì…ë‹ˆë‹¤.**

ì‹¤ì œ ë°ì´í„°ë¥¼ ë¶„ì„í•´ë³´ë©´:
- **80%ì˜ ì¿¼ë¦¬**ëŠ” ë‹¨ìˆœí•©ë‹ˆë‹¤ (ì¸ì‚¬, FAQ, ê°„ë‹¨í•œ ì •ë³´ ìš”ì²­)
- **15%ì˜ ì¿¼ë¦¬**ëŠ” ì¤‘ê°„ ë³µì¡ë„ì…ë‹ˆë‹¤ (ë¬¸ì„œ ê²€ìƒ‰, ê¸°ë³¸ ë¶„ì„)
- **5%ì˜ ì¿¼ë¦¬**ë§Œ ì§„ì§œ ë³µì¡í•©ë‹ˆë‹¤ (ì‹¬ì¸µ ë¶„ì„, ì°½ì˜ì  ë¬¸ì œ í•´ê²°)

í•˜ì§€ë§Œ ëª¨ë“  ì¿¼ë¦¬ì— ë™ì¼í•œ ë¦¬ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•˜ë©´:
```
ğŸ’° ë¹„ìš© ë‚­ë¹„: 80%ì˜ ë‹¨ìˆœ ì¿¼ë¦¬ê°€ ë¶ˆí•„ìš”í•œ API ë¹„ìš© ë°œìƒ
â±ï¸ ì‘ë‹µ ì§€ì—°: ê°„ë‹¨í•œ ì§ˆë¬¸ë„ LLM ì²˜ë¦¬ ëŒ€ê¸°
ğŸ“‰ ë¦¬ì†ŒìŠ¤ ë¶€ì¡±: ë³µì¡í•œ ì¿¼ë¦¬ì— ì¶©ë¶„í•œ ì»´í“¨íŒ… í• ë‹¹ ì–´ë ¤ì›€
```

### í•´ê²°ì±…: ì ì‘í˜• í’ˆì§ˆ (Adaptive Quality)

**ì ì‘í˜• í’ˆì§ˆ**ì€ ì¿¼ë¦¬ì˜ ë³µì¡ë„ì— ë”°ë¼ ì²˜ë¦¬ ë°©ì‹ì„ ìë™ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” ì „ëµì…ë‹ˆë‹¤.

í•µì‹¬ ì›ì¹™:
> "ëª¨ë“  ì¿¼ë¦¬ê°€ ìµœê³  í’ˆì§ˆì„ í•„ìš”ë¡œ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  
> ê° ì¿¼ë¦¬ì— ì ì ˆí•œ ìˆ˜ì¤€ì˜ ì²˜ë¦¬ë¥¼ ì œê³µí•˜ì‹­ì‹œì˜¤."

---

## í•µì‹¬ ê°œë…: ì ì‘í˜• í’ˆì§ˆì´ë€?

### ì •ì˜

ì ì‘í˜• í’ˆì§ˆì€ ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë³µì¡ë„ì— ë”°ë¼ ë¶„ë¥˜í•˜ê³ , ê° ì¹´í…Œê³ ë¦¬ì— ìµœì í™”ëœ ì²˜ë¦¬ ë°©ì‹ì„ ì ìš©í•˜ëŠ” ì‹œìŠ¤í…œ ì„¤ê³„ íŒ¨í„´ì…ë‹ˆë‹¤.

### 3ê°€ì§€ í’ˆì§ˆ ë ˆë²¨

#### Level 1: ê²½ëŸ‰ ì²˜ë¦¬ (Lightweight)
**ëŒ€ìƒ ì¿¼ë¦¬**: ì¸ì‚¬, ë‹¨ìˆœ í™•ì¸, ë°˜ë³µ ìš”ì²­

**ì²˜ë¦¬ ë°©ì‹**:
- ì‚¬ì „ ì •ì˜ëœ í…œí”Œë¦¿ ì‘ë‹µ
- ê·œì¹™ ê¸°ë°˜ íŒ¨í„´ ë§¤ì¹­
- LLM í˜¸ì¶œ ì—†ìŒ

**ì˜ˆì‹œ**:
- "ì•ˆë…•í•˜ì„¸ìš”" â†’ "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
- "ê°ì‚¬í•©ë‹ˆë‹¤" â†’ "ì²œë§Œì—ìš”! ë” í•„ìš”í•˜ì‹  ê²ƒì´ ìˆìœ¼ì‹ ê°€ìš”?"
- "ì¢…ë£Œ" â†’ "ì´ìš©í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤."

**íŠ¹ì§•**:
- âš¡ ì‘ë‹µ ì‹œê°„: < 10ms
- ğŸ’° ë¹„ìš©: $0
- ğŸ¯ ì •í™•ë„: 100% (í…œí”Œë¦¿ ê¸°ë°˜)

#### Level 2: ì¤‘ê°„ ì²˜ë¦¬ (Medium)
**ëŒ€ìƒ ì¿¼ë¦¬**: FAQ, ë¬¸ì„œ ê²€ìƒ‰, ê°„ë‹¨í•œ ì •ë³´ ìš”ì²­

**ì²˜ë¦¬ ë°©ì‹**:
- ë²¡í„° ê²€ìƒ‰ (RAG)
- ìºì‹œëœ ì‘ë‹µ í™œìš©
- ê²½ëŸ‰ LLM ì‚¬ìš© (ì„ íƒì )

**ì˜ˆì‹œ**:
- "ê³„ì • ì‚­ì œ ë°©ë²•ì€?" â†’ FAQ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰
- "í™˜ë¶ˆ ì •ì±…ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?" â†’ ì •ì±… ë¬¸ì„œ ê²€ìƒ‰
- "ì˜ì—…ì‹œê°„ì´ ì–¸ì œì¸ê°€ìš”?" â†’ ê¸°ë³¸ ì •ë³´ ì¡°íšŒ

**íŠ¹ì§•**:
- âš¡ ì‘ë‹µ ì‹œê°„: 100-500ms
- ğŸ’° ë¹„ìš©: ë‚®ìŒ (ê²€ìƒ‰ ë¹„ìš©ë§Œ)
- ğŸ¯ ì •í™•ë„: 85-95%

#### Level 3: ì „ì²´ ì²˜ë¦¬ (Full Context)
**ëŒ€ìƒ ì¿¼ë¦¬**: ë³µì¡í•œ ë¬¸ì œ í•´ê²°, ì°½ì˜ì  ì‘ì—…, ì‹¬ì¸µ ë¶„ì„

**ì²˜ë¦¬ ë°©ì‹**:
- ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ë¡œë”©
- ê³ ì„±ëŠ¥ LLM í˜¸ì¶œ
- ë©€í‹° ìŠ¤í… ì¶”ë¡ 
- ë„êµ¬ ì‚¬ìš©

**ì˜ˆì‹œ**:
- "ì§€ë‚œ 3ê°œì›” íŒë§¤ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì„œ ë‹¤ìŒ ë¶„ê¸° ì „ëµì„ ì œì•ˆí•´ì£¼ì„¸ìš”"
- "ì´ ì½”ë“œì˜ ë²„ê·¸ë¥¼ ì°¾ê³  ìµœì í™” ë°©ì•ˆì„ ì„¤ëª…í•´ì£¼ì„¸ìš”"
- "ê³ ê° ë¶ˆë§Œì„ ë¶„ì„í•´ì„œ ì œí’ˆ ê°œì„  ìš°ì„ ìˆœìœ„ë¥¼ ì •í•´ì£¼ì„¸ìš”"

**íŠ¹ì§•**:
- âš¡ ì‘ë‹µ ì‹œê°„: 2-10ì´ˆ
- ğŸ’° ë¹„ìš©: ë†’ìŒ (full API ë¹„ìš©)
- ğŸ¯ ì •í™•ë„: 95-99%

### íŒŒë ˆí†  ë²•ì¹™ì˜ ì ìš©

ì‹¤ì œ ë°ì´í„°:
```
ì¿¼ë¦¬ ë¶„í¬:
â”œâ”€ Level 1 (ê²½ëŸ‰): 60-70%
â”œâ”€ Level 2 (ì¤‘ê°„): 20-30%
â””â”€ Level 3 (ì „ì²´): 5-10%

ë¹„ìš© ë¶„í¬:
â”œâ”€ Level 1: ~0% (ê±°ì˜ ë¬´ë£Œ)
â”œâ”€ Level 2: ~15%
â””â”€ Level 3: ~85%

â†’ ìƒìœ„ 10% ì¿¼ë¦¬ê°€ 85% ë¹„ìš© ë°œìƒ!
```

**ì ì‘í˜• í’ˆì§ˆì˜ íš¨ê³¼**:
- Level 1-2 ì¿¼ë¦¬ ìµœì í™” â†’ ì „ì²´ ë¹„ìš©ì˜ 15% ì ˆê° ê°€ëŠ¥
- Level 3 ì¿¼ë¦¬ í’ˆì§ˆ í–¥ìƒ â†’ í•µì‹¬ ê°€ì¹˜ ì œê³µ
- ì „ì²´ì ìœ¼ë¡œ **70-85% ë¹„ìš© ì ˆê°** ë‹¬ì„± ê°€ëŠ¥

---

## ì¿¼ë¦¬ ë¶„ë¥˜ ì „ëµ

ì¿¼ë¦¬ë¥¼ ì •í™•í•˜ê²Œ ë¶„ë¥˜í•˜ëŠ” ê²ƒì´ ì ì‘í˜• í’ˆì§ˆì˜ í•µì‹¬ì…ë‹ˆë‹¤.

### ë°©ë²• 1: ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜

**ì¥ì **: ë¹ ë¥´ê³ , ì˜ˆì¸¡ ê°€ëŠ¥í•˜ê³ , ë¹„ìš© ì—†ìŒ  
**ë‹¨ì **: ìœ ì§€ë³´ìˆ˜ í•„ìš”, ì—£ì§€ ì¼€ì´ìŠ¤ ì²˜ë¦¬ ì–´ë ¤ì›€

#### êµ¬í˜„ ì˜ˆì‹œ

```python
class RuleBasedClassifier:
    """ê·œì¹™ ê¸°ë°˜ ì¿¼ë¦¬ ë¶„ë¥˜ê¸°"""
    
    def __init__(self):
        # Level 1: ê²½ëŸ‰ ì²˜ë¦¬ íŒ¨í„´
        self.lightweight_patterns = [
            r'^(ì•ˆë…•|hi|hello|hey)',
            r'^(ê°ì‚¬|thank)',
            r'^(ì˜ê°€|bye|goodbye)',
            r'^(ok|okay|ì•Œê² )',
            r'^(ë„¤|yes|ì‘)',
        ]
        
        # Level 2: ì¤‘ê°„ ì²˜ë¦¬ í‚¤ì›Œë“œ
        self.medium_keywords = [
            'ë°©ë²•', 'ì–´ë–»ê²Œ', 'how to', 'ê°€ì´ë“œ',
            'ì •ì±…', 'ê·œì •', 'ì‹œê°„', 'ìœ„ì¹˜', 'ì—°ë½',
            'ê°€ê²©', 'ë¹„ìš©', 'ìš”ê¸ˆ', 'FAQ'
        ]
        
        # Level 3: ë³µì¡í•œ ì²˜ë¦¬ ì§€í‘œ
        self.complex_indicators = [
            'ë¶„ì„', 'ì¶”ì²œ', 'ë¹„êµ', 'í‰ê°€',
            'ì „ëµ', 'ìµœì í™”', 'ê°œì„ ',
            'ì™œ', 'ì´ìœ ', 'ì›ì¸'
        ]
    
    def classify(self, query: str) -> str:
        """ì¿¼ë¦¬ë¥¼ 3ê°œ ë ˆë²¨ë¡œ ë¶„ë¥˜"""
        query_lower = query.lower().strip()
        
        # Level 1 ì²´í¬
        for pattern in self.lightweight_patterns:
            if re.match(pattern, query_lower):
                return 'lightweight'
        
        # ê¸¸ì´ ê¸°ë°˜ ì´ˆê¸° í•„í„°
        if len(query) < 10:
            return 'lightweight'
        
        # Level 2 ì²´í¬
        if any(keyword in query_lower for keyword in self.medium_keywords):
            # ì¶”ê°€ ë³µì¡ë„ ì²´í¬
            if len(query) < 50 and '?' in query:
                return 'medium'
        
        # Level 3 ì²´í¬
        if any(indicator in query_lower for indicator in self.complex_indicators):
            return 'full'
        
        # ë¬¸ì¥ ìˆ˜ ê¸°ë°˜
        sentence_count = len(re.split(r'[.!?]', query))
        if sentence_count > 2:
            return 'full'
        
        # ê¸°ë³¸ê°’: ì¤‘ê°„ ì²˜ë¦¬
        return 'medium'
```

### ë°©ë²• 2: ML ê¸°ë°˜ ë¶„ë¥˜

**ì¥ì **: ë†’ì€ ì •í™•ë„, ìë™ í•™ìŠµ  
**ë‹¨ì **: ì´ˆê¸° í•™ìŠµ ë°ì´í„° í•„ìš”, ì•½ê°„ì˜ ì§€ì—°

#### êµ¬í˜„ ì˜ˆì‹œ

```python
class MLBasedClassifier:
    """ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì¿¼ë¦¬ ë¶„ë¥˜ê¸°"""
    
    def __init__(self):
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.feature_extraction.text import TfidfVectorizer
        
        self.vectorizer = TfidfVectorizer(max_features=100)
        self.classifier = RandomForestClassifier(n_estimators=50)
        self.is_trained = False
    
    def extract_features(self, query: str) -> dict:
        """ì¿¼ë¦¬ì—ì„œ íŠ¹ì§• ì¶”ì¶œ"""
        return {
            'length': len(query),
            'word_count': len(query.split()),
            'sentence_count': len(re.split(r'[.!?]', query)),
            'has_question': '?' in query,
            'has_numbers': bool(re.search(r'\d', query)),
            'avg_word_length': np.mean([len(w) for w in query.split()]),
        }
    
    def train(self, queries: list, labels: list):
        """í•™ìŠµ ë°ì´í„°ë¡œ ëª¨ë¸ í›ˆë ¨"""
        # TF-IDF ë²¡í„°í™”
        X_text = self.vectorizer.fit_transform(queries)
        
        # ì¶”ê°€ íŠ¹ì§• ì¶”ì¶œ
        X_features = np.array([
            list(self.extract_features(q).values())
            for q in queries
        ])
        
        # íŠ¹ì§• ê²°í•©
        X = np.hstack([X_text.toarray(), X_features])
        
        # ëª¨ë¸ í›ˆë ¨
        self.classifier.fit(X, labels)
        self.is_trained = True
    
    def classify(self, query: str) -> str:
        """í›ˆë ¨ëœ ëª¨ë¸ë¡œ ë¶„ë¥˜"""
        if not self.is_trained:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # íŠ¹ì§• ì¶”ì¶œ
        X_text = self.vectorizer.transform([query])
        X_features = np.array([list(self.extract_features(query).values())])
        X = np.hstack([X_text.toarray(), X_features])
        
        # ì˜ˆì¸¡
        prediction = self.classifier.predict(X)[0]
        return prediction
```

### ë°©ë²• 3: í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼

**ê°€ì¥ ì¶”ì²œí•˜ëŠ” ë°©ì‹**: ê·œì¹™ ê¸°ë°˜ + ML ê²°í•©

```python
class HybridClassifier:
    """í•˜ì´ë¸Œë¦¬ë“œ ë¶„ë¥˜ê¸° (ê·œì¹™ + ML)"""
    
    def __init__(self):
        self.rule_classifier = RuleBasedClassifier()
        self.ml_classifier = MLBasedClassifier()
        self.use_ml = False
    
    def classify(self, query: str) -> str:
        """2ë‹¨ê³„ ë¶„ë¥˜"""
        
        # 1ë‹¨ê³„: ê·œì¹™ ê¸°ë°˜ ë¹ ë¥¸ í•„í„°
        rule_result = self.rule_classifier.classify(query)
        
        # í™•ì‹¤í•œ ê²½ìš° ë°”ë¡œ ë°˜í™˜
        if rule_result == 'lightweight':
            return 'lightweight'
        
        # 2ë‹¨ê³„: ê²½ê³„ ì¼€ì´ìŠ¤ëŠ” MLë¡œ ì •ë°€ ë¶„ë¥˜
        if self.use_ml and self.ml_classifier.is_trained:
            ml_result = self.ml_classifier.classify(query)
            return ml_result
        
        return rule_result
```

---

## 3ë‹¨ê³„ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

### ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```python
class AdaptiveQualitySystem:
    """ì ì‘í˜• í’ˆì§ˆ ê¸°ë°˜ AI ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.classifier = HybridClassifier()
        self.template_handler = TemplateHandler()
        self.search_handler = SearchHandler()
        self.llm_handler = LLMHandler()
        self.cache = ResponseCache()
        self.metrics = MetricsCollector()
    
    async def process_query(self, query: str, user_id: str) -> dict:
        """ì¿¼ë¦¬ ì²˜ë¦¬ ë©”ì¸ ë¡œì§"""
        
        start_time = time.time()
        
        # 1. ì¿¼ë¦¬ ë¶„ë¥˜
        query_level = self.classifier.classify(query)
        
        # 2. ìºì‹œ í™•ì¸
        cache_key = self._get_cache_key(query, query_level)
        cached_response = self.cache.get(cache_key)
        if cached_response:
            self.metrics.record('cache_hit', query_level)
            return cached_response
        
        # 3. ë ˆë²¨ë³„ ì²˜ë¦¬
        if query_level == 'lightweight':
            response = await self._handle_lightweight(query)
        elif query_level == 'medium':
            response = await self._handle_medium(query, user_id)
        else:  # full
            response = await self._handle_full(query, user_id)
        
        # 4. ìºì‹± (ë ˆë²¨ 1, 2ë§Œ)
        if query_level in ['lightweight', 'medium']:
            self.cache.set(cache_key, response, ttl=3600)
        
        # 5. ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        elapsed_time = time.time() - start_time
        self.metrics.record('query_processed', {
            'level': query_level,
            'time': elapsed_time,
            'cost': response.get('cost', 0)
        })
        
        return response
    
    async def _handle_lightweight(self, query: str) -> dict:
        """Level 1: í…œí”Œë¦¿ ê¸°ë°˜ ì²˜ë¦¬"""
        
        response_text = self.template_handler.get_response(query)
        
        return {
            'response': response_text,
            'level': 'lightweight',
            'cost': 0,
            'tokens': 0,
            'method': 'template'
        }
    
    async def _handle_medium(self, query: str, user_id: str) -> dict:
        """Level 2: ê²€ìƒ‰ ê¸°ë°˜ ì²˜ë¦¬"""
        
        # FAQ ê²€ìƒ‰
        faq_results = await self.search_handler.search_faq(query)
        
        if faq_results and faq_results[0]['score'] > 0.85:
            # ë†’ì€ ì‹ ë¢°ë„ â†’ ë°”ë¡œ ë°˜í™˜
            return {
                'response': faq_results[0]['answer'],
                'level': 'medium',
                'cost': 0.001,  # ê²€ìƒ‰ ë¹„ìš©ë§Œ
                'tokens': 0,
                'method': 'faq_search',
                'source': faq_results[0]['source']
            }
        
        # ë¬¸ì„œ ê²€ìƒ‰
        doc_results = await self.search_handler.search_documents(query, top_k=3)
        
        # ê²½ëŸ‰ LLMìœ¼ë¡œ ë‹µë³€ ìƒì„± (ì„ íƒì )
        response_text = await self.llm_handler.generate_light(
            query=query,
            context=doc_results,
            max_tokens=200
        )
        
        return {
            'response': response_text,
            'level': 'medium',
            'cost': 0.01,  # ê²½ëŸ‰ LLM ë¹„ìš©
            'tokens': 200,
            'method': 'search_and_light_llm'
        }
    
    async def _handle_full(self, query: str, user_id: str) -> dict:
        """Level 3: ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
        
        # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ
        user_context = await self._load_user_context(user_id)
        
        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        documents = await self.search_handler.search_documents(
            query, 
            top_k=10
        )
        
        # ëŒ€í™” ì´ë ¥
        conversation_history = await self._load_conversation(user_id)
        
        # ì „ì²´ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        full_context = self._build_full_context(
            query=query,
            user_context=user_context,
            documents=documents,
            history=conversation_history
        )
        
        # ê³ ì„±ëŠ¥ LLM í˜¸ì¶œ
        response_text = await self.llm_handler.generate_full(
            context=full_context,
            max_tokens=2000,
            temperature=0.7
        )
        
        return {
            'response': response_text,
            'level': 'full',
            'cost': 0.50,  # ì „ì²´ LLM ë¹„ìš©
            'tokens': 15000,  # context + response
            'method': 'full_context_llm'
        }
```

---

## ì‹¤ì „ êµ¬í˜„ ê°€ì´ë“œ

### 1ë‹¨ê³„: í…œí”Œë¦¿ í•¸ë“¤ëŸ¬ êµ¬í˜„

```python
class TemplateHandler:
    """ê²½ëŸ‰ ì²˜ë¦¬ìš© í…œí”Œë¦¿ ê´€ë¦¬"""
    
    def __init__(self):
        self.templates = {
            # ì¸ì‚¬
            'greeting': [
                "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
                "ë°˜ê°‘ìŠµë‹ˆë‹¤! ê¶ê¸ˆí•˜ì‹  ì ì„ ë§ì”€í•´ì£¼ì„¸ìš”.",
            ],
            
            # ê°ì‚¬
            'thanks': [
                "ì²œë§Œì—ìš”! ë” í•„ìš”í•˜ì‹  ê²ƒì´ ìˆìœ¼ì‹ ê°€ìš”?",
                "ë„ì›€ì´ ë˜ì—ˆë‹¤ë‹ˆ ê¸°ì©ë‹ˆë‹¤!",
            ],
            
            # ì¢…ë£Œ
            'goodbye': [
                "ì´ìš©í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”!",
                "ì•ˆë…•íˆ ê°€ì„¸ìš”!",
            ],
            
            # ê¸ì • í™•ì¸
            'confirmation': [
                "ì•Œê² ìŠµë‹ˆë‹¤!",
                "ë„¤, í™•ì¸í–ˆìŠµë‹ˆë‹¤!",
            ]
        }
        
        # íŒ¨í„´ ë§¤ì¹­ ê·œì¹™
        self.patterns = {
            'greeting': r'^(ì•ˆë…•|hi|hello|hey)',
            'thanks': r'(ê°ì‚¬|thank|ê³ ë§ˆ)',
            'goodbye': r'(ì˜ê°€|bye|goodbye)',
            'confirmation': r'^(ë„¤|yes|ok|okay|ì•Œê² )',
        }
    
    def get_response(self, query: str) -> str:
        """ì¿¼ë¦¬ì— ë§ëŠ” í…œí”Œë¦¿ ì‘ë‹µ ë°˜í™˜"""
        
        query_lower = query.lower().strip()
        
        # íŒ¨í„´ ë§¤ì¹­
        for category, pattern in self.patterns.items():
            if re.search(pattern, query_lower):
                # ëœë¤ ì„ íƒ (ë‹¤ì–‘ì„±)
                return random.choice(self.templates[category])
        
        # ê¸°ë³¸ ì‘ë‹µ
        return "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
```

### 2ë‹¨ê³„: FAQ ê²€ìƒ‰ ì‹œìŠ¤í…œ

```python
class SearchHandler:
    """FAQ ë° ë¬¸ì„œ ê²€ìƒ‰ í•¸ë“¤ëŸ¬"""
    
    def __init__(self):
        from sentence_transformers import SentenceTransformer
        import faiss
        
        # ì„ë² ë”© ëª¨ë¸
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
        
        # FAQ ë°ì´í„°ë² ì´ìŠ¤
        self.faq_db = self._load_faq_database()
        
        # FAISS ì¸ë±ìŠ¤
        self.faq_index = self._build_faq_index()
    
    def _load_faq_database(self) -> list:
        """FAQ ë¡œë“œ"""
        return [
            {
                'question': 'ê³„ì •ì„ ì‚­ì œí•˜ë ¤ë©´ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?',
                'answer': 'ì„¤ì • > ê³„ì • > ê³„ì • ì‚­ì œ ë©”ë‰´ì—ì„œ ì‚­ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
                'category': 'account'
            },
            {
                'question': 'í™˜ë¶ˆ ì •ì±…ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?',
                'answer': 'êµ¬ë§¤ í›„ 7ì¼ ì´ë‚´ 100% í™˜ë¶ˆ ê°€ëŠ¥í•©ë‹ˆë‹¤.',
                'category': 'payment'
            },
            {
                'question': 'ì˜ì—…ì‹œê°„ì´ ì–¸ì œì¸ê°€ìš”?',
                'answer': 'í‰ì¼ 09:00-18:00, ì£¼ë§ ë° ê³µíœ´ì¼ íœ´ë¬´ì…ë‹ˆë‹¤.',
                'category': 'info'
            },
            # ... ë” ë§ì€ FAQ
        ]
    
    def _build_faq_index(self):
        """FAISS ì¸ë±ìŠ¤ êµ¬ì¶•"""
        import faiss
        
        # FAQ ì§ˆë¬¸ ì„ë² ë”©
        questions = [faq['question'] for faq in self.faq_db]
        embeddings = self.encoder.encode(questions)
        
        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatIP(dimension)  # Inner Product
        
        # ì •ê·œí™” í›„ ì¶”ê°€
        faiss.normalize_L2(embeddings)
        index.add(embeddings)
        
        return index
    
    async def search_faq(self, query: str, top_k: int = 3) -> list:
        """FAQ ê²€ìƒ‰"""
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.encoder.encode([query])
        faiss.normalize_L2(query_embedding)
        
        # ê²€ìƒ‰
        scores, indices = self.faq_index.search(query_embedding, top_k)
        
        # ê²°ê³¼ êµ¬ì„±
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.faq_db):
                faq = self.faq_db[idx]
                results.append({
                    'question': faq['question'],
                    'answer': faq['answer'],
                    'score': float(score),
                    'source': f"FAQ-{faq['category']}"
                })
        
        return results
    
    async def search_documents(self, query: str, top_k: int = 5) -> list:
        """ë¬¸ì„œ ê²€ìƒ‰ (ì‹¤ì œë¡œëŠ” ë²¡í„° DB ì‚¬ìš©)"""
        
        # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜
        # ì‹¤ì œë¡œëŠ” ChromaDB, Pinecone ë“± ì‚¬ìš©
        
        return [
            {
                'content': f"ê´€ë ¨ ë¬¸ì„œ ë‚´ìš© {i+1}...",
                'score': 0.8 - (i * 0.1),
                'source': f"doc_{i+1}"
            }
            for i in range(top_k)
        ]
```

### 3ë‹¨ê³„: ì‘ë‹µ ìºì‹±

```python
class ResponseCache:
    """ì‘ë‹µ ìºì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        import hashlib
        from collections import OrderedDict
        
        self.cache = OrderedDict()
        self.max_size = 1000
        self.stats = {'hits': 0, 'misses': 0}
    
    def _generate_key(self, query: str, level: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        combined = f"{query.lower().strip()}:{level}"
        return hashlib.md5(combined.encode()).hexdigest()
    
    def get(self, key: str) -> dict:
        """ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        if key in self.cache:
            self.stats['hits'] += 1
            # LRU: ìµœê·¼ ì‚¬ìš©ìœ¼ë¡œ ì´ë™
            self.cache.move_to_end(key)
            return self.cache[key]
        
        self.stats['misses'] += 1
        return None
    
    def set(self, key: str, value: dict, ttl: int = 3600):
        """ìºì‹œì— ì €ì¥"""
        
        # í¬ê¸° ì œí•œ
        if len(self.cache) >= self.max_size:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            self.cache.popitem(last=False)
        
        # TTLê³¼ í•¨ê»˜ ì €ì¥
        self.cache[key] = {
            **value,
            '_cached_at': time.time(),
            '_ttl': ttl
        }
    
    def get_hit_rate(self) -> float:
        """ìºì‹œ íˆíŠ¸ìœ¨"""
        total = self.stats['hits'] + self.stats['misses']
        if total == 0:
            return 0.0
        return self.stats['hits'] / total
```

---

## ì„±ëŠ¥ ìµœì í™” ë° ë¹„ìš© ì ˆê°

### ì‹¤ì œ ì„±ëŠ¥ ë°ì´í„°

**ì¼€ì´ìŠ¤ ìŠ¤í„°ë””: ê³ ê° ì§€ì› ì±—ë´‡**

```
í™˜ê²½:
- ì¼ì¼ ì¿¼ë¦¬: 10,000ê°œ
- ê¸°ì¡´ ì‹œìŠ¤í…œ: ëª¨ë“  ì¿¼ë¦¬ LLM í˜¸ì¶œ

Before (ê¸°ì¡´ ë°©ì‹):
â”œâ”€ í‰ê·  ì‘ë‹µ ì‹œê°„: 2.5ì´ˆ
â”œâ”€ ì¼ì¼ API ë¹„ìš©: $150
â”œâ”€ ì›”ê°„ ë¹„ìš©: $4,500
â””â”€ ì‚¬ìš©ì ë§Œì¡±ë„: 78%

After (ì ì‘í˜• í’ˆì§ˆ):
â”œâ”€ Level 1 (60%): í‰ê·  8ms, ë¹„ìš© $0
â”œâ”€ Level 2 (30%): í‰ê·  300ms, ë¹„ìš© $30/ì¼
â”œâ”€ Level 3 (10%): í‰ê·  3ì´ˆ, ë¹„ìš© $50/ì¼
â”œâ”€ ì´ ì¼ì¼ ë¹„ìš©: $80
â”œâ”€ ì›”ê°„ ë¹„ìš©: $2,400 (â†“ 47% ì ˆê°)
â”œâ”€ í‰ê·  ì‘ë‹µ ì‹œê°„: 0.9ì´ˆ (â†“ 64% ê°œì„ )
â””â”€ ì‚¬ìš©ì ë§Œì¡±ë„: 89% (â†‘ 11%p)

ROI:
- ì—°ê°„ ë¹„ìš© ì ˆê°: $25,200
- ì‚¬ìš©ì ê²½í—˜ ê°œì„ ìœ¼ë¡œ ì´íƒˆë¥  15% ê°ì†Œ
```

### ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸

```markdown
âœ… ì¿¼ë¦¬ ë¶„ë¥˜ ì •í™•ë„
[ ] ë¶„ë¥˜ ì •í™•ë„ > 90%
[ ] ì˜¤ë¶„ë¥˜ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
[ ] ì£¼ê¸°ì  ë¶„ë¥˜ê¸° ì¬í›ˆë ¨

âœ… ìºì‹± ì „ëµ
[ ] ìºì‹œ íˆíŠ¸ìœ¨ > 40%
[ ] TTL ìµœì í™” (ë„ˆë¬´ ì§§ì§€ë„ ê¸¸ì§€ë„ ì•Šê²Œ)
[ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

âœ… í…œí”Œë¦¿ ê´€ë¦¬
[ ] í…œí”Œë¦¿ ì»¤ë²„ë¦¬ì§€ > 50% (Level 1 ì¿¼ë¦¬)
[ ] ì •ê¸°ì  í…œí”Œë¦¿ ì—…ë°ì´íŠ¸
[ ] A/B í…ŒìŠ¤íŠ¸ë¡œ íš¨ê³¼ ê²€ì¦

âœ… ê²€ìƒ‰ ì„±ëŠ¥
[ ] FAQ ê²€ìƒ‰ < 50ms
[ ] ë¬¸ì„œ ê²€ìƒ‰ < 200ms
[ ] ê²€ìƒ‰ ê²°ê³¼ ê´€ë ¨ì„± > 0.8

âœ… ë¹„ìš© ê´€ë¦¬
[ ] Level 1-2 ì²˜ë¦¬ ë¹„ìœ¨ > 80%
[ ] ì¼ì¼ ë¹„ìš© ëª¨ë‹ˆí„°ë§
[ ] ì˜ˆì‚° ì´ˆê³¼ ì•Œë¦¼ ì„¤ì •
```

---

## ëª¨ë‹ˆí„°ë§ ë° ê°œì„ 

### ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œìŠ¤í…œ

```python
class MetricsCollector:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
    
    def __init__(self):
        self.metrics = {
            'lightweight': {'count': 0, 'total_time': 0, 'total_cost': 0},
            'medium': {'count': 0, 'total_time': 0, 'total_cost': 0},
            'full': {'count': 0, 'total_time': 0, 'total_cost': 0},
        }
        self.errors = []
    
    def record(self, event_type: str, data: dict):
        """ì´ë²¤íŠ¸ ê¸°ë¡"""
        
        if event_type == 'query_processed':
            level = data['level']
            self.metrics[level]['count'] += 1
            self.metrics[level]['total_time'] += data['time']
            self.metrics[level]['total_cost'] += data.get('cost', 0)
    
    def get_summary(self) -> dict:
        """ìš”ì•½ í†µê³„"""
        
        summary = {}
        total_queries = sum(m['count'] for m in self.metrics.values())
        
        for level, data in self.metrics.items():
            count = data['count']
            if count > 0:
                summary[level] = {
                    'count': count,
                    'percentage': (count / total_queries) * 100,
                    'avg_time': data['total_time'] / count,
                    'avg_cost': data['total_cost'] / count,
                    'total_cost': data['total_cost']
                }
        
        summary['total'] = {
            'queries': total_queries,
            'total_cost': sum(m['total_cost'] for m in self.metrics.values())
        }
        
        return summary
    
    def print_report(self):
        """ë¦¬í¬íŠ¸ ì¶œë ¥"""
        summary = self.get_summary()
        
        print("=" * 60)
        print("ì ì‘í˜• í’ˆì§ˆ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¦¬í¬íŠ¸")
        print("=" * 60)
        
        for level in ['lightweight', 'medium', 'full']:
            if level in summary:
                data = summary[level]
                print(f"\n[{level.upper()}]")
                print(f"  ì¿¼ë¦¬ ìˆ˜: {data['count']:,} ({data['percentage']:.1f}%)")
                print(f"  í‰ê·  ì‹œê°„: {data['avg_time']*1000:.1f}ms")
                print(f"  í‰ê·  ë¹„ìš©: ${data['avg_cost']:.4f}")
                print(f"  ì´ ë¹„ìš©: ${data['total_cost']:.2f}")
        
        print(f"\n[TOTAL]")
        print(f"  ì „ì²´ ì¿¼ë¦¬: {summary['total']['queries']:,}")
        print(f"  ì „ì²´ ë¹„ìš©: ${summary['total']['total_cost']:.2f}")
        print("=" * 60)
```

### ëŒ€ì‹œë³´ë“œ ì˜ˆì‹œ

```python
def generate_dashboard():
    """ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ (ê°„ë‹¨í•œ ë²„ì „)"""
    
    import matplotlib.pyplot as plt
    
    # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    collector = MetricsCollector()
    summary = collector.get_summary()
    
    # ì¿¼ë¦¬ ë¶„í¬ íŒŒì´ ì°¨íŠ¸
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    levels = list(summary.keys())[:-1]  # total ì œì™¸
    counts = [summary[l]['count'] for l in levels]
    
    ax1.pie(counts, labels=levels, autopct='%1.1f%%')
    ax1.set_title('ì¿¼ë¦¬ ë¶„í¬')
    
    # ë¹„ìš© ë¶„í¬ ë§‰ëŒ€ ê·¸ë˜í”„
    costs = [summary[l]['total_cost'] for l in levels]
    ax2.bar(levels, costs)
    ax2.set_title('ë ˆë²¨ë³„ ì´ ë¹„ìš©')
    ax2.set_ylabel('ë¹„ìš© ($)')
    
    plt.tight_layout()
    plt.savefig('adaptive_quality_dashboard.png')
    print("ëŒ€ì‹œë³´ë“œ ì €ì¥ ì™„ë£Œ: adaptive_quality_dashboard.png")
```

---

## ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 1. ì ì§„ì  ë¡¤ì•„ì›ƒ

```python
class GradualRollout:
    """ì ì§„ì  ë°°í¬ ì „ëµ"""
    
    def __init__(self):
        self.rollout_percentage = 0  # 0-100
        self.legacy_system = LegacySystem()
        self.adaptive_system = AdaptiveQualitySystem()
    
    def process_query(self, query: str, user_id: str) -> dict:
        """ì¼ë¶€ íŠ¸ë˜í”½ë§Œ ì‹ ê·œ ì‹œìŠ¤í…œìœ¼ë¡œ"""
        
        # ì‚¬ìš©ì ID í•´ì‹±ìœ¼ë¡œ ì¼ê´€ëœ ë¶„ë°°
        user_hash = hash(user_id) % 100
        
        if user_hash < self.rollout_percentage:
            # ì‹ ê·œ ì‹œìŠ¤í…œ (ì ì‘í˜• í’ˆì§ˆ)
            return self.adaptive_system.process_query(query, user_id)
        else:
            # ê¸°ì¡´ ì‹œìŠ¤í…œ
            return self.legacy_system.process_query(query, user_id)
    
    def increase_rollout(self, increment: int = 10):
        """ë¡¤ì•„ì›ƒ ë¹„ìœ¨ ì¦ê°€"""
        self.rollout_percentage = min(100, self.rollout_percentage + increment)
        print(f"ë¡¤ì•„ì›ƒ ë¹„ìœ¨: {self.rollout_percentage}%")

# ì‚¬ìš© ì˜ˆì‹œ
rollout = GradualRollout()

# 1ì£¼ì°¨: 10%
rollout.rollout_percentage = 10

# ë¬¸ì œ ì—†ìœ¼ë©´ 2ì£¼ì°¨: 25%
rollout.increase_rollout(15)

# 3ì£¼ì°¨: 50%
rollout.increase_rollout(25)

# 4ì£¼ì°¨: 100% (ì „ì²´ ì „í™˜)
rollout.increase_rollout(50)
```

### 2. A/B í…ŒìŠ¤íŠ¸

```python
class ABTest:
    """A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬"""
    
    def __init__(self):
        self.variant_a_metrics = []  # ê¸°ì¡´ ì‹œìŠ¤í…œ
        self.variant_b_metrics = []  # ì ì‘í˜• í’ˆì§ˆ
    
    def assign_variant(self, user_id: str) -> str:
        """ì‚¬ìš©ìë¥¼ A/B ê·¸ë£¹ì— í• ë‹¹"""
        return 'B' if hash(user_id) % 2 == 0 else 'A'
    
    def record_result(self, variant: str, metrics: dict):
        """ê²°ê³¼ ê¸°ë¡"""
        if variant == 'A':
            self.variant_a_metrics.append(metrics)
        else:
            self.variant_b_metrics.append(metrics)
    
    def analyze(self) -> dict:
        """í†µê³„ ë¶„ì„"""
        import numpy as np
        from scipy import stats
        
        # í‰ê·  ë¹„ìš© ë¹„êµ
        cost_a = np.mean([m['cost'] for m in self.variant_a_metrics])
        cost_b = np.mean([m['cost'] for m in self.variant_b_metrics])
        
        # t-test
        costs_a = [m['cost'] for m in self.variant_a_metrics]
        costs_b = [m['cost'] for m in self.variant_b_metrics]
        t_stat, p_value = stats.ttest_ind(costs_a, costs_b)
        
        return {
            'variant_a_avg_cost': cost_a,
            'variant_b_avg_cost': cost_b,
            'cost_reduction': ((cost_a - cost_b) / cost_a) * 100,
            'p_value': p_value,
            'significant': p_value < 0.05
        }
```

### 3. ì˜¤ë¥˜ ì²˜ë¦¬ ë° í´ë°±

```python
class RobustAdaptiveSystem:
    """ê²¬ê³ í•œ ì ì‘í˜• ì‹œìŠ¤í…œ (í´ë°± í¬í•¨)"""
    
    async def process_query_safe(self, query: str, user_id: str) -> dict:
        """ì•ˆì „í•œ ì¿¼ë¦¬ ì²˜ë¦¬ (ì˜¤ë¥˜ ëŒ€ì‘)"""
        
        try:
            # ë¶„ë¥˜ ì‹œë„
            query_level = self.classifier.classify(query)
        except Exception as e:
            logger.error(f"ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            # í´ë°±: ì¤‘ê°„ ì²˜ë¦¬ë¡œ
            query_level = 'medium'
        
        try:
            # ë ˆë²¨ë³„ ì²˜ë¦¬
            if query_level == 'lightweight':
                return await self._handle_lightweight(query)
            elif query_level == 'medium':
                return await self._handle_medium_safe(query, user_id)
            else:
                return await self._handle_full_safe(query, user_id)
                
        except Exception as e:
            logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # ìµœì¢… í´ë°±: ê¸°ë³¸ LLM
            return await self._fallback_basic_llm(query)
    
    async def _handle_medium_safe(self, query: str, user_id: str) -> dict:
        """ì•ˆì „í•œ ì¤‘ê°„ ì²˜ë¦¬"""
        
        try:
            # ê²€ìƒ‰ ì‹œë„
            results = await self.search_handler.search_faq(query)
            if results and results[0]['score'] > 0.8:
                return self._format_response(results[0]['answer'], 'medium')
        except Exception as e:
            logger.warning(f"ê²€ìƒ‰ ì‹¤íŒ¨, LLMìœ¼ë¡œ í´ë°±: {e}")
        
        # í´ë°±: ê²½ëŸ‰ LLM
        return await self._fallback_light_llm(query)
    
    async def _fallback_basic_llm(self, query: str) -> dict:
        """ê¸°ë³¸ LLM í´ë°±"""
        response = await self.llm_handler.generate_basic(query)
        return {
            'response': response,
            'level': 'fallback',
            'cost': 0.05,
            'method': 'fallback_llm'
        }
```

### 4. ì§€ì†ì  í•™ìŠµ

```python
class ContinuousLearning:
    """ì§€ì†ì  í•™ìŠµ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.feedback_data = []
        self.retraining_threshold = 1000  # 1000ê°œ í”¼ë“œë°±ë§ˆë‹¤ ì¬í›ˆë ¨
    
    def collect_feedback(self, query: str, predicted_level: str, 
                        actual_level: str, user_rating: float):
        """í”¼ë“œë°± ìˆ˜ì§‘"""
        
        self.feedback_data.append({
            'query': query,
            'predicted': predicted_level,
            'actual': actual_level,
            'rating': user_rating,
            'timestamp': time.time()
        })
        
        # ì¬í›ˆë ¨ íŠ¸ë¦¬ê±°
        if len(self.feedback_data) >= self.retraining_threshold:
            self.retrain_classifier()
    
    def retrain_classifier(self):
        """ë¶„ë¥˜ê¸° ì¬í›ˆë ¨"""
        
        print(f"ì¬í›ˆë ¨ ì‹œì‘ ({len(self.feedback_data)} ìƒ˜í”Œ)")
        
        # ì‹¤ì œ ë ˆë²¨ì´ ìˆëŠ” ë°ì´í„°ë§Œ
        training_data = [
            (fb['query'], fb['actual'])
            for fb in self.feedback_data
            if fb['actual'] is not None
        ]
        
        if len(training_data) < 100:
            print("ë°ì´í„° ë¶€ì¡±, ì¬í›ˆë ¨ ì—°ê¸°")
            return
        
        queries, labels = zip(*training_data)
        
        # ML ë¶„ë¥˜ê¸° ì¬í›ˆë ¨
        self.ml_classifier.train(list(queries), list(labels))
        
        # í”¼ë“œë°± ë°ì´í„° ì•„ì¹´ì´ë¸Œ
        self._archive_feedback()
        self.feedback_data = []
        
        print("ì¬í›ˆë ¨ ì™„ë£Œ")
```

---

## ì‹¤ì „ ì‹œì‘ ê°€ì´ë“œ

### Step 1: í˜„ì¬ ì‹œìŠ¤í…œ ë¶„ì„ (1ì£¼)

```python
# ê¸°ì¡´ ì¿¼ë¦¬ ë¡œê·¸ ë¶„ì„
def analyze_current_queries(query_log: list) -> dict:
    """í˜„ì¬ ì¿¼ë¦¬ íŒ¨í„´ ë¶„ì„"""
    
    classifier = RuleBasedClassifier()
    
    distribution = {
        'lightweight': 0,
        'medium': 0,
        'full': 0
    }
    
    for query in query_log:
        level = classifier.classify(query['text'])
        distribution[level] += 1
    
    total = len(query_log)
    
    return {
        'total_queries': total,
        'lightweight': {
            'count': distribution['lightweight'],
            'percentage': (distribution['lightweight'] / total) * 100
        },
        'medium': {
            'count': distribution['medium'],
            'percentage': (distribution['medium'] / total) * 100
        },
        'full': {
            'count': distribution['full'],
            'percentage': (distribution['full'] / total) * 100
        }
    }

# ì˜ˆìƒ ë¹„ìš© ì ˆê° ê³„ì‚°
def estimate_savings(analysis: dict, current_cost_per_query: float) -> dict:
    """ì˜ˆìƒ ë¹„ìš© ì ˆê°ì•¡"""
    
    total = analysis['total_queries']
    current_total_cost = total * current_cost_per_query
    
    # ì ì‘í˜• í’ˆì§ˆ ì ìš© í›„
    new_cost = (
        analysis['lightweight']['count'] * 0 +  # ë¬´ë£Œ
        analysis['medium']['count'] * (current_cost_per_query * 0.1) +  # 10%
        analysis['full']['count'] * current_cost_per_query  # 100%
    )
    
    savings = current_total_cost - new_cost
    savings_percentage = (savings / current_total_cost) * 100
    
    return {
        'current_cost': current_total_cost,
        'new_cost': new_cost,
        'savings': savings,
        'savings_percentage': savings_percentage
    }
```

### Step 2: í…œí”Œë¦¿ ë° FAQ ì¤€ë¹„ (1ì£¼)

```markdown
ì²´í¬ë¦¬ìŠ¤íŠ¸:
[ ] ìƒìœ„ 100ê°œ ì¸ì‚¬/ê°ì‚¬ íŒ¨í„´ ìˆ˜ì§‘
[ ] í…œí”Œë¦¿ ì‘ì„± (ìµœì†Œ 20ê°œ)
[ ] FAQ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• (ìµœì†Œ 50ê°œ)
[ ] FAQ ì„ë² ë”© ìƒì„± ë° ì¸ë±ì‹±
[ ] ìºì‹œ ì €ì¥ì†Œ ì„¤ì •
```

### Step 3: ì‹œìŠ¤í…œ êµ¬í˜„ (2ì£¼)

```markdown
[ ] ì¿¼ë¦¬ ë¶„ë¥˜ê¸° êµ¬í˜„
[ ] í…œí”Œë¦¿ í•¸ë“¤ëŸ¬ êµ¬í˜„
[ ] ê²€ìƒ‰ í•¸ë“¤ëŸ¬ êµ¬í˜„
[ ] LLM í•¸ë“¤ëŸ¬ êµ¬í˜„
[ ] ìºì‹± ì‹œìŠ¤í…œ êµ¬í˜„
[ ] ë©”íŠ¸ë¦­ ìˆ˜ì§‘ êµ¬í˜„
```

### Step 4: í…ŒìŠ¤íŠ¸ ë° ì¡°ì • (1ì£¼)

```python
def run_comprehensive_test():
    """ì¢…í•© í…ŒìŠ¤íŠ¸"""
    
    system = AdaptiveQualitySystem()
    
    test_queries = [
        # Level 1
        ("ì•ˆë…•í•˜ì„¸ìš”", "lightweight"),
        ("ê°ì‚¬í•©ë‹ˆë‹¤", "lightweight"),
        
        # Level 2
        ("ê³„ì • ì‚­ì œ ë°©ë²•ì€?", "medium"),
        ("ì˜ì—…ì‹œê°„ì´ ì–¸ì œì¸ê°€ìš”?", "medium"),
        
        # Level 3
        ("ì§€ë‚œ 3ê°œì›” íŒë§¤ ì¶”ì´ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”", "full"),
        ("ì´ ì½”ë“œì˜ ì„±ëŠ¥ì„ ìµœì í™”í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•´ì£¼ì„¸ìš”", "full"),
    ]
    
    results = []
    for query, expected_level in test_queries:
        result = system.process_query(query, "test_user")
        results.append({
            'query': query,
            'expected': expected_level,
            'actual': result['level'],
            'correct': result['level'] == expected_level,
            'time': result.get('time', 0),
            'cost': result.get('cost', 0)
        })
    
    # ì •í™•ë„ ê³„ì‚°
    accuracy = sum(r['correct'] for r in results) / len(results)
    print(f"ë¶„ë¥˜ ì •í™•ë„: {accuracy * 100:.1f}%")
    
    return results
```

### Step 5: ì ì§„ì  ë°°í¬ (4ì£¼)

```markdown
ì£¼ì°¨ë³„ ê³„íš:

1ì£¼ì°¨ (10% íŠ¸ë˜í”½)
[ ] ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ê°€ë™
[ ] ì˜¤ë¥˜ìœ¨ < 1% í™•ì¸
[ ] ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

2ì£¼ì°¨ (25% íŠ¸ë˜í”½)
[ ] ë¹„ìš© ì ˆê° í™•ì¸
[ ] ì‚¬ìš©ì ë§Œì¡±ë„ ì¸¡ì •
[ ] ë²„ê·¸ ìˆ˜ì •

3ì£¼ì°¨ (50% íŠ¸ë˜í”½)
[ ] A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„
[ ] ë¶„ë¥˜ê¸° ì •í™•ë„ ê²€ì¦
[ ] ìµœì í™” ì ìš©

4ì£¼ì°¨ (100% íŠ¸ë˜í”½)
[ ] ì „ì²´ ì „í™˜
[ ] ìµœì¢… ì„±ê³¼ ì¸¡ì •
[ ] ë¬¸ì„œí™” ì™„ë£Œ
```

---

## ë§ˆì¹˜ë©°

**ì ì‘í˜• í’ˆì§ˆì˜ í•µì‹¬ ë©”ì‹œì§€**:

> "ëª¨ë“  ì¿¼ë¦¬ê°€ ìµœê³  í’ˆì§ˆì„ í•„ìš”ë¡œ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  
> 80%ì˜ ì¿¼ë¦¬ë¥¼ ê²½ëŸ‰ ì²˜ë¦¬í•˜ë©´ ì „ì²´ ë¹„ìš©ì´ ê¸‰ê°í•©ë‹ˆë‹¤."

**ê¸°ëŒ€ íš¨ê³¼**:
- âœ… ë¹„ìš© 70-85% ì ˆê°
- âœ… ì‘ë‹µ ì†ë„ 60-80% ê°œì„ 
- âœ… ì‚¬ìš©ì ë§Œì¡±ë„ 10-15%p í–¥ìƒ
- âœ… ì‹œìŠ¤í…œ í™•ì¥ì„± í¬ê²Œ ì¦ê°€

**ë‹¤ìŒ ë‹¨ê³„**:
1. í˜„ì¬ ì‹œìŠ¤í…œ ë¶„ì„ë¶€í„° ì‹œì‘
2. ì‘ê²Œ ì‹œì‘ (í…œí”Œë¦¿ 10ê°œ, FAQ 20ê°œ)
3. ì¸¡ì •í•˜ê³  ê°œì„ 
4. ì ì§„ì ìœ¼ë¡œ í™•ëŒ€

ì„±ê³µì„ ê¸°ì›í•©ë‹ˆë‹¤! ğŸš€

---

**ì‘ì„± ì¼ì**: 2025-12-25