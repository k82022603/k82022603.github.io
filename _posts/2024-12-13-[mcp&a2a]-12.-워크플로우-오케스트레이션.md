---
title: "[MCP&A2A] 12. ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜"
date: 2024-12-13 10:00:00 +0900
categories: [AI,  MCP & A2A]
mermaid: [True]
tags: [AI,  MCP,  A2A,  Guide,  MCP-A2A-Guide,  LangGraph,  Medium,  Claude.write]
---


## LangGraph ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°

ë³µì¡í•œ AI íƒœìŠ¤í¬ëŠ” ì—¬ëŸ¬ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ ìˆœì°¨ì  ë˜ëŠ” ë³‘ë ¬ë¡œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤. LangGraphëŠ” ìƒíƒœ ê¸°ë°˜ ê·¸ë˜í”„ë¡œ ì´ëŸ¬í•œ ë©€í‹°ìŠ¤í… ì›Œí¬í”Œë¡œìš°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.

### ì™œ LangGraphì¸ê°€?

| íŠ¹ì„± | LangGraph | LangChain | ìˆœìˆ˜ Python |
|------|-----------|-----------|-------------|
| **ìƒíƒœ ê´€ë¦¬** | âœ… ë‚´ì¥ | âŒ ìˆ˜ë™ | âŒ ìˆ˜ë™ |
| **ë¶„ê¸°/ì¡°ê±´** | âœ… ê°„ë‹¨ | âš ï¸ ë³µì¡ | âš ï¸ ë³µì¡ |
| **ì‹œê°í™”** | âœ… ìë™ | âŒ ì—†ìŒ | âŒ ì—†ìŒ |
| **ë””ë²„ê¹…** | âœ… ì‰¬ì›€ | âš ï¸ ì–´ë ¤ì›€ | âš ï¸ ì–´ë ¤ì›€ |
| **ì¬ì‹œë„** | âœ… ë‚´ì¥ | âŒ ìˆ˜ë™ | âŒ ìˆ˜ë™ |
| **ì²´í¬í¬ì¸íŠ¸** | âœ… ì§€ì› | âŒ ì—†ìŒ | âŒ ì—†ìŒ |

### LangGraph í•µì‹¬ ê°œë…

```
LangGraph = State + Nodes + Edges

State (ìƒíƒœ):
- TypedDictë¡œ ì •ì˜
- ë…¸ë“œ ê°„ ê³µìœ ë˜ëŠ” ë°ì´í„°
- ë¶ˆë³€ì„± ìœ ì§€

Node (ë…¸ë“œ):
- ê°œë³„ ì‘ì—… ë‹¨ìœ„
- Stateë¥¼ ë°›ì•„ì„œ ìˆ˜ì •ëœ State ë°˜í™˜
- í•¨ìˆ˜ë¡œ êµ¬í˜„

Edge (ì—£ì§€):
- ë…¸ë“œ ê°„ ì—°ê²°
- ì¡°ê±´ë¶€ ë¶„ê¸° ê°€ëŠ¥
- START â†’ ë…¸ë“œ1 â†’ ë…¸ë“œ2 â†’ END
```

## ê¸°ë³¸ ì›Œí¬í”Œë¡œìš° êµ¬ì¡°

### State ì •ì˜

```python
# orchestration/workflows/base.py
from typing import TypedDict, List, Optional, Annotated
from langgraph.graph import add_messages

class WorkflowState(TypedDict):
    """ì›Œí¬í”Œë¡œìš° ìƒíƒœ"""
    # ì…ë ¥
    query: str
    
    # ì¤‘ê°„ ë°ì´í„°
    documents: List[dict]
    analysis: Optional[dict]
    
    # ì¶œë ¥
    result: Optional[str]
    
    # ë©”íƒ€ë°ì´í„°
    step: str
    errors: List[str]
    
    # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ (ìë™ ëˆ„ì )
    messages: Annotated[List[dict], add_messages]
```

**add_messagesì˜ ì˜ë¯¸**:
- ë©”ì‹œì§€ë¥¼ ìë™ìœ¼ë¡œ ëˆ„ì 
- ë®ì–´ì“°ì§€ ì•Šê³  append
- ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬ì— ìœ ìš©

### Node ì •ì˜

```python
from langgraph.graph import StateGraph

def search_node(state: WorkflowState) -> WorkflowState:
    """ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ"""
    query = state["query"]
    
    # MCP ë„êµ¬ í˜¸ì¶œ
    results = mcp_client.hybrid_search(
        query=query,
        limit=10
    )
    
    # ìƒíƒœ ì—…ë°ì´íŠ¸
    state["documents"] = results
    state["step"] = "search_complete"
    
    return state

def analyze_node(state: WorkflowState) -> WorkflowState:
    """ë¬¸ì„œ ë¶„ì„ ë…¸ë“œ"""
    documents = state["documents"]
    
    # LLMìœ¼ë¡œ ë¶„ì„
    analysis = llm.invoke({
        "documents": documents,
        "task": "ë¶„ì„ ë° ìš”ì•½"
    })
    
    state["analysis"] = analysis
    state["step"] = "analysis_complete"
    
    return state

def generate_node(state: WorkflowState) -> WorkflowState:
    """ê²°ê³¼ ìƒì„± ë…¸ë“œ"""
    analysis = state["analysis"]
    
    # ìµœì¢… ê²°ê³¼ ìƒì„±
    result = llm.invoke({
        "analysis": analysis,
        "task": "ë³´ê³ ì„œ ìƒì„±"
    })
    
    state["result"] = result
    state["step"] = "completed"
    
    return state
```

### Graph êµ¬ì„±

```python
from langgraph.graph import START, END

# ê·¸ë˜í”„ ìƒì„±
workflow = StateGraph(WorkflowState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("search", search_node)
workflow.add_node("analyze", analyze_node)
workflow.add_node("generate", generate_node)

# ì—£ì§€ ì¶”ê°€ (ìˆœì„œ ì •ì˜)
workflow.add_edge(START, "search")
workflow.add_edge("search", "analyze")
workflow.add_edge("analyze", "generate")
workflow.add_edge("generate", END)

# ì»´íŒŒì¼
app = workflow.compile()
```

### ì‹¤í–‰

```python
# ì´ˆê¸° ìƒíƒœ
initial_state = {
    "query": "AI ì—ì´ì „íŠ¸ í”„ë¡œí† ì½œ ë¹„êµ",
    "documents": [],
    "analysis": None,
    "result": None,
    "step": "started",
    "errors": [],
    "messages": []
}

# ì‹¤í–‰
result = app.invoke(initial_state)

print(result["result"])
print(f"Steps: {result['step']}")
```

## ì¡°ê±´ë¶€ ë¶„ê¸°

### ì¡°ê±´ í•¨ìˆ˜

```python
def should_retry(state: WorkflowState) -> str:
    """ì¬ì‹œë„ ì—¬ë¶€ ê²°ì •"""
    if state.get("errors"):
        retry_count = state.get("retry_count", 0)
        if retry_count < 3:
            return "retry"
    return "continue"

def route_by_quality(state: WorkflowState) -> str:
    """í’ˆì§ˆì— ë”°ë¼ ë¼ìš°íŒ…"""
    quality_score = state.get("quality_score", 0)
    
    if quality_score >= 0.8:
        return "high_quality"
    elif quality_score >= 0.5:
        return "medium_quality"
    else:
        return "low_quality"
```

### ì¡°ê±´ë¶€ ì—£ì§€

```python
from langgraph.graph import StateGraph

workflow = StateGraph(WorkflowState)

workflow.add_node("search", search_node)
workflow.add_node("verify", verify_node)
workflow.add_node("retry_search", retry_search_node)
workflow.add_node("generate", generate_node)

# ì¼ë°˜ ì—£ì§€
workflow.add_edge(START, "search")

# ì¡°ê±´ë¶€ ì—£ì§€
workflow.add_conditional_edges(
    "verify",
    should_retry,
    {
        "retry": "retry_search",
        "continue": "generate"
    }
)

workflow.add_edge("retry_search", "verify")
workflow.add_edge("generate", END)

app = workflow.compile()
```

## ì‹¤ì „ ì›Œí¬í”Œë¡œìš° ì˜ˆì œ

### 1. ì¢…í•© ë¦¬ì„œì¹˜ ì›Œí¬í”Œë¡œìš°

```python
# orchestration/workflows/research_workflow.py
from typing import TypedDict, List, Optional
from langgraph.graph import StateGraph, START, END
import json

class ResearchState(TypedDict):
    query: str
    search_results: List[dict]
    ranked_results: List[dict]
    key_findings: List[str]
    report: Optional[str]
    confidence: float
    sources: List[str]

class ResearchWorkflow:
    def __init__(self, mcp_client, llm):
        self.mcp_client = mcp_client
        self.llm = llm
        self.workflow = self.build_workflow()
    
    def build_workflow(self):
        workflow = StateGraph(ResearchState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("search", self.search_documents)
        workflow.add_node("rank", self.rank_results)
        workflow.add_node("extract", self.extract_findings)
        workflow.add_node("synthesize", self.synthesize_report)
        workflow.add_node("verify", self.verify_sources)
        
        # ì—£ì§€ êµ¬ì„±
        workflow.add_edge(START, "search")
        workflow.add_edge("search", "rank")
        workflow.add_edge("rank", "extract")
        workflow.add_edge("extract", "synthesize")
        workflow.add_edge("synthesize", "verify")
        
        # ì¡°ê±´ë¶€ ì—£ì§€: ì‹ ë¢°ë„ì— ë”°ë¼ ì¬ê²€ìƒ‰ ë˜ëŠ” ì™„ë£Œ
        workflow.add_conditional_edges(
            "verify",
            self.check_confidence,
            {
                "retry": "search",
                "complete": END
            }
        )
        
        return workflow.compile()
    
    def search_documents(self, state: ResearchState) -> ResearchState:
        """1ë‹¨ê³„: ë¬¸ì„œ ê²€ìƒ‰"""
        print(f"ğŸ” ê²€ìƒ‰ ì¤‘: {state['query']}")
        
        results = self.mcp_client.hybrid_search(
            query=state["query"],
            limit=20,
            bm25_weight=0.5,
            vector_weight=0.5
        )
        
        state["search_results"] = results
        print(f"âœ… {len(results)}ê°œ ë¬¸ì„œ ë°œê²¬")
        
        return state
    
    def rank_results(self, state: ResearchState) -> ResearchState:
        """2ë‹¨ê³„: ê²°ê³¼ ìˆœìœ„ ì¬ì¡°ì •"""
        print("ğŸ“Š ê²°ê³¼ ìˆœìœ„ ì¡°ì • ì¤‘...")
        
        results = state["search_results"]
        query = state["query"]
        
        # LLMìœ¼ë¡œ ê´€ë ¨ì„± ì¬í‰ê°€
        prompt = f"""
        Query: {query}
        
        ë‹¤ìŒ ë¬¸ì„œë“¤ì˜ ê´€ë ¨ì„±ì„ 0-1 ì ìˆ˜ë¡œ í‰ê°€í•˜ì„¸ìš”:
        {json.dumps([r["title"] for r in results[:10]], ensure_ascii=False)}
        
        JSON ë°°ì—´ë¡œ ë°˜í™˜: [0.9, 0.8, ...]
        """
        
        scores = self.llm.invoke(prompt)
        # scores íŒŒì‹± (ê°„ì†Œí™”)
        
        # ì ìˆ˜ë¡œ ì¬ì •ë ¬
        ranked = sorted(
            zip(results, scores),
            key=lambda x: x[1],
            reverse=True
        )
        
        state["ranked_results"] = [r[0] for r in ranked[:10]]
        print(f"âœ… ìƒìœ„ 10ê°œ ì„ ì • ì™„ë£Œ")
        
        return state
    
    def extract_findings(self, state: ResearchState) -> ResearchState:
        """3ë‹¨ê³„: í•µì‹¬ ë°œê²¬ ì¶”ì¶œ"""
        print("ğŸ’¡ í•µì‹¬ ë°œê²¬ ì¶”ì¶œ ì¤‘...")
        
        documents = state["ranked_results"]
        
        # ê° ë¬¸ì„œì—ì„œ í•µì‹¬ ë‚´ìš© ì¶”ì¶œ
        findings = []
        sources = []
        
        for doc in documents[:5]:  # ìƒìœ„ 5ê°œë§Œ
            prompt = f"""
            ë‹¤ìŒ ë¬¸ì„œì—ì„œ '{state["query"]}'ì™€ ê´€ë ¨ëœ í•µì‹¬ ë°œê²¬ì„ 1-2ë¬¸ì¥ìœ¼ë¡œ ì¶”ì¶œ:
            
            {doc["content"][:500]}
            """
            
            finding = self.llm.invoke(prompt)
            findings.append(finding)
            sources.append(doc["title"])
        
        state["key_findings"] = findings
        state["sources"] = sources
        print(f"âœ… {len(findings)}ê°œ í•µì‹¬ ë°œê²¬ ì¶”ì¶œ")
        
        return state
    
    def synthesize_report(self, state: ResearchState) -> ResearchState:
        """4ë‹¨ê³„: ë³´ê³ ì„œ ìƒì„±"""
        print("ğŸ“ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        findings = state["key_findings"]
        query = state["query"]
        
        prompt = f"""
        ì§ˆë¬¸: {query}
        
        í•µì‹¬ ë°œê²¬:
        {chr(10).join([f"{i+1}. {f}" for i, f in enumerate(findings)])}
        
        ìœ„ ë°œê²¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì¢…í•© ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”:
        - ëª…í™•í•œ êµ¬ì¡° (ì„œë¡ , ë³¸ë¡ , ê²°ë¡ )
        - ë°œê²¬ ê°„ ì—°ê²°
        - ê°ê´€ì  í†¤
        """
        
        report = self.llm.invoke(prompt)
        
        state["report"] = report
        print("âœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
        
        return state
    
    def verify_sources(self, state: ResearchState) -> ResearchState:
        """5ë‹¨ê³„: ì¶œì²˜ ê²€ì¦"""
        print("âœ”ï¸ ì¶œì²˜ ê²€ì¦ ì¤‘...")
        
        report = state["report"]
        sources = state["sources"]
        
        # ê°„ë‹¨í•œ ì‹ ë¢°ë„ ê³„ì‚°
        # ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ê²€ì¦ ë¡œì§
        confidence = min(1.0, len(sources) / 5.0)
        
        state["confidence"] = confidence
        print(f"âœ… ì‹ ë¢°ë„: {confidence:.2f}")
        
        return state
    
    def check_confidence(self, state: ResearchState) -> str:
        """ì‹ ë¢°ë„ í™•ì¸"""
        if state["confidence"] < 0.6:
            print("âš ï¸ ì‹ ë¢°ë„ ë‚®ìŒ - ì¬ê²€ìƒ‰")
            return "retry"
        else:
            print("âœ… ì‹ ë¢°ë„ ì¶©ë¶„ - ì™„ë£Œ")
            return "complete"
    
    def run(self, query: str) -> dict:
        """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        initial_state = {
            "query": query,
            "search_results": [],
            "ranked_results": [],
            "key_findings": [],
            "report": None,
            "confidence": 0.0,
            "sources": []
        }
        
        result = self.workflow.invoke(initial_state)
        return result

# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    from mcp_client import MCPClient
    from llm_client import LLMClient
    
    mcp = MCPClient("http://localhost:8080")
    llm = LLMClient("ollama", model="llama3.2")
    
    workflow = ResearchWorkflow(mcp, llm)
    
    result = workflow.run("MCPì™€ A2A í”„ë¡œí† ì½œì˜ ì°¨ì´ì ")
    
    print("\n" + "="*50)
    print("ğŸ“Š ìµœì¢… ë³´ê³ ì„œ")
    print("="*50)
    print(result["report"])
    print(f"\nì‹ ë¢°ë„: {result['confidence']:.2f}")
    print(f"ì¶œì²˜ ìˆ˜: {len(result['sources'])}")
```

### 2. ì½”ë“œ ë¦¬ë·° ì›Œí¬í”Œë¡œìš°

```python
# orchestration/workflows/code_review_workflow.py
from typing import TypedDict, List
from langgraph.graph import StateGraph, START, END

class CodeReviewState(TypedDict):
    code: str
    language: str
    issues: List[dict]
    suggestions: List[dict]
    refactored_code: Optional[str]
    test_coverage: float

class CodeReviewWorkflow:
    def __init__(self, llm):
        self.llm = llm
        self.workflow = self.build_workflow()
    
    def build_workflow(self):
        workflow = StateGraph(CodeReviewState)
        
        workflow.add_node("analyze", self.analyze_code)
        workflow.add_node("security", self.check_security)
        workflow.add_node("performance", self.check_performance)
        workflow.add_node("style", self.check_style)
        workflow.add_node("suggest", self.generate_suggestions)
        workflow.add_node("refactor", self.refactor_code)
        
        # ë³‘ë ¬ ê²€ì‚¬
        workflow.add_edge(START, "analyze")
        workflow.add_edge("analyze", "security")
        workflow.add_edge("analyze", "performance")
        workflow.add_edge("analyze", "style")
        
        # ëª¨ë‘ ì™„ë£Œ í›„ ì œì•ˆ
        workflow.add_edge("security", "suggest")
        workflow.add_edge("performance", "suggest")
        workflow.add_edge("style", "suggest")
        
        # ë¦¬íŒ©í† ë§
        workflow.add_conditional_edges(
            "suggest",
            lambda s: "refactor" if len(s["issues"]) > 0 else "skip",
            {
                "refactor": "refactor",
                "skip": END
            }
        )
        
        workflow.add_edge("refactor", END)
        
        return workflow.compile()
    
    def analyze_code(self, state: CodeReviewState) -> CodeReviewState:
        """ì½”ë“œ êµ¬ì¡° ë¶„ì„"""
        print("ğŸ” ì½”ë“œ ë¶„ì„ ì¤‘...")
        
        # ì½”ë“œ ë³µì¡ë„, ë¼ì¸ ìˆ˜ ë“± ë¶„ì„
        # ...
        
        return state
    
    def check_security(self, state: CodeReviewState) -> CodeReviewState:
        """ë³´ì•ˆ ê²€ì‚¬"""
        print("ğŸ”’ ë³´ì•ˆ ê²€ì‚¬ ì¤‘...")
        
        prompt = f"""
        ë‹¤ìŒ {state["language"]} ì½”ë“œì˜ ë³´ì•ˆ ì·¨ì•½ì ì„ ì°¾ìœ¼ì„¸ìš”:
        
        {state["code"]}
        
        SQL ì¸ì ì…˜, XSS, í•˜ë“œì½”ë”©ëœ ë¹„ë°€ë²ˆí˜¸ ë“±ì„ í™•ì¸í•˜ì„¸ìš”.
        """
        
        issues = self.llm.invoke(prompt)
        
        if "issues" not in state:
            state["issues"] = []
        state["issues"].extend(self.parse_issues(issues, "security"))
        
        return state
    
    def check_performance(self, state: CodeReviewState) -> CodeReviewState:
        """ì„±ëŠ¥ ê²€ì‚¬"""
        print("âš¡ ì„±ëŠ¥ ê²€ì‚¬ ì¤‘...")
        
        # ì„±ëŠ¥ ì´ìŠˆ íƒì§€
        # ...
        
        return state
    
    def check_style(self, state: CodeReviewState) -> CodeReviewState:
        """ìŠ¤íƒ€ì¼ ê²€ì‚¬"""
        print("ğŸ¨ ìŠ¤íƒ€ì¼ ê²€ì‚¬ ì¤‘...")
        
        # ì½”ë”© ìŠ¤íƒ€ì¼ ê²€ì‚¬
        # ...
        
        return state
    
    def generate_suggestions(self, state: CodeReviewState) -> CodeReviewState:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        print("ğŸ’¡ ê°œì„  ì œì•ˆ ìƒì„± ì¤‘...")
        
        issues = state["issues"]
        
        suggestions = []
        for issue in issues:
            suggestion = f"Fix {issue['type']}: {issue['description']}"
            suggestions.append({
                "issue": issue,
                "suggestion": suggestion
            })
        
        state["suggestions"] = suggestions
        
        return state
    
    def refactor_code(self, state: CodeReviewState) -> CodeReviewState:
        """ì½”ë“œ ë¦¬íŒ©í† ë§"""
        print("ğŸ”§ ì½”ë“œ ë¦¬íŒ©í† ë§ ì¤‘...")
        
        suggestions = state["suggestions"]
        original_code = state["code"]
        
        prompt = f"""
        ë‹¤ìŒ ì½”ë“œë¥¼ ê°œì„  ì œì•ˆì— ë”°ë¼ ë¦¬íŒ©í† ë§í•˜ì„¸ìš”:
        
        ì›ë³¸ ì½”ë“œ:
        {original_code}
        
        ê°œì„  ì œì•ˆ:
        {json.dumps(suggestions, ensure_ascii=False)}
        
        ë¦¬íŒ©í† ë§ëœ ì½”ë“œë§Œ ë°˜í™˜í•˜ì„¸ìš”.
        """
        
        refactored = self.llm.invoke(prompt)
        state["refactored_code"] = refactored
        
        return state
    
    def parse_issues(self, text: str, category: str) -> List[dict]:
        """ì´ìŠˆ íŒŒì‹±"""
        # ê°„ì†Œí™”ëœ íŒŒì‹±
        return [
            {
                "type": category,
                "severity": "high",
                "description": text
            }
        ]
```

### 3. ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì›Œí¬í”Œë¡œìš°

```python
# orchestration/workflows/data_pipeline_workflow.py
from typing import TypedDict, List
from langgraph.graph import StateGraph, START, END
import pandas as pd

class DataPipelineState(TypedDict):
    source_url: str
    raw_data: Optional[pd.DataFrame]
    cleaned_data: Optional[pd.DataFrame]
    transformed_data: Optional[pd.DataFrame]
    validation_errors: List[str]
    output_path: Optional[str]

class DataPipelineWorkflow:
    def __init__(self):
        self.workflow = self.build_workflow()
    
    def build_workflow(self):
        workflow = StateGraph(DataPipelineState)
        
        workflow.add_node("extract", self.extract_data)
        workflow.add_node("clean", self.clean_data)
        workflow.add_node("transform", self.transform_data)
        workflow.add_node("validate", self.validate_data)
        workflow.add_node("load", self.load_data)
        
        workflow.add_edge(START, "extract")
        workflow.add_edge("extract", "clean")
        workflow.add_edge("clean", "transform")
        workflow.add_edge("transform", "validate")
        
        # ê²€ì¦ ê²°ê³¼ì— ë”°ë¼ ë¶„ê¸°
        workflow.add_conditional_edges(
            "validate",
            lambda s: "load" if not s["validation_errors"] else "clean",
            {
                "load": "load",
                "clean": "clean"  # ì¬ì²˜ë¦¬
            }
        )
        
        workflow.add_edge("load", END)
        
        return workflow.compile()
    
    def extract_data(self, state: DataPipelineState) -> DataPipelineState:
        """ë°ì´í„° ì¶”ì¶œ"""
        print(f"ğŸ“¥ ë°ì´í„° ì¶”ì¶œ ì¤‘: {state['source_url']}")
        
        # CSV, API ë“±ì—ì„œ ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(state["source_url"])
        
        state["raw_data"] = df
        print(f"âœ… {len(df)} í–‰ ì¶”ì¶œ ì™„ë£Œ")
        
        return state
    
    def clean_data(self, state: DataPipelineState) -> DataPipelineState:
        """ë°ì´í„° ì •ì œ"""
        print("ğŸ§¹ ë°ì´í„° ì •ì œ ì¤‘...")
        
        df = state["raw_data"].copy()
        
        # ì¤‘ë³µ ì œê±°
        df = df.drop_duplicates()
        
        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        df = df.fillna(method='ffill')
        
        # ì´ìƒì¹˜ ì œê±°
        # ...
        
        state["cleaned_data"] = df
        print(f"âœ… {len(df)} í–‰ ì •ì œ ì™„ë£Œ")
        
        return state
    
    def transform_data(self, state: DataPipelineState) -> DataPipelineState:
        """ë°ì´í„° ë³€í™˜"""
        print("ğŸ”„ ë°ì´í„° ë³€í™˜ ì¤‘...")
        
        df = state["cleaned_data"].copy()
        
        # ì»¬ëŸ¼ ë³€í™˜
        # íŒŒìƒ ë³€ìˆ˜ ìƒì„±
        # ì§‘ê³„
        # ...
        
        state["transformed_data"] = df
        print("âœ… ë³€í™˜ ì™„ë£Œ")
        
        return state
    
    def validate_data(self, state: DataPipelineState) -> DataPipelineState:
        """ë°ì´í„° ê²€ì¦"""
        print("âœ”ï¸ ë°ì´í„° ê²€ì¦ ì¤‘...")
        
        df = state["transformed_data"]
        errors = []
        
        # ìŠ¤í‚¤ë§ˆ ê²€ì¦
        required_columns = ["id", "value", "timestamp"]
        missing = set(required_columns) - set(df.columns)
        if missing:
            errors.append(f"Missing columns: {missing}")
        
        # ë°ì´í„° íƒ€ì… ê²€ì¦
        # ...
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ê²€ì¦
        # ...
        
        state["validation_errors"] = errors
        
        if errors:
            print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {len(errors)} ì—ëŸ¬")
        else:
            print("âœ… ê²€ì¦ í†µê³¼")
        
        return state
    
    def load_data(self, state: DataPipelineState) -> DataPipelineState:
        """ë°ì´í„° ë¡œë“œ"""
        print("ğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘...")
        
        df = state["transformed_data"]
        output_path = "/output/processed_data.csv"
        
        df.to_csv(output_path, index=False)
        
        state["output_path"] = output_path
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")
        
        return state
```

## ì§„í–‰ ìƒí™© ì¶”ì 

### Progress Callback

```python
from typing import Callable

class ProgressTracker:
    def __init__(self, callback: Callable[[str, int, int], None]):
        self.callback = callback
        self.current_step = 0
        self.total_steps = 0
    
    def start(self, total_steps: int):
        self.total_steps = total_steps
        self.current_step = 0
    
    def step(self, message: str):
        self.current_step += 1
        self.callback(message, self.current_step, self.total_steps)
    
    def complete(self):
        self.callback("ì™„ë£Œ", self.total_steps, self.total_steps)

# ì‚¬ìš© ì˜ˆì œ
def progress_callback(message: str, current: int, total: int):
    progress = (current / total) * 100
    print(f"[{progress:.0f}%] {message}")

tracker = ProgressTracker(progress_callback)

# ì›Œí¬í”Œë¡œìš°ì— í†µí•©
class TrackedWorkflow:
    def __init__(self, tracker: ProgressTracker):
        self.tracker = tracker
    
    def search_node(self, state):
        self.tracker.step("ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
        # ê²€ìƒ‰ ë¡œì§
        return state
    
    def analyze_node(self, state):
        self.tracker.step("ë¬¸ì„œ ë¶„ì„ ì¤‘...")
        # ë¶„ì„ ë¡œì§
        return state
```

## ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„

### Retry ë©”ì»¤ë‹ˆì¦˜

```python
from tenacity import retry, stop_after_attempt, wait_exponential

class ResilientWorkflow:
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10)
    )
    def search_with_retry(self, query: str):
        """ì¬ì‹œë„ ê°€ëŠ¥í•œ ê²€ìƒ‰"""
        try:
            results = self.mcp_client.hybrid_search(query=query)
            if not results:
                raise ValueError("No results found")
            return results
        except Exception as e:
            print(f"ê²€ìƒ‰ ì‹¤íŒ¨, ì¬ì‹œë„ ì¤‘: {e}")
            raise
    
    def search_node(self, state: WorkflowState) -> WorkflowState:
        try:
            results = self.search_with_retry(state["query"])
            state["search_results"] = results
        except Exception as e:
            state["errors"].append(str(e))
            # fallback ë¡œì§
            state["search_results"] = []
        
        return state
```

### Circuit Breaker

```python
from pybreaker import CircuitBreaker

class WorkflowWithCircuitBreaker:
    def __init__(self):
        self.breaker = CircuitBreaker(
            fail_max=5,
            reset_timeout=60
        )
    
    def call_external_api(self, url: str):
        @self.breaker
        def _call():
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            return response.json()
        
        try:
            return _call()
        except CircuitBreakerError:
            print("Circuit breaker open - using cache")
            return self.get_cached_data()
```

## ì²´í¬í¬ì¸íŠ¸ ë° ë³µêµ¬

### ìƒíƒœ ì €ì¥

```python
from langgraph.checkpoint.sqlite import SqliteSaver

# ì²´í¬í¬ì¸íŠ¸ ì €ì¥ì†Œ
memory = SqliteSaver.from_conn_string(":memory:")

# ì²´í¬í¬ì¸íŠ¸ í™œì„±í™”
app = workflow.compile(checkpointer=memory)

# ì‹¤í–‰ (ìë™ ì²´í¬í¬ì¸íŠ¸)
config = {"configurable": {"thread_id": "task_123"}}
result = app.invoke(initial_state, config)

# ì¤‘ë‹¨ëœ ì›Œí¬í”Œë¡œìš° ì¬ê°œ
resumed_result = app.invoke(None, config)  # ë§ˆì§€ë§‰ ìƒíƒœë¶€í„° ì¬ê°œ
```

## ì›Œí¬í”Œë¡œìš° ì‹œê°í™”

```python
from IPython.display import Image, display

# Mermaid ë‹¤ì´ì–´ê·¸ë¨ ìƒì„±
display(Image(app.get_graph().draw_mermaid_png()))
```

## í•µì‹¬ ìš”ì•½

### LangGraph ì¥ì 

- âœ… **ìƒíƒœ ê´€ë¦¬**: TypedDictë¡œ ëª…í™•í•œ ìƒíƒœ ì •ì˜
- âœ… **ë¶„ê¸°/ì¡°ê±´**: ì¡°ê±´ë¶€ ì—£ì§€ë¡œ ë³µì¡í•œ ë¡œì§
- âœ… **ì¬ì‹œë„**: tenacity, circuit breaker í†µí•©
- âœ… **ì²´í¬í¬ì¸íŠ¸**: ì¤‘ë‹¨/ì¬ê°œ ê°€ëŠ¥
- âœ… **ì‹œê°í™”**: ìë™ ê·¸ë˜í”„ ìƒì„±
- âœ… **ë””ë²„ê¹…**: ê° ë…¸ë“œë³„ ìƒíƒœ ì¶”ì 

### ì‹¤ë¬´ íŒ¨í„´

```
1. ìƒíƒœ ì •ì˜ (TypedDict)
2. ë…¸ë“œ êµ¬í˜„ (í•¨ìˆ˜)
3. ê·¸ë˜í”„ êµ¬ì„± (ì—£ì§€)
4. ì—ëŸ¬ ì²˜ë¦¬ (retry, circuit breaker)
5. ì§„í–‰ ì¶”ì  (callback)
6. ì²´í¬í¬ì¸íŠ¸ (ë³µêµ¬)
```

**ì‘ì„±ì¼**: 2024-12-13
