---
title: "프로덕션급 AI 에이전트 시스템의 7계층 아키텍처 분석"
date: 2026-01-02 09:00:00 +0900
categories: [AI,  Architecture]
tags: [AI,  Material,  ai-agent,  Medium,  Claude.write]
---


[**Building the 7 Layers of a Production-Grade Agentic AI System**](https://levelup.gitconnected.com/building-the-7-layers-of-a-production-grade-agentic-ai-system-37ee5d941f1c) - Service Layer, Middleware, Context Management and more


현대의 AI 에이전트 시스템은 단순한 프로토타입을 넘어 실제 프로덕션 환경에서 수천 명의 사용자를 안정적으로 처리할 수 있어야 합니다. 이를 위해서는 체계적인 아키텍처 설계와 운영 전략이 필수적입니다. 최근 LangGraph 1.0(2025년 10월 출시)과 Mem0 v1.0.0의 등장으로 프로덕션급 AI 시스템 구축이 한층 성숙해졌으며, 이 문서에서는 실제 운영 가능한 AI 에이전트 시스템의 핵심 계층들을 심층 분석합니다.

![production-grade-agentic-system](https://github.com/k82022603/k82022603.github.io/blob/bf048c49bb1e1b130acbebc1389e14f48f5b4c1b/assets/img/1_GB6tXauVBaHVGDE4L_FkYg.webp?raw=true)
**Production Grade Agentic System (Created by 
Fareed Khan)**

## 1. 모듈러 코드베이스 구축: 확장 가능한 기반 설계

프로덕션 시스템의 첫 단계는 명확한 모듈 분리입니다. 전통적인 단일 파일 스크립트와 달리, 엔터프라이즈급 AI 애플리케이션은 각 구성 요소를 독립적인 모듈로 분리하여 유지보수성과 확장성을 확보합니다. API 라우터, 핵심 비즈니스 로직, 데이터베이스 모델, 검증 스키마, 서비스 계층이 각각 독립된 디렉토리에 배치되며, 이는 Netflix의 Dispatch 프로젝트에서 영감을 받은 구조입니다.

의존성 관리 측면에서는 pyproject.toml을 활용한 현대적 접근이 표준이 되었습니다. 단순한 requirements.txt를 넘어서서 의존성 해결, 버전 관리, 빌드 시스템 명세를 포함하는 pyproject.toml은 복잡한 프로젝트에 필수적입니다. 특히 LangGraph 1.0.5(2025년 12월 기준 최신 버전)와 같은 프로덕션급 프레임워크를 사용할 때는 정확한 버전 핀닝이 중요합니다. 최신 LangGraph는 내구성 있는 상태 관리, 내장 지속성, 인간 개입 루프 패턴을 제공하며, 2.0까지 하위 호환성을 보장합니다.

환경 설정 관리는 개발, 스테이징, 프로덕션 환경별로 독립된 설정 파일을 유지하는 것이 핵심입니다. 단일 .env 파일 대신 .env.development, .env.staging, .env.production으로 분리하면 환경 전환 시 발생하는 설정 오류를 방지하고, 각 환경에 특화된 최적화를 적용할 수 있습니다. Pydantic Settings를 활용한 타입 안전한 설정 관리는 런타임 오류를 컴파일 타임으로 전환시켜 안정성을 크게 향상시킵니다.

컨테이너화 전략은 Docker Compose를 중심으로 전개됩니다. 프로덕션 시스템은 애플리케이션 서버, PostgreSQL(pgvector 확장 포함), Prometheus, Grafana, cAdvisor를 통합 오케스트레이션합니다. pgvector/pgvector:pg16 이미지를 사용하면 벡터 유사도 검색이 기본 제공되어 Mem0의 장기 기억 시스템과 원활히 통합됩니다. Mem0은 2025년 10월 $24M Series A 투자를 유치했으며, API 호출이 2025년 1분기 3,500만에서 3분기 1억 8,600만으로 급증하며 업계 표준 메모리 레이어로 자리잡았습니다.

## 2. 데이터 지속성 계층: 구조화된 관계형 모델링

SQLModel은 SQLAlchemy의 강력함과 Pydantic의 검증을 결합한 현대적 ORM입니다. 프로덕션 시스템에서는 BaseModel을 통한 공통 필드 추상화가 중요합니다. created_at 타임스탬프를 모든 테이블에 자동으로 추가하는 베이스 클래스는 DRY(Don't Repeat Yourself) 원칙을 따르며, UTC 기반 시간 관리로 타임존 문제를 원천 차단합니다.

사용자 인증 모델은 보안이 핵심입니다. 평문 비밀번호 저장은 절대 금기이며, bcrypt를 활용한 해싱과 솔팅이 필수입니다. User 모델에 verify_password와 hash_password 메서드를 캡슐화하면 비즈니스 로직에서 보안 실수를 방지할 수 있습니다. 이메일 필드는 unique=True, index=True로 설정하여 로그인 시 빠른 조회를 보장합니다.

세션 관리는 다중 대화 컨텍스트를 지원하는 구조로 설계됩니다. 사용자는 여러 개의 독립적인 채팅 세션을 가질 수 있으며, 각 세션은 UUID 기반 ID로 추측 불가능하게 관리됩니다. Session 모델은 User 모델과 외래 키로 연결되며, LangGraph의 체크포인트 시스템과 통합되어 대화 상태를 영구적으로 보존합니다.

DTO(Data Transfer Object) 패턴은 내부 데이터베이스 모델과 외부 API 응답을 분리합니다. Pydantic 스키마를 통한 입출력 검증은 XSS 공격, SQL 인젝션, 타입 오류를 사전에 차단합니다. UserCreate 스키마에서 비밀번호 복잡도를 검증하고, SecretStr 타입으로 로그 노출을 방지하는 등의 세밀한 보안 설계가 적용됩니다.

## 3. 보안 및 보호 계층: 다층 방어 전략

Rate Limiting은 API 남용을 방지하는 첫 번째 방어선입니다. SlowAPI를 활용하면 IP 주소 기반으로 요청 빈도를 제한할 수 있으며, 엔드포인트별로 세밀한 제어가 가능합니다. 예를 들어 로그인 엔드포인트는 분당 20회, 채팅 엔드포인트는 분당 100회로 차등 적용하여 봇 공격과 DDoS를 효과적으로 차단합니다. 2025년 FastAPI 프로덕션 베스트 프랙티스에 따르면 Nginx와 결합한 다층 Rate Limiting이 권장됩니다.

입력 검증(Sanitization)은 단순한 형식 검증을 넘어 적극적인 필터링을 포함합니다. HTML escape, script 태그 제거, null byte 제거가 기본이며, 정규식을 활용한 이메일 형식 검증도 필수입니다. 모든 사용자 입력은 신뢰할 수 없다는 원칙하에 다층 검증을 거쳐야 합니다.

JWT 인증은 무상태(stateless) 보안을 제공합니다. 매 요청마다 데이터베이스를 조회하지 않고도 암호화 서명으로 사용자를 검증할 수 있어 성능과 확장성이 뛰어납니다. JWT_SECRET_KEY는 환경 변수로 관리하며, 프로덕션에서는 최소 256비트 랜덤 문자열을 사용해야 합니다. 토큰에 JTI(JWT ID)를 포함하면 필요시 블랙리스트 구현도 가능합니다.

컨텍스트 윈도우 관리는 LLM 비용과 성능을 직접 좌우합니다. 대화가 길어지면 토큰 제한을 초과하거나 응답 품질이 저하되므로, 지능적인 메시지 트리밍이 필수입니다. LangChain의 trim_messages 함수는 모델별 토큰 카운터를 활용하여 최근 메시지를 우선 유지하면서 시스템 프롬프트를 보존합니다. 이를 통해 컨텍스트 일관성을 유지하면서도 토큰 한도 내에서 작동합니다.

## 4. 서비스 계층: 복원력 있는 인프라

Connection Pooling은 데이터베이스 성능의 핵심입니다. 매 요청마다 새 연결을 생성하면 핸드셰이크 오버헤드로 인해 응답 시간이 급증하고, 동시 사용자가 늘어나면 데이터베이스가 과부하로 중단됩니다. SQLAlchemy의 QueuePool을 사용하면 미리 확보된 연결을 재사용하여 레이턴시를 최소화합니다. pool_size=20, max_overflow=10 설정은 일반적으로 중소 규모(1만 동시 사용자) 시스템에 적합하며, pool_pre_ping=True로 끊긴 연결을 사전에 감지합니다.

LLM 가용성 처리는 단일 모델 의존성 리스크를 완화합니다. OpenAI API가 다운되거나 Rate Limit에 걸리면 서비스 전체가 중단될 수 있습니다. 이를 방지하기 위해 Circuit Breaker와 Fallback 패턴을 구현합니다. Tenacity 라이브러리를 활용하면 지수 백오프(exponential backoff)로 자동 재시도하며, 실패 시 대체 모델로 순환 전환합니다. gpt-4o 실패 시 gpt-4o-mini로 자동 전환하는 방식으로 가용성을 98% 이상 유지할 수 있습니다.

LLM 서비스 레지스트리는 모델을 동적으로 관리합니다. 각 모델을 사전 설정된 온도, 최대 토큰, 추론 노력도와 함께 등록하면, 런타임에 워크로드에 맞는 모델을 선택할 수 있습니다. 예를 들어 빠른 응답이 필요한 경우 gpt-4o-mini를, 복잡한 추론이 필요한 경우 gpt-5-mini를 사용하는 식입니다. 2025년 기준 OpenAI는 gpt-4.1-nano-2025-04-14와 같은 경량 모델도 제공하여 비용 최적화가 용이합니다.

## 5. 멀티 에이전트 아키텍처: 상태 관리와 도구 통합

LangGraph 1.0의 핵심 강점은 내구성 있는 실행(durable execution)입니다. 서버가 재시작되거나 네트워크가 끊겨도 에이전트는 마지막 체크포인트에서 정확히 재개됩니다. AsyncPostgresSaver를 사용하면 각 노드 실행 후 상태가 자동으로 PostgreSQL에 저장되며, 이는 수일간 지속되는 승인 프로세스나 백그라운드 작업에 필수적입니다. LangGraph Platform(현 LangSmith Deployment)은 2025년 10월 GA 출시로 엔터프라이즈급 배포와 관리를 더욱 간소화했습니다.

장기 기억(Long-Term Memory) 통합은 Mem0을 통해 구현됩니다. 단기 대화 히스토리는 현재 세션만 기억하지만, 장기 기억은 모든 세션에 걸친 사용자 선호도와 사실을 벡터 데이터베이스에 저장합니다. Mem0 v1.0.0은 pgvector와 원활히 통합되며, LOCOMO 벤치마크에서 OpenAI Memory 대비 26% 높은 정확도, 전체 컨텍스트 대비 91% 빠른 응답, 90% 낮은 토큰 사용량을 달성했습니다. 시스템 프롬프트에 관련 기억을 동적으로 주입하면 개인화된 응답이 가능합니다.

도구 호출(Tool Calling) 기능은 에이전트를 외부 시스템과 연결합니다. DuckDuckGo 검색 도구를 예로 들면, LLM이 최신 정보가 필요하다고 판단하면 자동으로 웹 검색을 실행하고 결과를 컨텍스트에 추가합니다. LangGraph의 노드 기반 아키텍처에서 tool_call 노드는 요청된 도구를 실행하고 결과를 ToolMessage로 반환하여 chat 노드로 루프백합니다. 2025년 12월 업데이트된 LangGraph는 도구가 그래프 상태를 직접 업데이트할 수 있어 더욱 강력합니다.

프롬프트 엔지니어링은 코드와 분리하여 관리합니다. Markdown 파일로 시스템 프롬프트를 저장하고, 런타임에 동적 변수를 주입하는 방식은 프롬프트 엔지니어가 코드 수정 없이 개선 작업을 할 수 있게 합니다. 에이전트 이름, 현재 날짜, 장기 기억 등을 템플릿 변수로 처리하면 유연성과 유지보수성이 크게 향상됩니다.

## 6. API 게이트웨이: 인증과 실시간 스트리밍

FastAPI의 Dependency Injection은 인증 로직을 재사용 가능하게 만듭니다. get_current_user 의존성을 정의하면 JWT 토큰을 자동으로 검증하고, 유효한 User 객체를 라우트에 주입합니다. 실패 시 401 에러를 자동으로 반환하므로 각 엔드포인트마다 인증 로직을 반복할 필요가 없습니다. 2025년 FastAPI 베스트 프랙티스에 따르면 의존성 캐싱을 활용하여 동일 요청 내 중복 호출을 방지해야 합니다.

세션 기반 토큰은 멀티 컨텍스트 대화를 지원합니다. 사용자 레벨 JWT 외에 세션별 JWT를 발급하면 프론트엔드가 대화 스레드를 쉽게 전환할 수 있습니다. 각 세션 ID는 LangGraph의 thread_id로 매핑되어 독립적인 상태 관리가 가능합니다.

Server-Sent Events(SSE) 스트리밍은 사용자 경험의 핵심입니다. LLM 응답은 느릴 수 있지만, 토큰을 실시간으로 전송하면 사용자는 즉각적인 피드백을 받습니다. FastAPI의 StreamingResponse와 async generator를 결합하면 SSE 구현이 간단합니다. data: {...}\n\n 형식으로 청크를 yield하며, 완료 시 done: true 플래그를 보내 클라이언트가 연결을 닫도록 합니다. Prometheus 메트릭으로 스트리밍 지연시간을 추적하여 성능을 모니터링합니다.

CORS 설정은 프로덕션에서 엄격하게 관리해야 합니다. allow_origins=["*"]는 개발에서는 편리하지만, 프로덕션에서는 보안 위험입니다. 명시적인 도메인 리스트를 환경 변수로 관리하고, max_age를 설정하여 preflight 요청 오버헤드를 줄입니다. 2025년 표준에 따르면 TLS 1.3와 HSTS 헤더를 함께 적용해야 합니다.

## 7. 관측성 및 운영 테스트: 데이터 기반 최적화

Prometheus 메트릭은 시스템 건강도를 정량화합니다. HTTP 요청 카운터, 레이턴시 히스토그램, 데이터베이스 연결 게이지, LLM 추론 시간을 수집하면 병목 지점을 정확히 파악할 수 있습니다. 특히 LLM 호출은 일반 API 호출보다 훨씬 느리므로 별도의 버킷(0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0초)으로 측정합니다. Grafana 대시보드에서 P50, P95, P99 백분위수를 시각화하면 성능 저하를 조기에 발견할 수 있습니다.

Middleware 기반 로깅은 요청별 컨텍스트를 자동으로 바인딩합니다. LoggingContextMiddleware가 JWT에서 user_id와 session_id를 추출하여 structlog 컨텍스트에 주입하면, 이후 모든 로그에 자동으로 포함됩니다. 이는 수천 건의 동시 요청 중 특정 사용자의 오류를 추적할 때 필수적입니다. 프로덕션에서는 JSON 형식 로그로 전환하여 ELK 스택이나 CloudWatch와 통합합니다.

Langfuse 통합은 LLM 호출을 추적합니다. CallbackHandler를 LangGraph 설정에 추가하면 각 노드 실행, 프롬프트, 응답, 도구 호출이 자동으로 기록됩니다. 토큰 사용량과 비용도 실시간으로 추적되어 예산 관리가 가능합니다. Langfuse 대시보드에서 특정 trace_id로 전체 에이전트 실행 경로를 시각화할 수 있어 디버깅이 크게 간소화됩니다.

Health Check 엔드포인트는 두 가지 레벨로 구현합니다. 기본 /health는 프로세스 생존 여부만 확인하는 liveness probe이고, /health/detailed는 데이터베이스, Redis, 외부 API 연결까지 검증하는 readiness probe입니다. Kubernetes 환경에서는 livenessProbe와 readinessProbe를 각각 설정하여 자가 치유(self-healing)를 활성화합니다.

## 평가 프레임워크: LLM-as-a-Judge

프로덕션 AI 시스템은 지속적인 평가가 필수입니다. 전통적인 소프트웨어와 달리 AI는 확률적이므로, 프롬프트 변경이나 모델 업데이트가 예상치 못한 부작용을 일으킬 수 있습니다. LLM-as-a-Judge 패턴은 강력한 모델(gpt-4o)을 평가자로 사용하여 에이전트 응답의 품질을 자동으로 채점합니다.

평가 메트릭은 다차원적으로 설계됩니다. Hallucination(환각) 점수는 사실 오류를 감지하고, Toxicity(유해성) 점수는 부적절한 언어를 탐지하며, Relevancy(관련성) 점수는 응답이 질문에 얼마나 부합하는지 측정합니다. 각 메트릭은 독립적인 Markdown 프롬프트로 정의되어 평가 기준을 명확히 문서화합니다.

Pydantic ScoreSchema는 평가 결과를 구조화합니다. 0.0에서 1.0 사이의 점수와 함께 reasoning 필드로 근거를 요구하면, 블랙박스 채점을 방지하고 평가 투명성을 확보할 수 있습니다. 평가 결과는 Langfuse에 자동으로 업로드되어 시계열 대시보드로 추세를 모니터링합니다. Hallucination 점수가 급증하면 즉시 알림을 받아 프롬프트를 롤백할 수 있습니다.

자동화된 평가 파이프라인은 CI/CD와 통합됩니다. 매일 밤 또는 배포 전에 최근 trace를 샘플링하여 평가를 실행하고, 품질 임계값을 통과해야 프로덕션에 배포됩니다. 이는 AI 시스템에 전통적인 소프트웨어의 회귀 테스트에 해당하는 안전장치를 제공합니다.

## 스트레스 테스팅: 실전 검증

AWS m6i.xlarge 인스턴스(4 vCPU, 16 GiB RAM)에서 1,500 동시 사용자 시뮬레이션은 시스템 한계를 파악하는 실전 테스트입니다. aiohttp의 비동기 클라이언트로 사용자당 로그인→세션 생성→채팅 플로우를 완전히 재현하면, 데이터베이스 연결 풀 고갈, Rate Limit 충돌, 레이턴시 급증 같은 문제를 사전에 발견할 수 있습니다.

테스트 결과 분석은 정량적이어야 합니다. 98.4% 성공률은 우수하지만, 실패한 1.6%(24건)의 원인을 분석해야 합니다. OpenAI Rate Limit 429 에러가 대부분이었다면 Fallback 로직이 작동했다는 증거입니다. Prometheus 쿼리로 RPS(Requests Per Second)를 측정하면 각 엔드포인트의 처리량을 정확히 파악할 수 있습니다. /api/v1/chatbot/chat가 180 RPS를 처리했다면, 이는 복잡한 LLM 워크플로우 치고 인상적인 수치입니다.

Langfuse trace 분석은 비용 최적화에 직접 연결됩니다. 각 채팅의 토큰 사용량과 비용을 추적하면, 평균 쿼리당 $0.0003~$0.0005 범위를 확인할 수 있습니다. 레이턴시 분포(0.98s~2.10s)를 분석하면 캐시 히트율과 모델별 성능 차이를 이해할 수 있습니다. P99 레이턴시가 5초를 초과하면 사용자 경험이 저하되므로, 이를 기준으로 알림을 설정합니다.

지속적인 부하 테스트(Soak Test)는 메모리 누수를 탐지합니다. 수시간 동안 일정한 부하를 유지하면서 메모리 사용량을 모니터링하면, 연결 누수나 캐시 미스매니지먼트를 발견할 수 있습니다. cAdvisor 메트릭으로 컨테이너별 CPU와 메모리 사용량을 추적하여 리소스 최적화 지점을 찾습니다.

## 최신 기술 스택 통합 (2025년 기준)

LangGraph 1.0의 주요 업데이트는 프로덕션 안정성에 집중되어 있습니다. Node/task level caching으로 중복 계산을 제거하고, Deferred nodes로 모든 업스트림 경로 완료 후 실행을 지연시킬 수 있습니다. Semantic search for long-term memory는 정확한 매칭 대신 의미 기반 검색을 지원하여 기억 시스템의 정확도를 크게 향상시켰습니다. Command 도구는 동적 엣지리스 플로우를 가능케 하여 노드가 런타임에 다음 실행 노드를 결정할 수 있게 합니다.

FastAPI 프로덕션 배포는 Gunicorn + Uvicorn workers 조합이 표준입니다. Gunicorn은 프로세스 관리를, Uvicorn은 ASGI 서버를 담당하여 멀티코어 활용과 비동기 처리를 동시에 달성합니다. Worker 수는 CPU 코어 수와 동일하게 설정하며(2코어면 2 workers), max_requests와 max_requests_jitter로 메모리 누수를 방지합니다. 2025년 베스트 프랙티스는 Nginx를 리버스 프록시로 앞단에 배치하여 SSL 종료, Rate Limiting, 정적 파일 서빙을 오프로드하는 것입니다.

Mem0의 최신 개선사항은 프로덕션 준비도를 높였습니다. API 현대화, 향상된 벡터 스토어 지원, GCP 통합 강화가 v1.0.0의 핵심입니다. AWS Agent SDK의 독점 메모리 프로바이더로 선정된 것은 엔터프라이즈 신뢰성을 입증합니다. CrewAI, Flowise, Langflow 같은 주요 에이전트 프레임워크가 Mem0를 네이티브로 통합하여 에코시스템이 급속히 확장되고 있습니다.

DevOps 자동화는 GitHub Actions와 Docker Hub를 중심으로 구성됩니다. 매 푸시마다 Docker 이미지를 자동 빌드하고, 테스트를 통과하면 레지스트리에 푸시하는 CI/CD 파이프라인은 수동 배포 오류를 제거합니다. Makefile로 복잡한 Docker 명령을 추상화하면 팀원들이 일관된 명령어로 작업할 수 있습니다. Kubernetes 배포 시에는 readinessProbe, livenessProbe, HPA(Horizontal Pod Autoscaler)를 반드시 설정하여 자가 치유와 자동 스케일링을 활성화합니다.

## 결론: 프로덕션급 AI의 핵심 원칙

진정한 프로덕션급 AI 에이전트 시스템은 단순히 작동하는 것을 넘어 **복원력(Resilience)**, **관측성(Observability)**, **확장성(Scalability)**을 갖춰야 합니다. 7계층 아키텍처는 이를 체계적으로 달성하는 청사진을 제공합니다. 모듈러 코드베이스는 장기 유지보수를, 데이터 지속성 계층은 타입 안전성을, 보안 계층은 다층 방어를, 서비스 계층은 고가용성을, 멀티 에이전트 아키텍처는 상태 관리를, API 게이트웨이는 사용자 경험을, 관측성 계층은 지속적 개선을 책임집니다.

2025년 현재 LangGraph 1.0, Mem0 v1.0.0, FastAPI의 성숙한 에코시스템은 엔터프라이즈급 AI 시스템 구축을 그 어느 때보다 접근 가능하게 만들었습니다. Uber, LinkedIn, Klarna 같은 대기업들이 이미 프로덕션에서 검증했으며, 이제는 스타트업도 동일한 품질의 인프라를 구축할 수 있습니다. 핵심은 처음부터 프로덕션을 염두에 두고 설계하는 것이며, 프로토타입 단계에서 이미 보안, 관측성, 복원력을 고려해야 합니다.

앞으로 AI 에이전트 시스템은 더욱 복잡해지고 중요해질 것입니다. 멀티 에이전트 협업, 분산 실행, 크로스 플랫폼 메모리 공유가 표준이 될 것이며, 이를 안정적으로 운영하려면 탄탄한 아키텍처 기반이 필수적입니다. 이 7계층 프레임워크는 그 여정의 출발점이자 성공적인 프로덕션 배포를 위한 검증된 로드맵입니다.

---

## 핵심 요약

### 7계층 아키텍처 개요

1. **모듈러 코드베이스**: pyproject.toml 기반 의존성 관리, 환경별 설정 분리, Docker Compose 오케스트레이션
2. **데이터 지속성**: SQLModel ORM, bcrypt 보안, UUID 세션, DTO 패턴
3. **보안/보호**: Rate Limiting, Sanitization, JWT 인증, 컨텍스트 윈도우 관리
4. **서비스 계층**: Connection Pooling, Circuit Breaker, LLM Fallback, 모델 레지스트리
5. **멀티 에이전트**: LangGraph 1.0 내구성 실행, Mem0 장기 기억, 도구 통합
6. **API 게이트웨이**: Dependency Injection, SSE 스트리밍, 세션 토큰, CORS
7. **관측성**: Prometheus/Grafana, Middleware 로깅, Langfuse 추적, Health Check

### 핵심 기술 스택 (2025년 기준)

- **프레임워크**: LangGraph 1.0.5, FastAPI, LangChain 1.0
- **메모리**: Mem0 v1.0.0 (26% ↑ 정확도, 91% ↑ 속도, 90% ↓ 비용)
- **데이터베이스**: PostgreSQL + pgvector, SQLAlchemy QueuePool
- **인프라**: Docker Compose, Gunicorn + Uvicorn, Nginx
- **관측성**: Prometheus, Grafana, Langfuse, structlog
- **보안**: JWT, bcrypt, SlowAPI, Pydantic validation

### 성능 벤치마크

- 1,500 동시 사용자 처리: 98.4% 성공률
- 평균 레이턴시: 1.2초 (LLM 워크플로우)
- RPS: 180 (채팅 엔드포인트), 245 (로그인)
- 비용: $0.0003~$0.0005/쿼리
- 가용성: 98%+ (자동 Fallback 포함)

### 프로덕션 체크리스트

✅ 환경별 설정 파일 분리 (.env.development, .env.production)  
✅ Connection Pool 설정 (pool_size, max_overflow, pool_pre_ping)  
✅ Rate Limiting (엔드포인트별 차등 적용)  
✅ JWT 토큰 보안 (256bit secret, JTI 포함)  
✅ LLM Fallback 전략 (Circuit Breaker + 모델 순환)  
✅ 장기 기억 통합 (Mem0 + pgvector)  
✅ SSE 스트리밍 (토큰별 실시간 응답)  
✅ Prometheus/Grafana 모니터링  
✅ Health Check (liveness + readiness probe)  
✅ LLM-as-a-Judge 평가 (Hallucination, Toxicity, Relevancy)  
✅ 스트레스 테스트 (동시 사용자 시뮬레이션)  
✅ CI/CD 파이프라인 (GitHub Actions + Docker Hub)

---

**작성일자: 2026-01-02**