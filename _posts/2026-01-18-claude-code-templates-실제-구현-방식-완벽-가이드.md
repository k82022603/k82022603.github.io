---
title: "Claude Code Templates ì‹¤ì œ êµ¬í˜„ ë°©ì‹ ì™„ë²½ ê°€ì´ë“œ"
date: 2026-01-18 15:00:00 +0900
categories: [AI,  Claude Code]
mermaid: [True]
tags: [AI,  claude-code-templates,  Claude.write]
---


## ë¬¸ì„œ ê°œìš”

ì´ ë¬¸ì„œëŠ” Claude Code Templatesì˜ ë‚´ë¶€ êµ¬ì¡°ì™€ ì‹¤ì œ êµ¬í˜„ ë°©ì‹ì„ ìƒì„¸íˆ ë‹¤ë£¹ë‹ˆë‹¤. ë‹¨ìˆœíˆ ì‚¬ìš©ë²•ì„ ë„˜ì–´ì„œ, Agent, Command, MCP, Skillsê°€ ì–´ë–»ê²Œ êµ¬ì„±ë˜ê³  ì‘ë™í•˜ëŠ”ì§€, ê·¸ë¦¬ê³  ì§ì ‘ ì»¤ìŠ¤í…€ ì»´í¬ë„ŒíŠ¸ë¥¼ ë§Œë“¤ ë•Œ í•„ìš”í•œ ëª¨ë“  ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ì„ ì œê³µí•©ë‹ˆë‹¤.

## GitHUB

[claude-code-templates](https://github.com/davila7/claude-code-templates)

## íŒŒì¼ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ì €ì¥ì†Œ êµ¬ì¡°

Claude Code Templates ì €ì¥ì†ŒëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê³„ì¸µ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤:

```
claude-code-templates/
â”œâ”€â”€ cli-tool/                      # CLI ë„êµ¬ ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ index.js              # ë©”ì¸ ì„¤ì¹˜ ë¡œì§
â”‚   â”‚   â”œâ”€â”€ analytics.js          # Analytics Dashboard
â”‚   â”‚   â””â”€â”€ health-check.js       # Health Check ë„êµ¬
â”‚   â”œâ”€â”€ bin/
â”‚   â”‚   â””â”€â”€ create-claude-config.js  # CLI ì§„ì…ì 
â”‚   â””â”€â”€ components/               # ëª¨ë“  í…œí”Œë¦¿ íŒŒì¼
â”‚       â”œâ”€â”€ agents/
â”‚       â”‚   â”œâ”€â”€ development-team/
â”‚       â”‚   â”œâ”€â”€ development-tools/
â”‚       â”‚   â”œâ”€â”€ ai-specialists/
â”‚       â”‚   â””â”€â”€ business-team/
â”‚       â”œâ”€â”€ commands/
â”‚       â”‚   â”œâ”€â”€ testing/
â”‚       â”‚   â”œâ”€â”€ performance/
â”‚       â”‚   â””â”€â”€ deployment/
â”‚       â”œâ”€â”€ mcps/
â”‚       â”‚   â”œâ”€â”€ development/
â”‚       â”‚   â”œâ”€â”€ database/
â”‚       â”‚   â””â”€â”€ cloud/
â”‚       â”œâ”€â”€ settings/
â”‚       â”œâ”€â”€ hooks/
â”‚       â””â”€â”€ skills/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ index.html               # ì›¹ ì¹´íƒˆë¡œê·¸ í”„ë¡ íŠ¸ì—”ë“œ
â”‚   â”œâ”€â”€ components.json          # ì»´í¬ë„ŒíŠ¸ í†µí•© ì¹´íƒˆë¡œê·¸
â”‚   â””â”€â”€ js/
â”œâ”€â”€ api/                         # Vercel Serverless Functions
â”‚   â””â”€â”€ track-download-supabase.js
â”œâ”€â”€ generate_components_json.py  # ì¹´íƒˆë¡œê·¸ ìƒì„± ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ package.json
```

### ì„¤ì¹˜ í›„ í”„ë¡œì íŠ¸ êµ¬ì¡°

CLIë¡œ ì»´í¬ë„ŒíŠ¸ë¥¼ ì„¤ì¹˜í•˜ë©´ í”„ë¡œì íŠ¸ì— ë‹¤ìŒ êµ¬ì¡°ê°€ ìƒì„±ë©ë‹ˆë‹¤:

```
your-project/
â”œâ”€â”€ .claude/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ code-reviewer.md
â”‚   â”‚   â””â”€â”€ security-auditor.md
â”‚   â”œâ”€â”€ commands/
â”‚   â”‚   â”œâ”€â”€ generate-tests.md
â”‚   â”‚   â””â”€â”€ deploy-staging.md
â”‚   â”œâ”€â”€ mcps/
â”‚   â”‚   â””â”€â”€ mcp-servers.json
â”‚   â”œâ”€â”€ settings/
â”‚   â”‚   â””â”€â”€ progressive-disclosure.yaml
â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â””â”€â”€ pre-commit-validation.sh
â”‚   â””â”€â”€ skills/
â”‚       â””â”€â”€ pdf-processor/
â”‚           â”œâ”€â”€ SKILL.md
â”‚           â”œâ”€â”€ scripts/
â”‚           â”œâ”€â”€ references/
â”‚           â””â”€â”€ assets/
â”œâ”€â”€ CLAUDE.md                    # í”„ë¡œì íŠ¸ ë©”íƒ€ë°ì´í„°
â””â”€â”€ [your project files]
```

**ì¤‘ìš”:** ì €ì¥ì†Œì˜ ì¹´í…Œê³ ë¦¬ ê²½ë¡œ(ì˜ˆ: `development-tools/code-reviewer.md`)ëŠ” ì„¤ì¹˜ ì‹œ í‰íƒ„í™”(flatten)ë˜ì–´ `.claude/agents/code-reviewer.md`ì²˜ëŸ¼ íŒŒì¼ëª…ë§Œ ìœ ì§€ë©ë‹ˆë‹¤.

## Agent êµ¬í˜„ ìƒì„¸

### Markdown íŒŒì¼ êµ¬ì¡°

AgentëŠ” ìˆœìˆ˜ Markdown íŒŒì¼ë¡œ ì‘ì„±ë˜ë©°, frontmatterëŠ” ì„ íƒì‚¬í•­ì…ë‹ˆë‹¤. ê¸°ë³¸ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

~~~markdown
# Agent Name

Brief description of the agent's purpose and capabilities.

## Expertise

- Domain knowledge area 1
- Domain knowledge area 2
- Domain knowledge area 3
- Key capabilities and specializations
- Primary use cases

## Instructions

You are a [Role] specialized in [Domain]. Your primary responsibilities include:

1. **[Responsibility Area 1]**: Detailed description of what you do in this area
   - Specific action or check
   - Another specific action
   - Guideline or best practice

2. **[Responsibility Area 2]**: Detailed description
   - Specific action or check
   - Another specific action

3. **[Responsibility Area 3]**: Detailed description

### Key Principles

- Principle 1: Explanation
- Principle 2: Explanation
- Principle 3: Explanation

### Workflow

When asked to perform your role:

1. First, analyze the context
2. Then, identify potential issues
3. Next, provide specific recommendations
4. Finally, explain your reasoning

## Examples

### Example 1: [Use Case Name]

**User Request:**
```
```

**Your Response:**
```
I'll analyze this authentication function for security and best practices.

[Detailed analysis]

Findings:
1. [Issue 1]: [Description]
   - Recommendation: [Specific fix]
   
2. [Issue 2]: [Description]
   - Recommendation: [Specific fix]

Summary: [Overall assessment]
```

### Example 2: [Another Use Case]

[Similar structure]

## Output Format

Your responses should:
- Start with a brief summary
- Provide structured findings
- Include specific code examples when relevant
- End with actionable recommendations
- Use clear, professional language

## Limitations

Be aware of:
- What you should NOT do
- When to defer to other agents
- What information you need before proceeding
~~~

### ì‹¤ì œ Code Reviewer Agent ì˜ˆì œ

~~~markdown
# Code Reviewer

Expert code reviewer specializing in quality, security, and best practices across multiple programming languages.

## Expertise

- Code quality assessment and improvement
- Security vulnerability identification
- Design pattern recognition and anti-pattern detection
- Performance optimization opportunities
- Test coverage analysis
- Documentation completeness
- Language-specific best practices (Python, JavaScript, Java, Go, etc.)

## Instructions

You are a Senior Code Reviewer with 15+ years of experience across diverse technology stacks. Your reviews are thorough, constructive, and educational. You prioritize:

1. **Security First**: Always check for common vulnerabilities
   - SQL injection, XSS, CSRF vulnerabilities
   - Authentication and authorization flaws
   - Data exposure risks
   - Cryptographic weaknesses

2. **Code Quality**: Maintain high standards
   - Readability and maintainability
   - Proper error handling
   - Appropriate use of design patterns
   - DRY (Don't Repeat Yourself) principle
   - SOLID principles adherence

3. **Performance**: Identify bottlenecks
   - Algorithm efficiency (Big O analysis)
   - Database query optimization
   - Memory management
   - Unnecessary computations

4. **Testing**: Ensure comprehensive coverage
   - Unit test presence and quality
   - Edge cases handled
   - Integration test coverage
   - Mocking appropriateness

### Key Principles

- **Be Constructive**: Always explain WHY something is an issue and HOW to fix it
- **Be Specific**: Reference line numbers and provide code examples
- **Prioritize**: Mark issues as Critical/High/Medium/Low
- **Educate**: Share knowledge about patterns and practices
- **Be Balanced**: Acknowledge good code alongside criticisms

### Review Workflow

For each code review request:

1. **Initial Scan**: Get overall understanding of the change
2. **Security Audit**: Check for vulnerabilities
3. **Logic Review**: Verify correctness and edge cases
4. **Quality Assessment**: Evaluate structure and patterns
5. **Test Review**: Check test coverage and quality
6. **Documentation Check**: Ensure adequate comments and docs
7. **Summary**: Provide structured findings with priorities

## Examples

### Example 1: Authentication Function Review

**User Request:**
```

def login_user(username, password):
    user = db.query("SELECT * FROM users WHERE username = '" + username + "'")
    if user and user.password == password:
        return create_session(user)
    return None
```

**Your Response:**
```
I'll review this authentication function for security and best practices.

ğŸ”´ **CRITICAL SECURITY ISSUES:**

1. **SQL Injection Vulnerability** (Line 2)
   - **Issue**: Direct string concatenation in SQL query
   - **Risk**: Attacker can inject arbitrary SQL
   - **Fix**: Use parameterized queries
   ```python
   user = db.query("SELECT * FROM users WHERE username = ?", (username,))
   ```

2. **Plain Text Password Comparison** (Line 3)
   - **Issue**: Comparing passwords in plain text
   - **Risk**: Passwords should never be stored or compared in plain text
   - **Fix**: Use password hashing
   ```python
   if user and check_password_hash(user.password_hash, password):
   ```

ğŸŸ¡ **MEDIUM PRIORITY ISSUES:**

3. **No Input Validation**
   - **Issue**: Username not validated before use
   - **Fix**: Add input validation
   ```python
   if not username or not isinstance(username, str):
       raise ValueError("Invalid username")
   ```

4. **Timing Attack Vulnerability**
   - **Issue**: Different execution paths leak information
   - **Fix**: Use constant-time comparison and always hash

5. **No Logging**
   - **Issue**: Failed login attempts not logged
   - **Fix**: Add security logging for audit trail

**Recommended Implementation:**
```python
import bcrypt
import logging

def login_user(username: str, password: str) -> Optional[Session]:
    # Input validation
    if not username or not isinstance(username, str):
        logging.warning(f"Invalid username format attempted")
        return None
    
    # Parameterized query
    user = db.query(
        "SELECT id, username, password_hash FROM users WHERE username = ?",
        (username,)
    )
    
    if not user:
        logging.warning(f"Login attempt for non-existent user: {username}")
        # Still hash to prevent timing attacks
        bcrypt.checkpw(password.encode('utf-8'), bcrypt.gensalt())
        return None
    
    # Constant-time password verification
    if bcrypt.checkpw(password.encode('utf-8'), user.password_hash):
        logging.info(f"Successful login: {username}")
        return create_session(user)
    
    logging.warning(f"Failed login attempt: {username}")
    return None
```

**Summary:**
- âŒ **DO NOT MERGE**: Critical security vulnerabilities must be fixed first
- ğŸ”´ 2 Critical issues (SQL injection, plain text password)
- ğŸŸ¡ 3 Medium issues (validation, timing attack, logging)
- âœ… After fixes, this will be production-ready
```

### Example 2: React Component Review

**User Request:**
```

function UserList() {
  const [users, setUsers] = useState([]);
  
  useEffect(() => {
    fetch('/api/users')
      .then(res => res.json())
      .then(data => setUsers(data));
  }, []);
  
  return (
    <div>
      {users.map(user => (
        <div key={user.id}>
          <h3>{user.name}</h3>
          <p>{user.email}</p>
        </div>
      ))}
    </div>
  );
}
```

**Your Response:**
```
I'll review this React component for best practices and potential issues.

ğŸŸ¡ **MEDIUM PRIORITY ISSUES:**

1. **Missing Error Handling** (Lines 5-7)
   - **Issue**: No error handling for failed API requests
   - **Impact**: User sees nothing when API fails
   - **Fix**: Add error state and handling

2. **No Loading State** (Lines 5-7)
   - **Issue**: No indication while data is loading
   - **Impact**: Poor UX, empty screen initially
   - **Fix**: Add loading state

3. **Memory Leak Risk** (Lines 5-7)
   - **Issue**: No cleanup if component unmounts during fetch
   - **Impact**: Potential state update on unmounted component warning
   - **Fix**: Use AbortController or cleanup flag

4. **Missing PropTypes/TypeScript**
   - **Issue**: No type checking
   - **Fix**: Add TypeScript or PropTypes

ğŸŸ¢ **GOOD PRACTICES OBSERVED:**

- âœ… Proper use of hooks (useState, useEffect)
- âœ… Correct dependency array in useEffect
- âœ… Proper key prop in list rendering
- âœ… Clean, readable code structure

**Recommended Implementation:**
```typescript
import { useState, useEffect } from 'react';

interface User {
  id: number;
  name: string;
  email: string;
}

function UserList() {
  const [users, setUsers] = useState<User[]>([]);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState<string | null>(null);
  
  useEffect(() => {
    const abortController = new AbortController();
    
    async function fetchUsers() {
      try {
        setLoading(true);
        setError(null);
        
        const response = await fetch('/api/users', {
          signal: abortController.signal
        });
        
        if (!response.ok) {
          throw new Error(`HTTP error! status: ${response.status}`);
        }
        
        const data = await response.json();
        setUsers(data);
      } catch (err) {
        if (err.name !== 'AbortError') {
          setError(err.message || 'Failed to fetch users');
          console.error('Error fetching users:', err);
        }
      } finally {
        setLoading(false);
      }
    }
    
    fetchUsers();
    
    return () => {
      abortController.abort();
    };
  }, []);
  
  if (loading) {
    return <div className="loading">Loading users...</div>;
  }
  
  if (error) {
    return (
      <div className="error">
        Error: {error}
        <button onClick={() => window.location.reload()}>Retry</button>
      </div>
    );
  }
  
  if (users.length === 0) {
    return <div className="empty">No users found</div>;
  }
  
  return (
    <div className="user-list">
      {users.map(user => (
        <div key={user.id} className="user-card">
          <h3>{user.name}</h3>
          <p>{user.email}</p>
        </div>
      ))}
    </div>
  );
}

export default UserList;
```

**Testing Recommendations:**
```typescript
// UserList.test.tsx
describe('UserList', () => {
  it('should show loading state initially', () => {
    render(<UserList />);
    expect(screen.getByText('Loading users...')).toBeInTheDocument();
  });
  
  it('should display users after successful fetch', async () => {
    const mockUsers = [
      { id: 1, name: 'Alice', email: 'alice@example.com' },
      { id: 2, name: 'Bob', email: 'bob@example.com' }
    ];
    
    global.fetch = jest.fn().mockResolvedValue({
      ok: true,
      json: async () => mockUsers
    });
    
    render(<UserList />);
    
    await waitFor(() => {
      expect(screen.getByText('Alice')).toBeInTheDocument();
      expect(screen.getByText('Bob')).toBeInTheDocument();
    });
  });
  
  it('should show error message on fetch failure', async () => {
    global.fetch = jest.fn().mockRejectedValue(new Error('Network error'));
    
    render(<UserList />);
    
    await waitFor(() => {
      expect(screen.getByText(/Error:/)).toBeInTheDocument();
    });
  });
});
```

**Summary:**
- âœ… **CAN MERGE WITH IMPROVEMENTS**: No blocking issues
- ğŸŸ¡ 4 Medium issues (error handling, loading state, cleanup, types)
- âœ… Good foundation with proper React patterns
- ğŸ“ Add tests before merging to production
```

## Output Format

Your code reviews should always follow this structure:

1. **Issue Classification** with emoji indicators:
   - ğŸ”´ Critical (security, data loss, crashes)
   - ğŸŸ¡ Medium (quality, performance, maintenance)
   - ğŸ”µ Low (style, minor improvements)
   - ğŸŸ¢ Good practices observed

2. **Each Issue Contains**:
   - **Issue**: Clear description
   - **Impact**: Why it matters
   - **Fix**: Specific solution with code example

3. **Final Summary**:
   - Merge recommendation (âœ… Can merge / âš ï¸ Needs fixes / âŒ Do not merge)
   - Count of issues by severity
   - Overall assessment
   - Next steps

## Limitations

- I do not have access to the full codebase, only the code shown
- I cannot run tests or execute code
- For complex architectural decisions, suggest discussing with the team
- For domain-specific business logic, defer to domain experts
- I provide suggestions, but final decisions rest with the team
~~~

### Agent í˜¸ì¶œ ë°©ì‹

ì„¤ì¹˜ëœ AgentëŠ” ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

**ë°©ì‹ 1: ë©˜ì…˜(@) ì‚¬ìš©**
```
```

**ë°©ì‹ 2: ëª…ì‹œì  ì§€ì •**
```
I need a code review. Use the code-reviewer agent.
```

ClaudeëŠ” `.claude/agents/code-reviewer.md` íŒŒì¼ì˜ ë‚´ìš©ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ë¡œë“œí•˜ì—¬, í•´ë‹¹ Agentì˜ ì—­í• ê³¼ ì§€ì¹¨ì— ë”°ë¼ ì‘ë‹µí•©ë‹ˆë‹¤.

## Command êµ¬í˜„ ìƒì„¸

### Frontmatter í•„ìˆ˜ ì‚¬ìš©

Anthropic ê³µì‹ ë¬¸ì„œì™€ Issue #65ì— ë”°ë¥´ë©´, CommandëŠ” **ë°˜ë“œì‹œ frontmatter**ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì „ì²´ ë§ˆí¬ë‹¤ìš´ ë‚´ìš©ì´ API ìš”ì²­ì— í¬í•¨ë˜ì–´ ë¶ˆí•„ìš”í•œ í† í°ì„ ì†Œë¹„í•©ë‹ˆë‹¤.

### ì˜¬ë°”ë¥¸ Command êµ¬ì¡°

~~~markdown
---
name: generate-tests
description: Automatically generate unit tests for code
category: testing
version: 1.0.0
author: Your Name
---

Analyze the provided code and generate comprehensive unit tests.

## Requirements

- Use the project's existing test framework (Jest, pytest, etc.)
- Cover happy path, edge cases, and error conditions
- Include setup and teardown if needed
- Follow naming conventions: test_*, should_*, it_should_*
- Aim for 80%+ code coverage

## Test Structure

For each function/method:
1. Test normal behavior
2. Test boundary conditions
3. Test error cases
4. Test interaction with dependencies (use mocks)

## Output Format

Provide:
- Test file name following project conventions
- Complete test code ready to run
- Brief explanation of what each test covers

## Example

For a function like:
```python
def calculate_discount(price, discount_percent):
    if discount_percent < 0 or discount_percent > 100:
        raise ValueError("Discount must be 0-100")
    return price * (1 - discount_percent / 100)
```

Generate tests like:
```python
import pytest
from calculator import calculate_discount

def test_calculate_discount_normal_case():
    assert calculate_discount(100, 10) == 90.0
    
def test_calculate_discount_no_discount():
    assert calculate_discount(100, 0) == 100.0
    
def test_calculate_discount_full_discount():
    assert calculate_discount(100, 100) == 0.0
    
def test_calculate_discount_invalid_negative():
    with pytest.raises(ValueError):
        calculate_discount(100, -5)
        
def test_calculate_discount_invalid_over_100():
    with pytest.raises(ValueError):
        calculate_discount(100, 150)
```
~~~

### ë³µì¡í•œ Command ì˜ˆì œ: CI/CD ìë™í™”

~~~markdown
---
name: run-ci
description: Run complete CI pipeline including tests, lint, build
category: testing
version: 2.1.0
dependencies:
  - test-framework
  - linter
  - build-tools
---

Execute the complete CI pipeline as it would run in the CI/CD system.

## Execution Steps

1. **Environment Check**
   - Verify all required tools are installed
   - Check environment variables
   - Validate configuration files

2. **Dependency Installation**
   - Install/update project dependencies
   - Verify lockfile integrity

3. **Code Quality**
   - Run linter (ESLint, Pylint, etc.)
   - Check code formatting (Prettier, Black, etc.)
   - Run type checker (TypeScript, mypy, etc.)

4. **Testing**
   - Run unit tests
   - Generate coverage report
   - Run integration tests if available
   - Run E2E tests if configured

5. **Build**
   - Compile/bundle the project
   - Verify build artifacts
   - Check bundle size

6. **Security**
   - Scan dependencies for vulnerabilities
   - Check for exposed secrets

## Failure Handling

If any step fails:
- Stop execution immediately
- Report which step failed
- Show relevant error messages
- Suggest fixes for common issues

## Success Criteria

All of the following must pass:
- âœ… Zero linting errors
- âœ… All tests passing
- âœ… Code coverage â‰¥ 80% (or project threshold)
- âœ… Build completes successfully
- âœ… No high-severity security vulnerabilities
- âœ… Type checking passes (if applicable)

## Output Format

Provide a structured summary:

```
CI Pipeline Results
===================

Environment: âœ… OK
Dependencies: âœ… OK
Linting: âœ… OK (0 errors, 3 warnings)
Type Check: âœ… OK
Tests: âœ… OK (145 passed, 0 failed)
Coverage: âœ… OK (87.3%)
Build: âœ… OK (bundle size: 234 KB)
Security: âš ï¸  WARNING (2 moderate vulnerabilities)

Overall: âœ… PASS (1 warning)

Details:
- Linting warnings:
  src/utils/legacy.js:45 - Deprecated API usage
  src/components/Modal.jsx:12 - Missing PropTypes
  src/api/client.js:89 - Console.log statement

- Security warnings:
  axios@0.21.1 - Update to 0.21.2 (moderate)
  lodash@4.17.19 - Update to 4.17.21 (moderate)

Recommendations:
1. Address linting warnings before next release
2. Update dependencies: npm audit fix
3. Consider adding PropTypes or migrating to TypeScript
```

## Project-Specific Adaptation

Check for project configuration and adapt commands:

**package.json scripts:**
```json
{
  "scripts": {
    "lint": "eslint .",
    "test": "jest",
    "build": "webpack"
  }
}
```

**Detected setup:**
- Language: JavaScript
- Framework: React (inferred from dependencies)
- Test runner: Jest
- Linter: ESLint
- Bundler: Webpack

**Execution commands:**
```bash
npm run lint
npm run test -- --coverage
npm run build
npm audit
```

For Python projects with pytest:
```bash
pylint src/
pytest --cov=src tests/
python setup.py build
safety check
```

## Interactive Mode

If critical failures occur, ask user:

```
The CI pipeline failed at the Testing stage.

Error: 15 tests failed in authentication module

Options:
1. Show detailed test output
2. Skip tests and continue to build
3. Abort pipeline
4. Run only failed tests again

What would you like to do?
```
~~~

### Command ì‚¬ìš© ë°©ë²•

```bash
# Claude Code ì„¸ì…˜ì—ì„œ
/generate-tests

# ë˜ëŠ” íŒŒì¼ ì§€ì •
/generate-tests src/auth/login.py

# ë˜ëŠ” ì „ì²´ ë””ë ‰í† ë¦¬
/generate-tests src/
```

## MCP(Model Context Protocol) êµ¬í˜„ ìƒì„¸

### JSON ì„¤ì • íŒŒì¼ êµ¬ì¡°

MCPëŠ” `.claude/mcp-servers.json` íŒŒì¼ì— ì •ì˜ë©ë‹ˆë‹¤:

```json
{
  "mcpServers": {
    "github": {
      "url": "https://api.github.com",
      "auth": {
        "type": "bearer",
        "token": "${GITHUB_TOKEN}"
      },
      "tools": [
        {
          "name": "create_issue",
          "description": "Create a new GitHub issue",
          "parameters": {
            "title": "string",
            "body": "string",
            "labels": "array",
            "assignees": "array"
          }
        },
        {
          "name": "list_pull_requests",
          "description": "List pull requests in a repository",
          "parameters": {
            "state": "enum[open,closed,all]",
            "sort": "enum[created,updated,popularity]",
            "direction": "enum[asc,desc]"
          }
        },
        {
          "name": "get_commit_history",
          "description": "Get commit history for a repository",
          "parameters": {
            "since": "date",
            "until": "date",
            "author": "string"
          }
        }
      ],
      "rateLimits": {
        "requestsPerHour": 5000,
        "requestsPerMinute": 60
      },
      "timeout": 30000,
      "retries": 3
    },
    
    "postgresql": {
      "url": "http://localhost:5432",
      "auth": {
        "type": "basic",
        "username": "${DB_USER}",
        "password": "${DB_PASSWORD}"
      },
      "tools": [
        {
          "name": "execute_query",
          "description": "Execute a SQL query and return results",
          "parameters": {
            "query": "string",
            "params": "array"
          },
          "security": {
            "readOnly": true,
            "allowedTables": ["users", "products", "orders"],
            "forbiddenKeywords": ["DROP", "DELETE", "TRUNCATE"]
          }
        },
        {
          "name": "get_schema",
          "description": "Get database schema information",
          "parameters": {
            "table": "string"
          }
        },
        {
          "name": "explain_query",
          "description": "Get query execution plan",
          "parameters": {
            "query": "string"
          }
        }
      ],
      "connectionPool": {
        "min": 2,
        "max": 10,
        "idleTimeout": 30000
      }
    },
    
    "slack": {
      "url": "https://slack.com/api",
      "auth": {
        "type": "bearer",
        "token": "${SLACK_BOT_TOKEN}"
      },
      "tools": [
        {
          "name": "post_message",
          "description": "Post a message to a Slack channel",
          "parameters": {
            "channel": "string",
            "text": "string",
            "blocks": "array",
            "thread_ts": "string"
          }
        },
        {
          "name": "upload_file",
          "description": "Upload a file to Slack",
          "parameters": {
            "channels": "array",
            "file": "binary",
            "title": "string",
            "initial_comment": "string"
          }
        },
        {
          "name": "list_channels",
          "description": "List all channels the bot has access to",
          "parameters": {
            "types": "enum[public,private,im,mpim]"
          }
        }
      ]
    }
  },
  
  "globalSettings": {
    "defaultTimeout": 30000,
    "enableLogging": true,
    "logLevel": "info",
    "logFile": ".claude/logs/mcp.log"
  }
}
```

### ì»¤ìŠ¤í…€ MCP ì„œë²„ êµ¬í˜„

Node.jsë¡œ ì»¤ìŠ¤í…€ MCP ì„œë²„ë¥¼ ë§Œë“œëŠ” ì™„ì „í•œ ì˜ˆì œ:

```javascript
// mcp-timetracking-server.js
const express = require('express');
const morgan = require('morgan');
const helmet = require('helmet');
const rateLimit = require('express-rate-limit');

const app = express();

// ë³´ì•ˆ ë° ë¯¸ë“¤ì›¨ì–´
app.use(helmet());
app.use(express.json());
app.use(morgan('combined'));

// Rate limiting
const limiter = rateLimit({
  windowMs: 15 * 60 * 1000, // 15ë¶„
  max: 100 // ìµœëŒ€ 100 ìš”ì²­
});
app.use(limiter);

// ì¸ì¦ ë¯¸ë“¤ì›¨ì–´
const authenticate = (req, res, next) => {
  const token = req.headers.authorization?.replace('Bearer ', '');
  
  if (!token || token !== process.env.MCP_AUTH_TOKEN) {
    return res.status(401).json({ 
      error: 'Unauthorized',
      message: 'Valid authentication token required' 
    });
  }
  
  // í† í°ì—ì„œ ì‚¬ìš©ì ID ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” JWT ë””ì½”ë”© ë“±)
  req.userId = 'user-from-token';
  next();
};

// ëª¨ë“  MCP ì—”ë“œí¬ì¸íŠ¸ì— ì¸ì¦ ì ìš©
app.use('/mcp', authenticate);

// MCP ì„œë²„ ë©”íƒ€ë°ì´í„°
app.get('/mcp/info', (req, res) => {
  res.json({
    name: "timetracking-mcp",
    version: "1.0.0",
    description: "Time tracking and project management integration",
    capabilities: [
      "time_entry",
      "time_query",
      "project_list",
      "report_generation"
    ],
    author: "Your Organization",
    documentation: "https://docs.yourorg.com/mcp/timetracking"
  });
});

// ë„êµ¬ 1: ì‘ì—… ì‹œê°„ ê¸°ë¡
app.post('/mcp/tools/time_entry', async (req, res) => {
  const { project_id, duration, description, date } = req.body;
  
  try {
    // ì…ë ¥ ê²€ì¦
    if (!project_id || !duration || !description) {
      return res.status(400).json({
        success: false,
        error: 'Missing required fields',
        required: ['project_id', 'duration', 'description']
      });
    }
    
    if (duration <= 0 || duration > 24) {
      return res.status(400).json({
        success: false,
        error: 'Duration must be between 0 and 24 hours'
      });
    }
    
    // ë‚´ë¶€ API í˜¸ì¶œ
    const result = await internalTimeAPI.createEntry({
      user_id: req.userId,
      project_id,
      duration,
      description,
      date: date || new Date().toISOString(),
      created_at: new Date().toISOString()
    });
    
    // ê°ì‚¬ ë¡œê·¸
    await auditLog.log({
      action: 'TIME_ENTRY_CREATED',
      user_id: req.userId,
      project_id,
      duration,
      timestamp: new Date().toISOString()
    });
    
    res.json({
      success: true,
      entry_id: result.id,
      message: `Logged ${duration} hours to ${result.project_name}`,
      data: {
        id: result.id,
        project_id,
        project_name: result.project_name,
        duration,
        description,
        date: result.date
      }
    });
    
  } catch (error) {
    console.error('Error creating time entry:', error);
    
    res.status(500).json({
      success: false,
      error: error.message || 'Internal server error',
      error_code: error.code || 'UNKNOWN_ERROR'
    });
  }
});

// ë„êµ¬ 2: ì‘ì—… ì‹œê°„ ì¡°íšŒ
app.post('/mcp/tools/time_query', async (req, res) => {
  const { start_date, end_date, project_id, group_by } = req.body;
  
  try {
    // ë‚ ì§œ ê²€ì¦
    const start = new Date(start_date || Date.now() - 7 * 24 * 60 * 60 * 1000);
    const end = new Date(end_date || Date.now());
    
    if (start > end) {
      return res.status(400).json({
        success: false,
        error: 'start_date must be before end_date'
      });
    }
    
    // ë°ì´í„° ì¡°íšŒ
    const entries = await internalTimeAPI.queryEntries({
      user_id: req.userId,
      start_date: start.toISOString(),
      end_date: end.toISOString(),
      project_id: project_id || null
    });
    
    // ë°ì´í„° ì§‘ê³„
    let summary = {
      total_hours: 0,
      total_entries: entries.length,
      period: {
        start: start.toISOString().split('T')[0],
        end: end.toISOString().split('T')[0]
      },
      breakdown: {}
    };
    
    if (group_by === 'project') {
      entries.forEach(entry => {
        const key = entry.project_name;
        if (!summary.breakdown[key]) {
          summary.breakdown[key] = {
            hours: 0,
            entries: 0
          };
        }
        summary.breakdown[key].hours += entry.duration;
        summary.breakdown[key].entries += 1;
        summary.total_hours += entry.duration;
      });
    } else if (group_by === 'date') {
      entries.forEach(entry => {
        const key = entry.date.split('T')[0];
        if (!summary.breakdown[key]) {
          summary.breakdown[key] = {
            hours: 0,
            entries: 0
          };
        }
        summary.breakdown[key].hours += entry.duration;
        summary.breakdown[key].entries += 1;
        summary.total_hours += entry.duration;
      });
    } else {
      summary.total_hours = entries.reduce((sum, e) => sum + e.duration, 0);
      summary.entries = entries;
    }
    
    res.json({
      success: true,
      summary,
      raw_entries: group_by ? undefined : entries
    });
    
  } catch (error) {
    console.error('Error querying time entries:', error);
    
    res.status(500).json({
      success: false,
      error: error.message || 'Internal server error'
    });
  }
});

// ë„êµ¬ 3: í”„ë¡œì íŠ¸ ëª©ë¡
app.post('/mcp/tools/project_list', async (req, res) => {
  const { active_only = true, include_archived = false } = req.body;
  
  try {
    const projects = await internalTimeAPI.listProjects({
      user_id: req.userId,
      active: active_only,
      include_archived
    });
    
    res.json({
      success: true,
      count: projects.length,
      projects: projects.map(p => ({
        id: p.id,
        name: p.name,
        client: p.client,
        status: p.status,
        budget_hours: p.budget_hours,
        spent_hours: p.spent_hours,
        budget_remaining: p.budget_hours - p.spent_hours
      }))
    });
    
  } catch (error) {
    console.error('Error listing projects:', error);
    
    res.status(500).json({
      success: false,
      error: error.message || 'Internal server error'
    });
  }
});

// ë„êµ¬ 4: ë¦¬í¬íŠ¸ ìƒì„±
app.post('/mcp/tools/generate_report', async (req, res) => {
  const { 
    start_date, 
    end_date, 
    format = 'markdown',
    include_details = false 
  } = req.body;
  
  try {
    // ë°ì´í„° ìˆ˜ì§‘
    const entries = await internalTimeAPI.queryEntries({
      user_id: req.userId,
      start_date,
      end_date
    });
    
    // ë¦¬í¬íŠ¸ ìƒì„±
    let report = '';
    
    if (format === 'markdown') {
      report = generateMarkdownReport(entries, { start_date, end_date, include_details });
    } else if (format === 'csv') {
      report = generateCSVReport(entries);
    } else if (format === 'json') {
      report = JSON.stringify(entries, null, 2);
    }
    
    res.json({
      success: true,
      format,
      report,
      generated_at: new Date().toISOString()
    });
    
  } catch (error) {
    console.error('Error generating report:', error);
    
    res.status(500).json({
      success: false,
      error: error.message || 'Internal server error'
    });
  }
});

// ë¦¬í¬íŠ¸ ìƒì„± í—¬í¼ í•¨ìˆ˜
function generateMarkdownReport(entries, options) {
  const { start_date, end_date, include_details } = options;
  
  let report = `# Time Tracking Report\n\n`;
  report += `**Period:** ${start_date} to ${end_date}\n\n`;
  
  const totalHours = entries.reduce((sum, e) => sum + e.duration, 0);
  report += `**Total Hours:** ${totalHours.toFixed(2)}\n\n`;
  
  // í”„ë¡œì íŠ¸ë³„ ì§‘ê³„
  const byProject = {};
  entries.forEach(entry => {
    if (!byProject[entry.project_name]) {
      byProject[entry.project_name] = {
        hours: 0,
        entries: []
      };
    }
    byProject[entry.project_name].hours += entry.duration;
    byProject[entry.project_name].entries.push(entry);
  });
  
  report += `## Breakdown by Project\n\n`;
  Object.keys(byProject).forEach(project => {
    const data = byProject[project];
    report += `### ${project}\n`;
    report += `- Hours: ${data.hours.toFixed(2)}\n`;
    report += `- Entries: ${data.entries.length}\n\n`;
    
    if (include_details) {
      report += `#### Details\n`;
      data.entries.forEach(entry => {
        report += `- ${entry.date}: ${entry.duration}h - ${entry.description}\n`;
      });
      report += `\n`;
    }
  });
  
  return report;
}

function generateCSVReport(entries) {
  let csv = 'Date,Project,Duration,Description\n';
  entries.forEach(entry => {
    csv += `${entry.date},${entry.project_name},${entry.duration},"${entry.description}"\n`;
  });
  return csv;
}

// ì—ëŸ¬ í•¸ë“¤ëŸ¬
app.use((err, req, res, next) => {
  console.error('Unhandled error:', err);
  
  res.status(500).json({
    success: false,
    error: 'Internal server error',
    message: process.env.NODE_ENV === 'development' ? err.message : undefined
  });
});

// ì„œë²„ ì‹œì‘
const PORT = process.env.PORT || 3000;
app.listen(PORT, () => {
  console.log(`MCP Time Tracking Server running on port ${PORT}`);
  console.log(`Environment: ${process.env.NODE_ENV || 'development'}`);
});

// ì •ìƒ ì¢…ë£Œ ì²˜ë¦¬
process.on('SIGTERM', () => {
  console.log('SIGTERM signal received: closing HTTP server');
  server.close(() => {
    console.log('HTTP server closed');
  });
});
```

### MCP ì„œë²„ ë“±ë¡

```json
// .claude/mcp-servers.json
{
  "mcpServers": {
    "timetracking": {
      "url": "http://localhost:3000",
      "auth": {
        "type": "bearer",
        "token": "${TIMETRACKING_TOKEN}"
      },
      "tools": [
        {
          "name": "time_entry",
          "description": "Log work hours to a project",
          "parameters": {
            "project_id": "string",
            "duration": "number",
            "description": "string",
            "date": "string (optional)"
          }
        },
        {
          "name": "time_query",
          "description": "Query logged hours for a period",
          "parameters": {
            "start_date": "date",
            "end_date": "date",
            "project_id": "string (optional)",
            "group_by": "enum[project,date] (optional)"
          }
        },
        {
          "name": "project_list",
          "description": "List available projects",
          "parameters": {
            "active_only": "boolean (optional)",
            "include_archived": "boolean (optional)"
          }
        },
        {
          "name": "generate_report",
          "description": "Generate time tracking report",
          "parameters": {
            "start_date": "date",
            "end_date": "date",
            "format": "enum[markdown,csv,json] (optional)",
            "include_details": "boolean (optional)"
          }
        }
      ]
    }
  }
}
```

### MCP ì‚¬ìš© ì˜ˆì œ

```
User: "ì˜¤ëŠ˜ Project Alphaì— 3ì‹œê°„ ì‘ì—…í–ˆì–´. ì¸ì¦ ê¸°ëŠ¥ êµ¬í˜„í–ˆë‹¤ê³  ê¸°ë¡í•´ì¤˜."

Claude: [time_entry ë„êµ¬ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤...]

POST /mcp/tools/time_entry
{
  "project_id": "alpha",
  "duration": 3,
  "description": "ì¸ì¦ ê¸°ëŠ¥ êµ¬í˜„",
  "date": "2026-01-18"
}

"3ì‹œê°„ì´ Project Alphaì— ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤."

User: "ì´ë²ˆ ì£¼ì— ì´ ëª‡ ì‹œê°„ ì¼í–ˆëŠ”ì§€ í™•ì¸í•´ì¤˜."

Claude: [time_query ë„êµ¬ë¡œ ì´ë²ˆ ì£¼ ê¸°ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤...]

POST /mcp/tools/time_query
{
  "start_date": "2026-01-13",
  "end_date": "2026-01-18",
  "group_by": "project"
}

"ì´ë²ˆ ì£¼(1ì›” 13ì¼ - 1ì›” 18ì¼) ì´ ì‘ì—… ì‹œê°„: 28.5ì‹œê°„

í”„ë¡œì íŠ¸ë³„ ë¶„ì„:
- Project Alpha: 15ì‹œê°„ (7ê±´)
- Project Beta: 10ì‹œê°„ (5ê±´)
- Internal Tools: 3.5ì‹œê°„ (3ê±´)"
```

## Skills êµ¬í˜„ ìƒì„¸

### SKILL.md íŒŒì¼ êµ¬ì¡°

SkillsëŠ” frontmatter + markdown êµ¬ì¡°ë¡œ, ê°€ì¥ ë³µì¡í•œ ì»´í¬ë„ŒíŠ¸ì…ë‹ˆë‹¤:

~~~markdown
---
name: pdf-processor
description: Comprehensive PDF processing with text extraction, metadata parsing, and page manipulation
allowed-tools: Bash, Read, Write, FileEdit
version: 2.0.0
author: Your Organization
model: sonnet-4  # ì„ íƒì : íŠ¹ì • ëª¨ë¸ ì§€ì •
---

# PDF Processor Skill

Advanced PDF processing capabilities including text extraction, metadata manipulation, page splitting, and content analysis.

## Purpose

This skill enables Claude to:
- Extract text content from PDF documents
- Parse and modify PDF metadata
- Split and merge PDF files
- Analyze document structure
- Convert PDFs to other formats

## Progressive Disclosure

This skill uses progressive disclosure to load only necessary resources:

1. **Initial Request**: Load core processing logic
2. **Text Extraction**: Load extraction scripts when needed
3. **Metadata Operations**: Load metadata tools on demand
4. **Advanced Features**: Load splitting/merging only if requested

### File Organization

- `scripts/extract_text.py`: Text extraction logic
- `scripts/parse_metadata.py`: Metadata parsing
- `scripts/split_pdf.py`: Page splitting
- `scripts/merge_pdf.py`: PDF merging
- `references/pdf_structure.md`: PDF format reference
- `assets/default_metadata.json`: Metadata templates

## Usage

### Basic Text Extraction

To extract text from a PDF:

```
Use the pdf-processor skill to extract text from document.pdf
```

I will:
1. Run `scripts/extract_text.py document.pdf`
2. Return structured text with page numbers
3. Provide summary statistics

### Metadata Operations

To view or modify PDF metadata:

```
Use pdf-processor to show metadata for report.pdf
```

I will:
1. Run `scripts/parse_metadata.py report.pdf --show`
2. Display title, author, creation date, etc.
3. Offer to modify any field

### Advanced: Page Manipulation

To split a PDF:

```
Split pages 1-10 from large_document.pdf into separate_file.pdf
```

I will:
1. Validate page range
2. Run `scripts/split_pdf.py large_document.pdf 1-10 separate_file.pdf`
3. Confirm successful creation

## Implementation Details

### Text Extraction Process

1. Check if pypdf2 is installed
2. Open PDF file
3. Extract text page by page
4. Clean and format output
5. Return structured data

### Error Handling

Common errors and solutions:

- **Encrypted PDF**: Ask user for password
- **Scanned PDF (image)**: Suggest OCR processing
- **Corrupted file**: Report specific error
- **Missing dependencies**: Provide installation command

### Output Format

Text extraction returns:

```json
{
  "filename": "document.pdf",
  "pages": 15,
  "extracted_text": [
    {
      "page": 1,
      "text": "Page 1 content...",
      "word_count": 245
    },
    ...
  ],
  "metadata": {
    "title": "Document Title",
    "author": "Author Name",
    "created": "2026-01-18"
  },
  "summary": {
    "total_words": 3421,
    "total_characters": 18932,
    "average_words_per_page": 228
  }
}
```

## Examples

### Example 1: Extract and Summarize

**User Request:**
```
Extract text from meeting_notes.pdf and summarize key points
```

**Execution:**
1. Load extraction script
2. Extract all text
3. Analyze content
4. Generate summary

**Output:**
```
Extracted text from meeting_notes.pdf (5 pages, 1,234 words)

Key Points:
1. Q4 revenue target: $2.5M
2. New product launch scheduled for March
3. Team expansion: hiring 3 engineers
4. Customer retention rate: 94%
5. Action items assigned to department leads

Full text available in output.txt
```

### Example 2: Metadata Modification

**User Request:**
```
Update the author of report.pdf to "Engineering Team" and set creation date to today
```

**Execution:**
1. Load metadata script
2. Read current metadata
3. Update specified fields
4. Write back to PDF

**Output:**
```
Updated metadata for report.pdf:

Changes:
- Author: "John Doe" â†’ "Engineering Team"
- Creation Date: "2025-11-15" â†’ "2026-01-18"

Other metadata preserved:
- Title: "Q4 Engineering Report"
- Subject: "Performance Analysis"
- Keywords: "engineering, performance, Q4"

File successfully updated.
```

### Example 3: Complex Operation

**User Request:**
```
Split the first 20 pages of handbook.pdf, extract text from those pages, and create a summary document
```

**Execution:**
1. Split pages 1-20 to temp file
2. Extract text from temp file
3. Generate summary
4. Create new PDF with summary
5. Clean up temp files

**Output:**
```
Completed multi-step PDF processing:

Step 1: âœ… Split pages 1-20 from handbook.pdf
Step 2: âœ… Extracted text (4,532 words)
Step 3: âœ… Generated summary (350 words)
Step 4: âœ… Created summary.pdf

Output files:
- handbook_pages_1-20.pdf (extracted pages)
- summary.pdf (summary document)
- handbook_summary.txt (text version)

Summary highlights:
- Introduction and company overview
- Core values and mission statement
- Organizational structure
- Key policies and procedures
```

## Limitations

- Maximum PDF size: 100 MB
- Encrypted PDFs require password
- Scanned documents need OCR (not included)
- Complex forms may not extract perfectly
- Embedded multimedia not processed

## Dependencies

Required Python packages:
- pypdf2>=3.0.0
- pdfplumber>=0.9.0
- reportlab>=4.0.0 (for PDF creation)

Install via:
```bash
pip install pypdf2 pdfplumber reportlab --break-system-packages
```
~~~

### ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
.claude/skills/pdf-processor/
â”œâ”€â”€ SKILL.md                    # ë©”ì¸ ìŠ¤í‚¬ ì •ì˜
â”œâ”€â”€ LICENSE.txt                 # ë¼ì´ì„¼ìŠ¤
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract_text.py        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
â”‚   â”œâ”€â”€ parse_metadata.py      # ë©”íƒ€ë°ì´í„° íŒŒì‹±
â”‚   â”œâ”€â”€ split_pdf.py           # PDF ë¶„í• 
â”‚   â”œâ”€â”€ merge_pdf.py           # PDF ë³‘í•©
â”‚   â””â”€â”€ convert_to_text.py     # í…ìŠ¤íŠ¸ ë³€í™˜
â”œâ”€â”€ references/
â”‚   â”œâ”€â”€ pdf_structure.md       # PDF êµ¬ì¡° ì„¤ëª…
â”‚   â”œâ”€â”€ metadata_fields.md     # ë©”íƒ€ë°ì´í„° í•„ë“œ ëª©ë¡
â”‚   â””â”€â”€ best_practices.md      # ëª¨ë²” ì‚¬ë¡€
â””â”€â”€ assets/
    â”œâ”€â”€ default_metadata.json  # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° í…œí”Œë¦¿
    â””â”€â”€ output_template.md     # ì¶œë ¥ í…œí”Œë¦¿
```

### ì‹¤ì œ Python ìŠ¤í¬ë¦½íŠ¸ ì˜ˆì œ

```python
# scripts/extract_text.py
#!/usr/bin/env python3
"""
PDF Text Extraction Script
Extracts text content from PDF files with page-level granularity
"""

import sys
import json
from pathlib import Path

try:
    import PyPDF2
    import pdfplumber
except ImportError:
    print("Error: Required packages not installed")
    print("Install with: pip install pypdf2 pdfplumber --break-system-packages")
    sys.exit(1)

def extract_text_pypdf2(pdf_path):
    """Extract text using PyPDF2 (faster but less accurate)"""
    extracted_pages = []
    
    try:
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            
            for page_num, page in enumerate(pdf_reader.pages, start=1):
                text = page.extract_text()
                word_count = len(text.split())
                
                extracted_pages.append({
                    'page': page_num,
                    'text': text,
                    'word_count': word_count
                })
    
    except Exception as e:
        return {'error': str(e), 'method': 'pypdf2'}
    
    return {'pages': extracted_pages, 'method': 'pypdf2'}

def extract_text_pdfplumber(pdf_path):
    """Extract text using pdfplumber (slower but more accurate)"""
    extracted_pages = []
    
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages, start=1):
                text = page.extract_text()
                
                # Extract tables if present
                tables = page.extract_tables()
                
                word_count = len(text.split())
                
                page_data = {
                    'page': page_num,
                    'text': text,
                    'word_count': word_count,
                    'has_tables': len(tables) > 0,
                    'table_count': len(tables)
                }
                
                if tables:
                    page_data['tables'] = tables
                
                extracted_pages.append(page_data)
    
    except Exception as e:
        return {'error': str(e), 'method': 'pdfplumber'}
    
    return {'pages': extracted_pages, 'method': 'pdfplumber'}

def get_metadata(pdf_path):
    """Extract PDF metadata"""
    try:
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            metadata = pdf_reader.metadata
            
            return {
                'title': metadata.get('/Title', 'Unknown'),
                'author': metadata.get('/Author', 'Unknown'),
                'subject': metadata.get('/Subject', ''),
                'creator': metadata.get('/Creator', ''),
                'producer': metadata.get('/Producer', ''),
                'creation_date': metadata.get('/CreationDate', ''),
                'modification_date': metadata.get('/ModDate', '')
            }
    except Exception as e:
        return {'error': str(e)}

def calculate_summary(pages):
    """Calculate summary statistics"""
    total_words = sum(p['word_count'] for p in pages)
    total_chars = sum(len(p['text']) for p in pages)
    avg_words_per_page = total_words / len(pages) if pages else 0
    
    return {
        'total_pages': len(pages),
        'total_words': total_words,
        'total_characters': total_chars,
        'average_words_per_page': round(avg_words_per_page, 2)
    }

def main():
    if len(sys.argv) < 2:
        print("Usage: python extract_text.py <pdf_file> [--method pypdf2|pdfplumber]")
        sys.exit(1)
    
    pdf_path = sys.argv[1]
    method = 'pdfplumber'  # Default to more accurate method
    
    if len(sys.argv) > 2 and sys.argv[2] == '--method':
        method = sys.argv[3] if len(sys.argv) > 3 else 'pdfplumber'
    
    if not Path(pdf_path).exists():
        print(f"Error: File not found: {pdf_path}")
        sys.exit(1)
    
    # Extract text
    if method == 'pypdf2':
        result = extract_text_pypdf2(pdf_path)
    else:
        result = extract_text_pdfplumber(pdf_path)
    
    if 'error' in result:
        print(f"Error extracting text: {result['error']}")
        sys.exit(1)
    
    # Get metadata
    metadata = get_metadata(pdf_path)
    
    # Calculate summary
    summary = calculate_summary(result['pages'])
    
    # Compile final output
    output = {
        'filename': Path(pdf_path).name,
        'extracted_text': result['pages'],
        'metadata': metadata,
        'summary': summary,
        'extraction_method': result['method']
    }
    
    # Output as JSON
    print(json.dumps(output, indent=2, ensure_ascii=False))

if __name__ == '__main__':
    main()
```

### Progressive Disclosure êµ¬í˜„

Progressive DisclosureëŠ” í•„ìš”í•œ ìˆœê°„ì—ë§Œ ê´€ë ¨ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ í† í°ì„ ì ˆì•½í•˜ëŠ” íŒ¨í„´ì…ë‹ˆë‹¤:

~~~markdown
---
name: advanced-pdf-skill
description: Advanced PDF processing with progressive disclosure
allowed-tools: Bash, Read, Write
version: 3.0.0
---

# Advanced PDF Processing Skill

## Progressive Disclosure Strategy

Information is loaded in stages based on user requests:

### Stage 1: Initial Request (Always Loaded)
- Core skill description
- Available operations overview
- Basic usage instructions
**Token Cost: ~500 tokens**

### Stage 2: Text Extraction (Loaded on demand)
**Trigger**: User asks to extract text
**Files Loaded**:
- `scripts/extract_text.py`
- `references/extraction_methods.md`
**Token Cost: ~1,200 tokens**

### Stage 3: Metadata Operations (Loaded on demand)
**Trigger**: User asks about metadata
**Files Loaded**:
- `scripts/parse_metadata.py`
- `references/metadata_fields.md`
**Token Cost: ~800 tokens**

### Stage 4: Advanced Operations (Loaded on demand)
**Trigger**: User asks to split/merge/convert
**Files Loaded**:
- `scripts/split_pdf.py`
- `scripts/merge_pdf.py`
- `scripts/convert_pdf.py`
- `references/advanced_techniques.md`
**Token Cost: ~2,000 tokens**

## Implementation

When user makes a request:

1. **Identify Operation Type**
   ```
   User: "Extract text from document.pdf"
   â†’ Trigger: Text Extraction (Stage 2)
   ```

2. **Load Only Necessary Files**
   ```
   Load: scripts/extract_text.py
   Load: references/extraction_methods.md
   Skip: All metadata and advanced files
   ```

3. **Execute and Return**
   ```
   Run extraction script
   Return formatted results
   ```

4. **Unload After Use**
   ```
   Once task complete, release loaded context
   Ready for next operation
   ```

## Workflow Example

**First Request:**
```
User: "What can you do with PDFs?"
Claude loads: SKILL.md only (~500 tokens)
Response: Overview of capabilities
```

**Second Request:**
```
User: "Extract text from report.pdf"
Claude loads: extract_text.py + extraction_methods.md (~1,700 total tokens)
Executes: Text extraction
Response: Extracted text and summary
```

**Third Request:**
```
User: "Now split the first 10 pages"
Claude loads: split_pdf.py + advanced_techniques.md (~2,000 additional tokens)
Executes: PDF splitting
Response: Confirmation and file location
```

**Total Token Usage: ~4,200 tokens**
**Traditional Approach (load everything): ~15,000 tokens**
**Savings: 72%**
~~~

## ì„¤ì¹˜ ë©”ì»¤ë‹ˆì¦˜ ìƒì„¸

### CLI ë„êµ¬ ì‘ë™ ë°©ì‹

```javascript
// cli-tool/src/index.js (ê°„ì†Œí™”ëœ ë²„ì „)

const axios = require('axios');
const fs = require('fs-extra');
const path = require('path');

const GITHUB_RAW_BASE = 'https://raw.githubusercontent.com/davila7/claude-code-templates/main';
const API_BASE = 'https://aitmpl.com/api';

async function installComponent(type, name, options = {}) {
  console.log(`Installing ${type}: ${name}`);
  
  try {
    // 1. ì»´í¬ë„ŒíŠ¸ ì •ë³´ ì¡°íšŒ
    const componentInfo = await getComponentInfo(type, name);
    
    if (!componentInfo) {
      throw new Error(`Component not found: ${type}/${name}`);
    }
    
    // 2. GitHubì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    const url = `${GITHUB_RAW_BASE}/cli-tool/components/${type}/${componentInfo.path}`;
    const response = await axios.get(url);
    
    // 3. ë¡œì»¬ ê²½ë¡œ ê²°ì • (ì¹´í…Œê³ ë¦¬ ì œê±°)
    const filename = path.basename(componentInfo.path);
    const targetDir = `.claude/${type}`;
    const targetPath = path.join(targetDir, filename);
    
    // 4. ë””ë ‰í† ë¦¬ ìƒì„±
    await fs.ensureDir(targetDir);
    
    // 5. íŒŒì¼ ì €ì¥
    await fs.writeFile(targetPath, response.data);
    
    // 6. ì¶”ê°€ íŒŒì¼ ì²˜ë¦¬ (Skillsì˜ ê²½ìš°)
    if (type === 'skills') {
      await installSkillAssets(name, componentInfo);
    }
    
    // 7. ë‹¤ìš´ë¡œë“œ í†µê³„ ì „ì†¡
    if (!options.dryRun) {
      await trackDownload(type, name, componentInfo.path);
    }
    
    console.log(`âœ… Installed: ${targetPath}`);
    return { success: true, path: targetPath };
    
  } catch (error) {
    console.error(`âŒ Failed to install ${type}/${name}:`, error.message);
    return { success: false, error: error.message };
  }
}

async function getComponentInfo(type, name) {
  // components.jsonì—ì„œ ì»´í¬ë„ŒíŠ¸ ì •ë³´ ì¡°íšŒ
  const catalogUrl = `${GITHUB_RAW_BASE}/docs/components.json`;
  const response = await axios.get(catalogUrl);
  const catalog = response.data;
  
  const components = catalog[type + 's'] || []; // 'agent' â†’ 'agents'
  return components.find(c => c.name === name);
}

async function installSkillAssets(skillName, componentInfo) {
  // Skillì˜ ê²½ìš° scripts/, references/, assets/ ë””ë ‰í† ë¦¬ë„ ë‹¤ìš´ë¡œë“œ
  const skillDir = `.claude/skills/${skillName}`;
  await fs.ensureDir(skillDir);
  
  const subdirs = ['scripts', 'references', 'assets'];
  
  for (const subdir of subdirs) {
    const url = `${GITHUB_RAW_BASE}/cli-tool/components/skills/${componentInfo.category}/${skillName}/${subdir}`;
    
    try {
      // ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
      const files = await listGitHubDirectory(url);
      
      for (const file of files) {
        const fileUrl = `${url}/${file}`;
        const targetPath = path.join(skillDir, subdir, file);
        
        const response = await axios.get(fileUrl);
        await fs.ensureFile(targetPath);
        await fs.writeFile(targetPath, response.data);
        
        // Python ìŠ¤í¬ë¦½íŠ¸ëŠ” ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
        if (file.endsWith('.py') || file.endsWith('.sh')) {
          await fs.chmod(targetPath, 0o755);
        }
      }
    } catch (error) {
      // ì„ íƒì  ë””ë ‰í† ë¦¬ì´ë¯€ë¡œ ì—ëŸ¬ ë¬´ì‹œ
      console.log(`  â„¹ï¸  No ${subdir}/ directory for ${skillName}`);
    }
  }
}

async function trackDownload(type, name, path) {
  // Supabaseë¡œ ë‹¤ìš´ë¡œë“œ í†µê³„ ì „ì†¡
  try {
    await axios.post(`${API_BASE}/track-download-supabase`, {
      type,
      name,
      path,
      category: path.split('/')[0],
      timestamp: new Date().toISOString(),
      cli_version: require('../package.json').version
    });
  } catch (error) {
    // í†µê³„ ì „ì†¡ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ
    console.log('  â„¹ï¸  Could not track download (offline mode)');
  }
}

// ë°°ì¹˜ ì„¤ì¹˜
async function batchInstall(components) {
  const results = [];
  
  for (const component of components) {
    const { type, name } = component;
    const result = await installComponent(type, name);
    results.push({ type, name, ...result });
  }
  
  return results;
}

module.exports = {
  installComponent,
  batchInstall
};
```

### ì»´í¬ë„ŒíŠ¸ ì¹´íƒˆë¡œê·¸ ìƒì„±

```python
# generate_components_json.py (ê°„ì†Œí™”ëœ ë²„ì „)

import os
import json
import hashlib
from pathlib import Path

def generate_catalog():
    catalog = {
        'agents': [],
        'commands': [],
        'mcps': [],
        'settings': [],
        'hooks': [],
        'skills': []
    }
    
    components_dir = Path('cli-tool/components')
    
    # ê° ì»´í¬ë„ŒíŠ¸ íƒ€ì…ë³„ë¡œ ìŠ¤ìº”
    for component_type in catalog.keys():
        type_dir = components_dir / component_type
        
        if not type_dir.exists():
            continue
        
        # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìˆœíšŒ
        for category_dir in type_dir.iterdir():
            if not category_dir.is_dir():
                continue
            
            category_name = category_dir.name
            
            # ê° ì»´í¬ë„ŒíŠ¸ íŒŒì¼ ì²˜ë¦¬
            for component_file in category_dir.glob('*.md'):
                component_info = parse_component_file(
                    component_file,
                    component_type,
                    category_name
                )
                
                if component_info:
                    catalog[component_type].append(component_info)
    
    # ë³´ì•ˆ ê²€ì¦ ì¶”ê°€
    catalog = add_security_scores(catalog)
    
    # ë‹¤ìš´ë¡œë“œ í†µê³„ ì¶”ê°€
    catalog = add_download_stats(catalog)
    
    # ì •ë ¬ (ì¸ê¸°ë„ìˆœ)
    for component_type in catalog.keys():
        catalog[component_type].sort(
            key=lambda x: x.get('downloads', 0),
            reverse=True
        )
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    output_path = Path('docs/components.json')
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(catalog, f, indent=2, ensure_ascii=False)
    
    print(f'âœ… Generated catalog: {output_path}')
    print(f'   Total components: {sum(len(v) for v in catalog.values())}')

def parse_component_file(file_path, component_type, category):
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Frontmatter íŒŒì‹±
    frontmatter = {}
    if content.startswith('---'):
        parts = content.split('---', 2)
        if len(parts) >= 3:
            frontmatter_text = parts[1]
            for line in frontmatter_text.strip().split('\n'):
                if ':' in line:
                    key, value = line.split(':', 1)
                    frontmatter[key.strip()] = value.strip()
    
    # ì²« ë²ˆì§¸ í—¤ë”© ì¶”ì¶œ (ì œëª©)
    title = file_path.stem.replace('-', ' ').title()
    for line in content.split('\n'):
        if line.startswith('# '):
            title = line[2:].strip()
            break
    
    # ì„¤ëª… ì¶”ì¶œ (ì²« ë²ˆì§¸ ë‹¨ë½)
    description = ''
    in_content = False
    for line in content.split('\n'):
        if line.startswith('---'):
            in_content = not in_content
            continue
        if in_content and line.strip() and not line.startswith('#'):
            description = line.strip()
            break
    
    # ìƒëŒ€ ê²½ë¡œ
    relative_path = f'{category}/{file_path.name}'
    
    # íŒŒì¼ í•´ì‹œ (ë¬´ê²°ì„± ê²€ì¦ìš©)
    file_hash = hashlib.sha256(content.encode()).hexdigest()[:16]
    
    return {
        'name': file_path.stem,
        'title': title,
        'description': description or frontmatter.get('description', ''),
        'category': category.replace('-', ' ').title(),
        'path': relative_path,
        'version': frontmatter.get('version', '1.0.0'),
        'author': frontmatter.get('author', 'Community'),
        'hash': file_hash,
        'file_size': file_path.stat().st_size,
        'last_modified': file_path.stat().st_mtime
    }

def add_security_scores(catalog):
    # ë³´ì•ˆ ê²€ì¦ ê²°ê³¼ ì¶”ê°€
    security_report_path = Path('cli-tool/security-report.json')
    
    if not security_report_path.exists():
        return catalog
    
    with open(security_report_path, 'r') as f:
        security_data = json.load(f)
    
    for component_type in catalog.keys():
        for component in catalog[component_type]:
            component_key = f"{component_type}/{component['path']}"
            
            if component_key in security_data:
                component['security_score'] = security_data[component_key]['score']
                component['security_issues'] = security_data[component_key]['issues']
            else:
                component['security_score'] = 100
                component['security_issues'] = []
    
    return catalog

def add_download_stats(catalog):
    # Supabaseì—ì„œ ë‹¤ìš´ë¡œë“œ í†µê³„ ê°€ì ¸ì˜¤ê¸° (ì‹¤ì œë¡œëŠ” API í˜¸ì¶œ)
    # ì—¬ê¸°ì„œëŠ” ëª¨ì˜ ë°ì´í„°
    
    for component_type in catalog.keys():
        for component in catalog[component_type]:
            # ì‹¤ì œë¡œëŠ” DB ì¡°íšŒ
            component['downloads'] = 0
            component['last_downloaded'] = None
    
    return catalog

if __name__ == '__main__':
    generate_catalog()
```

## Global Agents êµ¬í˜„

### Global Agent ìƒì„± ë©”ì»¤ë‹ˆì¦˜

```javascript
// cli-tool/src/global-agents.js

const fs = require('fs-extra');
const path = require('path');
const os = require('os');

async function createGlobalAgent(agentName) {
  console.log(`Creating global agent: ${agentName}`);
  
  try {
    // 1. Agent íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    const agentFile = await downloadAgent(agentName);
    
    // 2. ì‹¤í–‰ ê°€ëŠ¥í•œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
    const binDir = getGlobalBinDir();
    const scriptPath = path.join(binDir, agentName);
    
    const script = generateAgentScript(agentName, agentFile);
    
    await fs.ensureDir(binDir);
    await fs.writeFile(scriptPath, script);
    await fs.chmod(scriptPath, 0o755); // ì‹¤í–‰ ê¶Œí•œ
    
    // 3. PATHì— ì¶”ê°€ (í•„ìš”í•œ ê²½ìš°)
    await ensureInPath(binDir);
    
    console.log(`âœ… Global agent created: ${agentName}`);
    console.log(`   Usage: ${agentName} "your prompt here"`);
    
    return { success: true, path: scriptPath };
    
  } catch (error) {
    console.error(`âŒ Failed to create global agent:`, error.message);
    return { success: false, error: error.message };
  }
}

function getGlobalBinDir() {
  // OSë³„ ê¸€ë¡œë²Œ bin ë””ë ‰í† ë¦¬
  if (os.platform() === 'win32') {
    return path.join(os.homedir(), 'AppData', 'Local', 'claude-agents');
  } else {
    return path.join(os.homedir(), '.local', 'bin');
  }
}

function generateAgentScript(agentName, agentFilePath) {
  // Unix ì…¸ ìŠ¤í¬ë¦½íŠ¸ (WindowsëŠ” .cmd íŒŒì¼)
  if (os.platform() === 'win32') {
    return generateWindowsScript(agentName, agentFilePath);
  }
  
  return `#!/usr/bin/env node

/**
 * Global Agent: ${agentName}
 * Auto-generated by Claude Code Templates
 */

const { spawn } = require('child_process');
const path = require('path');
const fs = require('fs');

const AGENT_FILE = '${agentFilePath}';
const AGENT_NAME = '${agentName}';

async function main() {
  // ì‚¬ìš©ì ì…ë ¥ ìˆ˜ì§‘
  const userPrompt = process.argv.slice(2).join(' ');
  
  if (!userPrompt) {
    console.error('Usage: ${agentName} "your prompt here"');
    console.error('Example: ${agentName} "Review this code for security issues"');
    process.exit(1);
  }
  
  // Agent íŒŒì¼ ì¡´ì¬ í™•ì¸
  if (!fs.existsSync(AGENT_FILE)) {
    console.error(\`Error: Agent file not found: \${AGENT_FILE}\`);
    console.error('Run: npx claude-code-templates@latest --update-agent ${agentName}');
    process.exit(1);
  }
  
  // Claude Code ì‹¤í–‰
  const claude = spawn('claude', [
    '--agent', AGENT_NAME,
    '--prompt', userPrompt
  ], {
    stdio: 'inherit',
    env: {
      ...process.env,
      CLAUDE_AGENT_FILE: AGENT_FILE
    }
  });
  
  claude.on('error', (error) => {
    console.error('Error executing Claude Code:', error.message);
    console.error('Make sure Claude Code is installed: https://claude.ai/code');
    process.exit(1);
  });
  
  claude.on('close', (code) => {
    process.exit(code);
  });
}

main().catch((error) => {
  console.error('Unexpected error:', error);
  process.exit(1);
});
`;
}

function generateWindowsScript(agentName, agentFilePath) {
  return `@echo off
REM Global Agent: ${agentName}
REM Auto-generated by Claude Code Templates

node "${agentFilePath}" %*
`;
}

async function ensureInPath(binDir) {
  const currentPath = process.env.PATH || '';
  
  if (currentPath.includes(binDir)) {
    return; // ì´ë¯¸ PATHì— ìˆìŒ
  }
  
  console.log(`\nâš ï¸  Please add ${binDir} to your PATH:`);
  
  if (os.platform() === 'win32') {
    console.log(`   setx PATH "%PATH%;${binDir}"`);
  } else {
    const shell = process.env.SHELL || '/bin/bash';
    const rcFile = shell.includes('zsh') ? '~/.zshrc' : '~/.bashrc';
    
    console.log(`   echo 'export PATH="$PATH:${binDir}"' >> ${rcFile}`);
    console.log(`   source ${rcFile}`);
  }
  
  console.log('');
}

module.exports = {
  createGlobalAgent,
  listGlobalAgents,
  updateGlobalAgent,
  removeGlobalAgent
};
```

### ì‚¬ìš© ì˜ˆì œ

```bash
# Global agent ìƒì„±
npx claude-code-templates@latest --create-agent code-reviewer

# ì–´ë””ì„œë“  ì‚¬ìš©
cd ~/projects/my-app
code-reviewer "Review this authentication function"

# ì—¬ëŸ¬ agent ìƒì„±
npx claude-code-templates@latest --create-agent security-auditor
npx claude-code-templates@latest --create-agent performance-optimizer

# ì‚¬ìš©
security-auditor "Scan this codebase for vulnerabilities"
performance-optimizer "Analyze bundle size and suggest improvements"

# ëª©ë¡ ë³´ê¸°
npx claude-code-templates@latest --list-agents

# ì—…ë°ì´íŠ¸
npx claude-code-templates@latest --update-agent code-reviewer

# ì œê±°
npx claude-code-templates@latest --remove-agent code-reviewer
```

## ê²°ë¡ 

ì´ ë¬¸ì„œëŠ” Claude Code Templatesì˜ ì‹¤ì œ êµ¬í˜„ ë°©ì‹ì„ ê¸°ìˆ ì ìœ¼ë¡œ ìƒì„¸íˆ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤. ê° ì»´í¬ë„ŒíŠ¸ íƒ€ì…(Agent, Command, MCP, Skills)ì˜ íŒŒì¼ êµ¬ì¡°, ì‘ì„± ë°©ë²•, ê·¸ë¦¬ê³  ë‚´ë¶€ ë©”ì»¤ë‹ˆì¦˜ì„ ì‹¤ì œ ì½”ë“œ ì˜ˆì œì™€ í•¨ê»˜ ì„¤ëª…í–ˆìŠµë‹ˆë‹¤.

í•µì‹¬ í¬ì¸íŠ¸:
- **Agents**: Markdown íŒŒì¼, ì—­í• ê³¼ ì§€ì¹¨ ì •ì˜
- **Commands**: Frontmatter í•„ìˆ˜, ìŠ¬ë˜ì‹œ ëª…ë ¹ì–´
- **MCPs**: JSON ì„¤ì •, Node.js ì„œë²„ êµ¬í˜„
- **Skills**: ê°€ì¥ ë³µì¡, Progressive Disclosure íŒ¨í„´
- **Global Agents**: ì „ì—­ ì‹¤í–‰ ê°€ëŠ¥, CLI ë˜í¼
- **ì„¤ì¹˜ ë©”ì»¤ë‹ˆì¦˜**: GitHub â†’ ë¡œì»¬, ì¹´í…Œê³ ë¦¬ í‰íƒ„í™”
- **ì¹´íƒˆë¡œê·¸ ìƒì„±**: Python ìŠ¤í¬ë¦½íŠ¸, ìë™í™”

ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì»¤ìŠ¤í…€ ì»´í¬ë„ŒíŠ¸ë¥¼ ë§Œë“¤ê±°ë‚˜, ê¸°ì¡´ ì»´í¬ë„ŒíŠ¸ë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜, ì „ì²´ ì‹œìŠ¤í…œì˜ ì‘ë™ ë°©ì‹ì„ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ì‘ì„± ì¼ì: 2026-01-18**
