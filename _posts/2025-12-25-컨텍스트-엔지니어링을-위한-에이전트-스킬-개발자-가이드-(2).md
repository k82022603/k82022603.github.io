---
title: "Anthropicì˜ Multi-Agent Researcher ì˜ˆì‹œ"
date: 2025-12-25 09:00:00 +0900
categories: [AI,  Context Engineering]
mermaid: [True]
tags: [AI,  context-engineering,  agent-skills,  anthropic-skills,  AIOptimization,  prompt-engineering,  Claude.write]
---

class LeadResearcher:
    def plan_research(self, topic):
        # ê³„íšì„ ë©”ëª¨ë¦¬ì— ì €ì¥ (ì»¨í…ìŠ¤íŠ¸ ì ˆì•½)
        plan = self.create_research_plan(topic)
        self.memory.write_note('research_plan', plan)
        
        # ì»¨í…ìŠ¤íŠ¸ì—ëŠ” ìš”ì•½ë§Œ í¬í•¨
        return f"ì—°êµ¬ ê³„íš ìˆ˜ë¦½ ì™„ë£Œ (ì„¸ë¶€ì‚¬í•­ì€ ë©”ëª¨ë¦¬ì— ì €ì¥)"
```

### 2. Select (ì„ íƒ) - ê´€ë ¨ ì •ë³´ë§Œ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨

**ê°œë…**: ì „ì²´ê°€ ì•„ë‹Œ ì‘ì—… ê´€ë ¨ ì •ë³´ë§Œ ì„ íƒì ìœ¼ë¡œ ë¡œë“œ

```python
class SelectiveContextLoader:
    """ì„ íƒì  ì»¨í…ìŠ¤íŠ¸ ë¡œë”©"""
    
    def __init__(self):
        self.full_state = {}
        self.vector_db = ChromaDB()
    
    def add_to_state(self, key, value, metadata=None):
        """ìƒíƒœì— ì¶”ê°€"""
        self.full_state[key] = value
        
        # ë²¡í„° DBì—ë„ ì €ì¥ (ê²€ìƒ‰ìš©)
        self.vector_db.add(
            text=str(value),
            metadata={'key': key, **(metadata or {})}
        )
    
    def get_relevant_context(self, query, max_items=3):
        """ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ìƒíƒœë§Œ ë¡œë“œ"""
        # ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ í•­ëª© ì°¾ê¸°
        results = self.vector_db.search(query, top_k=max_items)
        
        # ì „ì²´ ìƒíƒœì—ì„œ í•´ë‹¹ í•­ëª©ë“¤ë§Œ ê°€ì ¸ì˜¤ê¸°
        context = {}
        for result in results:
            key = result.metadata['key']
            if key in self.full_state:
                context[key] = self.full_state[key]
        
        return context

# ì‚¬ìš© ì˜ˆ
loader = SelectiveContextLoader()

# ë§ì€ ì •ë³´ ì €ì¥
for i in range(100):
    loader.add_to_state(
        f'doc_{i}',
        f'Document {i} content...',
        metadata={'category': 'research'}
    )

# ì¿¼ë¦¬ì— ê´€ë ¨ëœ ê²ƒë§Œ ë¡œë“œ
relevant = loader.get_relevant_context(
    "Python programming tips",
    max_items=3
)
# â†’ 100ê°œ ì¤‘ 3ê°œë§Œ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨!
```

### 3. Compress (ì••ì¶•) - í•„ìš”í•œ í† í°ë§Œ ìœ ì§€

**ê°œë…**: ì»¨í…ìŠ¤íŠ¸ë¥¼ ì••ì¶•í•˜ì—¬ í† í° ìˆ˜ ê°ì†Œ

#### 3-1. ëŒ€í™” ìš”ì•½

```python
class ConversationCompressor:
    """ëŒ€í™” ì´ë ¥ ì••ì¶•"""
    
    def compress_long_conversation(self, messages):
        """ê¸´ ëŒ€í™”ë¥¼ ìš”ì•½ìœ¼ë¡œ ì••ì¶•"""
        
        if len(messages) < 10:
            return messages  # ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ
        
        # ìµœê·¼ 3ê°œëŠ” ìœ ì§€
        recent = messages[-3:]
        
        # ë‚˜ë¨¸ì§€ëŠ” ìš”ì•½
        old_messages = messages[:-3]
        summary = self.summarize_messages(old_messages)
        
        return [
            {"role": "system", "content": f"ì´ì „ ëŒ€í™” ìš”ì•½: {summary}"},
            *recent
        ]
    
    def summarize_messages(self, messages):
        """ë©”ì‹œì§€ ë°°ì¹˜ë¥¼ ìš”ì•½"""
        combined = "\n".join([
            f"{m['role']}: {m['content']}" 
            for m in messages
        ])
        
        summary_prompt = f"""
        ë‹¤ìŒ ëŒ€í™”ë¥¼ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½:
        
        {combined}
        
        í•µì‹¬ ê²°ì •ì‚¬í•­ê³¼ ì¤‘ìš” ì •ë³´ë§Œ í¬í•¨í•˜ì„¸ìš”.
        """
        
        return call_llm(summary_prompt)

# ì‹¤ì „ íš¨ê³¼ (LangChain í…ŒìŠ¤íŠ¸):
# ì „: 115,000 í† í°
# í›„:  60,000 í† í° (48% ê°ì†Œ)
```

#### 3-2. ë„êµ¬ ì¶œë ¥ ì••ì¶•

```python
class ToolOutputCompressor:
    """ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ ì••ì¶•"""
    
    def compress_large_output(self, tool_output, max_tokens=1000):
        """í° ë„êµ¬ ì¶œë ¥ì„ ì••ì¶•"""
        
        current_tokens = count_tokens(tool_output)
        
        if current_tokens <= max_tokens:
            return tool_output
        
        # JSONì¸ ê²½ìš°: ì¤‘ìš” í•„ë“œë§Œ ì¶”ì¶œ
        if is_json(tool_output):
            return self.extract_important_fields(tool_output)
        
        # í…ìŠ¤íŠ¸ì¸ ê²½ìš°: ìš”ì•½
        return self.summarize_text(tool_output, max_tokens)
    
    def extract_important_fields(self, json_output):
        """JSONì—ì„œ ì¤‘ìš” í•„ë“œë§Œ"""
        data = json.loads(json_output)
        
        # ì˜ˆ: API ì‘ë‹µì—ì„œ 'results' í•„ë“œë§Œ
        if 'results' in data:
            return json.dumps({'results': data['results'][:5]})  # ìƒìœ„ 5ê°œë§Œ
        
        return json_output
```

### 4. Isolate (ê²©ë¦¬) - ì»¨í…ìŠ¤íŠ¸ ë¶„ë¦¬

**ê°œë…**: ê´€ë ¨ ì—†ëŠ” ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¦¬í•˜ì—¬ í˜¼ë€ ë°©ì§€

```python
class ContextIsolation:
    """ì»¨í…ìŠ¤íŠ¸ ê²©ë¦¬ ì „ëµ"""
    
    def create_isolated_sessions(self, tasks):
        """ê° ì‘ì—…ë§ˆë‹¤ ë…ë¦½ì  ì„¸ì…˜"""
        results = []
        
        for task in tasks:
            # ìƒˆë¡œìš´ ë…ë¦½ ì„¸ì…˜
            session = self.create_new_session()
            
            # ì´ ì‘ì—…ì—ë§Œ í•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸
            relevant_context = self.get_task_context(task)
            session.set_context(relevant_context)
            
            # ì‹¤í–‰
            result = session.execute(task)
            results.append(result)
            
            # ì„¸ì…˜ ì¢…ë£Œ (ì»¨í…ìŠ¤íŠ¸ ì •ë¦¬)
            session.close()
        
        return results
    
    def parallel_isolated_execution(self, subtasks):
        """ë³‘ë ¬ ê²©ë¦¬ ì‹¤í–‰"""
        from concurrent.futures import ThreadPoolExecutor
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = [
                executor.submit(self.execute_isolated, task)
                for task in subtasks
            ]
            
            return [f.result() for f in futures]
    
    def execute_isolated(self, task):
        """ì™„ì „íˆ ê²©ë¦¬ëœ í™˜ê²½ì—ì„œ ì‹¤í–‰"""
        # 1. ë…ë¦½ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = {
            'system_prompt': self.get_task_specific_prompt(task),
            'tools': self.get_task_specific_tools(task),
            'documents': self.get_task_specific_docs(task)
        }
        
        # 2. ê²©ë¦¬ ì‹¤í–‰
        return self.execute_with_context(task, context)
```

**ê²©ë¦¬ì˜ ì¥ì :**

```
ë¬¸ì œ: ì‘ì—… Aì˜ ì»¨í…ìŠ¤íŠ¸ê°€ ì‘ì—… Bì— ì˜í–¥
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Task A: "ë¶„ì„" â†’ Context: í†µê³„ ë¬¸ì„œ  â”‚
â”‚ Task B: "ê¸€ì“°ê¸°" â†’ Context: ì—¬ì „íˆ í†µê³„â”‚
â”‚ â†’ ê²°ê³¼: ê¸€ì´ ë„ˆë¬´ ê¸°ìˆ ì ìœ¼ë¡œ ì‘ì„±ë¨   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

í•´ê²°: ê²©ë¦¬ëœ ì‹¤í–‰
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Session A    â”‚    â”‚ Session B    â”‚
â”‚ í†µê³„ ë¬¸ì„œ     â”‚    â”‚ ê¸€ì“°ê¸° ê°€ì´ë“œâ”‚
â”‚ ë¶„ì„ ë„êµ¬     â”‚    â”‚ ìŠ¤íƒ€ì¼ ê°€ì´ë“œâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†’ ê° ì‘ì—…ì´ ë…ë¦½ì ìœ¼ë¡œ ìµœì í™”ë¨
```

---

## Context Rot ì™„ì „ ì •ë³µ

### ğŸ”¬ ìµœì‹  ì—°êµ¬ (Chroma Research, 2025)

#### ì‹¤í—˜ ì„¤ì •

**í‰ê°€ ëŒ€ìƒ:** 18ê°œ SOTA ëª¨ë¸
- GPT-4.1
- Claude 4  
- Gemini 2.5
- Qwen 3
- ê¸°íƒ€ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸

**í…ŒìŠ¤íŠ¸ ê³¼ì œ:**
1. Needle-in-Haystack (ì •ë³´ ê²€ìƒ‰)
2. ë°˜ë³µ ë‹¨ì–´ ìƒì„±
3. ëŒ€í™”í˜• Q&A
4. ë‹¤ë‹¨ê³„ ì¶”ë¡ 

#### í•µì‹¬ ë°œê²¬ì‚¬í•­

**1. ë¹„ê· ì¼í•œ ì„±ëŠ¥ ì €í•˜**

```
ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ì •í™•ë„:

ë‹¨ìˆœ ì‘ì—… (ë°˜ë³µ ë‹¨ì–´):
1K í† í°:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 95% ì •í™•ë„
10K í† í°:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     80% ì •í™•ë„
50K í† í°:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               30% ì •í™•ë„
100K í† í°: â–ˆâ–ˆ                   10% ì •í™•ë„

ë³µì¡í•œ ì‘ì—… (ë‹¤ë‹¨ê³„ ì¶”ë¡ ):
1K í† í°:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     80% ì •í™•ë„
10K í† í°:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           50% ì •í™•ë„
50K í† í°:  â–ˆâ–ˆâ–ˆâ–ˆ                 20% ì •í™•ë„
100K í† í°: â–ˆ                     5% ì •í™•ë„

â†’ ë³µì¡í•œ ì‘ì—…ì¼ìˆ˜ë¡ ì„±ëŠ¥ ì €í•˜ ë” ì‹¬ê°
```

**2. ìœ„ì¹˜ í¸í–¥ (Positional Bias)**

```
ì •ë³´ ìœ„ì¹˜ì— ë”°ë¥¸ ì •í™•ë„:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Document (4K tokens)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [Start] Needle here â†’ 75% ì •í™•ë„   â”‚
â”‚                                    â”‚
â”‚ [Middle] Needle here â†’ 55% ì •í™•ë„  â”‚
â”‚                                    â”‚
â”‚ [End] Needle here â†’ 72% ì •í™•ë„     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

U-Shaped Attention Curve í™•ì¸
```

**3. ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„ì˜ ì˜í–¥**

```
Needleê³¼ Haystackì˜ ìœ ì‚¬ë„:

ë‚®ì€ ìœ ì‚¬ë„ (ì™„ì „íˆ ë‹¤ë¥¸ ì£¼ì œ):
â””â”€ ì •í™•ë„: 70%

ì¤‘ê°„ ìœ ì‚¬ë„ (ê´€ë ¨ ì£¼ì œ):  
â””â”€ ì •í™•ë„: 55% âš ï¸ (ê°€ì¥ ì–´ë ¤ì›€)

ë†’ì€ ìœ ì‚¬ë„ (ê°™ì€ ì£¼ì œ):
â””â”€ ì •í™•ë„: 65%

â†’ ë¹„ìŠ·í•˜ì§€ë§Œ ë‹¤ë¥¸ ì •ë³´ê°€ ê°€ì¥ í˜¼ë€ìŠ¤ëŸ¬ì›€
```

**4. Distractorì˜ ì˜í–¥**

```
ë°©í•´ ìš”ì†Œ(Distractor) ê°œìˆ˜:

0ê°œ: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 95%
1ê°œ: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     80%
2ê°œ: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         60%
4ê°œ: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               30%

â†’ Distractorê°€ ë§ì„ìˆ˜ë¡ ê¸‰ê²©í•œ ì„±ëŠ¥ ì €í•˜
```

**5. ë¬¸ì„œ êµ¬ì¡°ì˜ ì˜í–¥**

```
ë†€ë¼ìš´ ë°œê²¬:

êµ¬ì¡°í™”ëœ ì—ì„¸ì´ (coherent):
â””â”€ ì •í™•ë„: 55%

ë¬´ì‘ìœ„ ë¬¸ì¥ (shuffled):
â””â”€ ì •í™•ë„: 70%

â†’ êµ¬ì¡°í™”ëœ ë¬¸ì„œê°€ ì˜¤íˆë ¤ ë” ì–´ë ¤ì›€!
   (ëª¨ë¸ì´ ë‚´ëŸ¬í‹°ë¸Œë¥¼ ë”°ë¼ê°€ë‹¤ ì •ë³´ ë†“ì¹¨)
```

**6. ëª¨ë¸ë³„ ì°¨ì´**

```
ê±°ë¶€/í™˜ê° íŒ¨í„´:

GPT ì‹œë¦¬ì¦ˆ:
â””â”€ í™•ì‹  ìˆê²Œ ì˜ëª»ëœ ë‹µë³€ (í™˜ê°)

Claude ì‹œë¦¬ì¦ˆ:
â””â”€ ë¶ˆí™•ì‹¤í•  ë•Œ ë‹µë³€ ê±°ë¶€

â†’ ì•ˆì „ì„± vs ìœ ìš©ì„± íŠ¸ë ˆì´ë“œì˜¤í”„
```

### ì‹¤ì „ í•´ê²° ë°©ì•ˆ

#### 1. ì ì‘í˜• ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°

```python
class AdaptiveContextWindow:
    """ì‘ì—… ë³µì¡ë„ì— ë”°ë¥¸ ë™ì  ìœˆë„ìš° ì¡°ì ˆ"""
    
    def __init__(self):
        self.base_limits = {
            'simple': 4000,      # ê°„ë‹¨í•œ ê²€ìƒ‰
            'moderate': 16000,   # ì¼ë°˜ì  ì‘ì—…
            'complex': 64000,    # ë³µì¡í•œ ë¶„ì„
            'max': 128000        # ìµœëŒ€ ìš©ëŸ‰
        }
    
    def determine_optimal_window(self, task):
        """ìµœì  ìœˆë„ìš° í¬ê¸° ê²°ì •"""
        
        # ì‘ì—… ë³µì¡ë„ ë¶„ì„
        complexity = self.analyze_task_complexity(task)
        
        # ê¸°ë³¸ í¬ê¸° ì„ íƒ
        base_size = self.base_limits[complexity]
        
        # ì‹¤ì‹œê°„ ì¡°ì •
        # Context Rot ì§•í›„ê°€ ë³´ì´ë©´ ì¶•ì†Œ
        if self.detect_context_rot(task):
            return base_size * 0.7  # 30% ì¶•ì†Œ
        
        return base_size
    
    def detect_context_rot(self, task):
        """Context Rot ì§•í›„ ê°ì§€"""
        indicators = {
            'hallucination': self.check_hallucination(),
            'refusal': self.check_refusal_rate(),
            'latency': self.check_response_time(),
            'quality': self.check_output_quality()
        }
        
        # 2ê°œ ì´ìƒ ì§•í›„ â†’ Context Rot
        return sum(indicators.values()) >= 2
```

#### 2. ì „ëµì  ì •ë³´ ë°°ì¹˜

```python
class StrategicPlacement:
    """ì •ë³´ë¥¼ ì „ëµì  ìœ„ì¹˜ì— ë°°ì¹˜"""
    
    def optimize_placement(self, critical_info, supporting_docs):
        """
        U-Shaped Attentionì„ ê³ ë ¤í•œ ë°°ì¹˜
        """
        
        # í•µì‹¬ ì •ë³´: ì‹œì‘ê³¼ ë
        context = [
            "=== CRITICAL INFORMATION ===",
            critical_info,
            "=== CRITICAL INFORMATION END ===",
            "",
        ]
        
        # ë³´ì¡° ë¬¸ì„œ: ì¤‘ê°„
        context.extend(supporting_docs)
        
        # í•µì‹¬ ì •ë³´ ìš”ì•½: ë‹¤ì‹œ ëì—
        context.extend([
            "",
            "=== KEY POINTS RECAP ===",
            self.summarize(critical_info),
            "=== END ==="
        ])
        
        return "\n".join(context)
```

#### 3. Distractor í•„í„°ë§

```python
class DistractorFilter:
    """ë°©í•´ ìš”ì†Œ ì œê±°"""
    
    def remove_distractors(self, query, documents):
        """
        ì¿¼ë¦¬ì™€ ë¹„ìŠ·í•˜ì§€ë§Œ ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œ ì œê±°
        (ê°€ì¥ ìœ„í—˜í•œ ì¤‘ê°„ ìœ ì‚¬ë„ ë²”ìœ„)
        """
        
        query_embedding = get_embedding(query)
        
        filtered = []
        for doc in documents:
            doc_embedding = get_embedding(doc)
            similarity = cosine_similarity([query_embedding], [doc_embedding])[0][0]
            
            # ìœ„í—˜ êµ¬ê°„ (0.3-0.6) ì œì™¸
            if similarity < 0.3 or similarity > 0.6:
                filtered.append(doc)
            else:
                # ì¤‘ê°„ ìœ ì‚¬ë„: ì¶”ê°€ ê²€ì¦
                if self.verify_relevance(query, doc):
                    filtered.append(doc)
        
        return filtered
    
    def verify_relevance(self, query, document):
        """ì¶”ê°€ ê²€ì¦ (í‚¤ì›Œë“œ ê¸°ë°˜)"""
        query_keywords = extract_keywords(query)
        doc_keywords = extract_keywords(document)
        
        overlap = len(query_keywords & doc_keywords)
        return overlap >= 3  # ìµœì†Œ 3ê°œ í‚¤ì›Œë“œ ê³µí†µ
```

#### 4. ë¬¸ì„œ ì „ì²˜ë¦¬

```python
def preprocess_for_retrieval(document):
    """
    ì—°êµ¬ ê²°ê³¼: êµ¬ì¡°í™”ëœ ë¬¸ì„œê°€ ë” ì–´ë ¤ì›€
    â†’ ê²€ìƒ‰ ìµœì í™”ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬
    """
    
    # ê¸´ ë‚´ëŸ¬í‹°ë¸Œë¥¼ ë…ë¦½ì  ì²­í¬ë¡œ ë¶„í• 
    chunks = split_into_independent_chunks(document)
    
    # ê° ì²­í¬ì— ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    enriched_chunks = []
    for chunk in chunks:
        enriched = f"""
        [Document: {document.title}]
        [Section: {chunk.section}]
        
        {chunk.content}
        
        [End of section]
        """
        enriched_chunks.append(enriched)
    
    return enriched_chunks
```

#### 5. ê³„ì¸µì  ê²€ìƒ‰

```python
class HierarchicalRetrieval:
    """Context Rot ë°©ì§€ ê³„ì¸µì  ê²€ìƒ‰"""
    
    def retrieve(self, query, max_tokens=10000):
        """
        1ì°¨: ë¬¸ì„œ ë ˆë²¨ ê²€ìƒ‰ â†’ ìš”ì•½
        2ì°¨: ê´€ë ¨ ë¬¸ì„œ ë‚´ ì„¸ë¶€ ê²€ìƒ‰
        """
        
        # Phase 1: ë¬¸ì„œ ë ˆë²¨ (ìš”ì•½ë§Œ)
        doc_summaries = self.get_document_summaries(query, top_k=10)
        
        current_tokens = count_tokens('\n'.join(doc_summaries))
        
        if current_tokens > max_tokens:
            return doc_summaries[:5]  # ìš”ì•½ë§Œ ë°˜í™˜
        
        # Phase 2: ì„¸ë¶€ ë‚´ìš© (í•„ìš”ì‹œ)
        detailed_sections = []
        remaining_tokens = max_tokens - current_tokens
        
        for doc_summary in doc_summaries:
            if remaining_tokens < 1000:
                break
            
            # í•´ë‹¹ ë¬¸ì„œì˜ ê´€ë ¨ ì„¹ì…˜ë§Œ ê°€ì ¸ì˜¤ê¸°
            sections = self.get_relevant_sections(
                query, 
                doc_summary.doc_id,
                max_tokens=remaining_tokens
            )
            
            detailed_sections.extend(sections)
            remaining_tokens -= count_tokens('\n'.join(sections))
        
        return doc_summaries + detailed_sections
```

#### 6. ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

```python
class ContextRotMonitor:
    """Context Rot ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        self.metrics_history = []
    
    def monitor_session(self, context_size, response_quality, latency):
        """ì„¸ì…˜ ëª¨ë‹ˆí„°ë§"""
        
        metrics = {
            'timestamp': datetime.now(),
            'context_tokens': context_size,
            'quality_score': response_quality,
            'latency_ms': latency
        }
        
        self.metrics_history.append(metrics)
        
        # Context Rot ê°ì§€
        if self.is_rot_detected():
            self.trigger_alert()
    
    def is_rot_detected(self):
        """íŒ¨í„´ ë¶„ì„ìœ¼ë¡œ ê°ì§€"""
        
        if len(self.metrics_history) < 5:
            return False
        
        recent = self.metrics_history[-5:]
        
        # 1. í’ˆì§ˆ ì €í•˜ ì¶”ì„¸
        quality_trend = [m['quality_score'] for m in recent]
        if self.is_declining(quality_trend):
            return True
        
        # 2. ë ˆì´í„´ì‹œ ì¦ê°€
        latency_trend = [m['latency_ms'] for m in recent]
        if np.mean(latency_trend) > 5000:  # 5ì´ˆ ì´ìƒ
            return True
        
        # 3. ì»¨í…ìŠ¤íŠ¸ í¬ê¸° vs í’ˆì§ˆ ì—­ìƒê´€
        sizes = [m['context_tokens'] for m in recent]
        qualities = [m['quality_score'] for m in recent]
        correlation = np.corrcoef(sizes, qualities)[0, 1]
        
        if correlation < -0.7:  # ê°•í•œ ìŒì˜ ìƒê´€ê´€ê³„
            return True
        
        return False
    
    def trigger_alert(self):
        """ì•Œë¦¼ ë° ìë™ ì¡°ì¹˜"""
        logger.warning("Context Rot detected!")
        
        # ìë™ ì¡°ì¹˜
        self.reduce_context_size()
        self.clear_cache()
        self.switch_to_summarization_mode()
```

### ğŸ“Š ì‹¤ì „ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼

```
í…ŒìŠ¤íŠ¸: ê³ ê° ì§€ì› ì±—ë´‡ (1000ê°œ ëŒ€í™”)

Before ìµœì í™”:
â”œâ”€ í‰ê·  ì»¨í…ìŠ¤íŠ¸: 85K í† í°
â”œâ”€ ì •í™•ë„: 62%
â”œâ”€ í‰ê·  ì‘ë‹µ ì‹œê°„: 8.5ì´ˆ
â””â”€ ì›” ë¹„ìš©: $2,400

After ìµœì í™” (Context Engineering ì ìš©):
â”œâ”€ í‰ê·  ì»¨í…ìŠ¤íŠ¸: 12K í† í° (â†“ 86%)
â”œâ”€ ì •í™•ë„: 89% (â†‘ 27%p)
â”œâ”€ í‰ê·  ì‘ë‹µ ì‹œê°„: 2.1ì´ˆ (â†“ 75%)
â””â”€ ì›” ë¹„ìš©: $380 (â†“ 84%)

ì ìš© ê¸°ë²•:
âœ… Hierarchical Memory (3ê³„ì¸µ)
âœ… Adaptive Window Sizing
âœ… Distractor Filtering  
âœ… Strategic Placement
âœ… Real-time Monitoring
```

---

## ì‹¤ì „ êµ¬í˜„ ê°€ì´ë“œ

### ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```python
"""
í”„ë¡œë•ì…˜ê¸‰ ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì‹œìŠ¤í…œ
"""

import anthropic
from typing import List, Dict, Optional
import numpy as np
from datetime import datetime

class ProductionContextEngine:
    """
    Agent-Skills ì›ì¹™ì„ ì ìš©í•œ 
    í”„ë¡œë•ì…˜ ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì—”ì§„
    """
    
    def __init__(self, api_key: str):
        # Core components
        self.client = anthropic.Anthropic(api_key=api_key)
        
        # Memory system (Agent-Skills: memory-systems)
        self.memory = HierarchicalMemory()
        
        # Context optimization (Agent-Skills: context-optimization)
        self.optimizer = ContextOptimizationPipeline()
        
        # Context monitoring (Agent-Skills: context-degradation)
        self.monitor = ContextRotMonitor()
        
        # Evaluation (Agent-Skills: evaluation)
        self.evaluator = LLMJudge()
        
        # Configuration
        self.config = {
            'max_context_tokens': 100000,
            'target_context_tokens': 10000,
            'enable_caching': True,
            'enable_monitoring': True
        }
    
    async def process_query(
        self,
        user_query: str,
        documents: Optional[List[str]] = None,
        conversation_history: Optional[List[Dict]] = None,
        task_complexity: str = 'moderate'
    ) -> Dict:
        """
        ì¿¼ë¦¬ ì²˜ë¦¬ ë©”ì¸ í”Œë¡œìš°
        
        Args:
            user_query: ì‚¬ìš©ì ì§ˆì˜
            documents: ì°¸ì¡° ë¬¸ì„œë“¤ (optional)
            conversation_history: ëŒ€í™” ì´ë ¥ (optional)
            task_complexity: ì‘ì—… ë³µì¡ë„ ('simple'|'moderate'|'complex')
        
        Returns:
            {
                'response': ì‘ë‹µ í…ìŠ¤íŠ¸,
                'metrics': ì„±ëŠ¥ ë©”íŠ¸ë¦­,
                'context_used': ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸
            }
        """
        
        start_time = datetime.now()
        
        # Phase 1: Context ì¤€ë¹„
        print("ğŸ“Š Phase 1: Context Preparation")
        optimized_context = await self._prepare_context(
            user_query,
            documents or [],
            conversation_history or [],
            task_complexity
        )
        
        # Phase 2: API í˜¸ì¶œ
        print("ğŸ¤– Phase 2: Claude API Call")
        response = await self._call_claude_api(optimized_context)
        
        # Phase 3: í‰ê°€ ë° ëª¨ë‹ˆí„°ë§
        print("ğŸ“ˆ Phase 3: Evaluation & Monitoring")
        metrics = self._collect_metrics(
            user_query,
            optimized_context,
            response,
            start_time
        )
        
        # Phase 4: ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸
        print("ğŸ’¾ Phase 4: Memory Update")
        self._update_memory(user_query, response)
        
        return {
            'response': response,
            'metrics': metrics,
            'context_used': optimized_context
        }
    
    async def _prepare_context(
        self,
        query: str,
        documents: List[str],
        history: List[Dict],
        complexity: str
    ) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (ìµœì í™” ì ìš©)"""
        
        # 1. ì ì‘í˜• ìœˆë„ìš° í¬ê¸° ê²°ì •
        window_manager = AdaptiveContextWindow()
        max_tokens = window_manager.determine_optimal_window({
            'query': query,
            'complexity': complexity
        })
        
        print(f"  â””â”€ Target context size: {max_tokens:,} tokens")
        
        # 2. ê´€ë ¨ ë©”ëª¨ë¦¬ ê²€ìƒ‰
        memory_context = self.memory.retrieve(query, max_items=3)
        
        # 3. ë¬¸ì„œ ìµœì í™”
        if documents:
            documents = self.optimizer.optimize(
                query=query,
                raw_documents=documents,
                max_tokens=int(max_tokens * 0.6)  # 60% for documents
            )
        
        # 4. ëŒ€í™” ì´ë ¥ ì••ì¶•
        if history:
            compressor = ConversationCompressor()
            history = compressor.compress_long_conversation(history)
        
        # 5. ê³„ì¸µì  ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self._build_hierarchical_context(
            query=query,
            documents=documents,
            memory=memory_context,
            history=history
        )
        
        # 6. ìµœì¢… í† í° ê²€ì¦
        actual_tokens = count_tokens(context)
        if actual_tokens > max_tokens:
            print(f"  âš ï¸  Context too large ({actual_tokens:,} > {max_tokens:,}), compressing...")
            context = self._emergency_compression(context, max_tokens)
        
        print(f"  âœ… Final context: {count_tokens(context):,} tokens")
        
        return context
    
    def _build_hierarchical_context(
        self,
        query: str,
        documents: List[str],
        memory: List[tuple],
        history: List[Dict]
    ) -> str:
        """
        ê³„ì¸µì  ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        (Agent-Skills: context-fundamentals ì›ì¹™ ì ìš©)
        """
        
        sections = []
        
        # Level 1: í•µì‹¬ ì‘ì—… ì •ë³´ (ìµœìš°ì„ )
        sections.append(f"""
# PRIMARY OBJECTIVE
User Query: {query}

Task: Provide accurate, helpful response based on available context.
""")
        
        # Level 2: ê´€ë ¨ ë©”ëª¨ë¦¬ (ë†’ì€ ìš°ì„ ìˆœìœ„)
        if memory:
            memory_text = "\n".join([
                f"- [{source}] {content}"
                for source, content in memory
            ])
            sections.append(f"""
# RELEVANT MEMORY
{memory_text}
""")
        
        # Level 3: ì°¸ì¡° ë¬¸ì„œ (ì¤‘ê°„ ìš°ì„ ìˆœìœ„)
        if documents:
            # Strategic Placement: í•µì‹¬ ë¬¸ì„œë¥¼ ì‹œì‘ê³¼ ëì—
            doc_text = "\n\n".join([
                f"## Document {i+1}\n{doc}"
                for i, doc in enumerate(documents)
            ])
            sections.append(f"""
# REFERENCE DOCUMENTS
{doc_text}
""")
        
        # Level 4: ëŒ€í™” ì´ë ¥ (ë³´ì¡° ì •ë³´)
        if history:
            history_text = "\n".join([
                f"{msg['role']}: {msg['content'][:200]}..."
                if len(msg['content']) > 200
                else f"{msg['role']}: {msg['content']}"
                for msg in history
            ])
            sections.append(f"""
# CONVERSATION HISTORY
{history_text}
""")
        
        return "\n\n".join(sections)
    
    async def _call_claude_api(self, context: str) -> str:
        """Claude API í˜¸ì¶œ (ìºì‹± ì§€ì›)"""
        
        try:
            if self.config['enable_caching']:
                # Prompt Caching í™œìš©
                message = self.client.messages.create(
                    model="claude-sonnet-4-20250514",
                    max_tokens=4096,
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": context,
                                    "cache_control": {"type": "ephemeral"}
                                }
                            ]
                        }
                    ]
                )
            else:
                message = self.client.messages.create(
                    model="claude-sonnet-4-20250514",
                    max_tokens=4096,
                    messages=[{"role": "user", "content": context}]
                )
            
            return message.content[0].text
            
        except Exception as e:
            print(f"âŒ API Error: {e}")
            return f"Error occurred: {str(e)}"
    
    def _collect_metrics(
        self,
        query: str,
        context: str,
        response: str,
        start_time: datetime
    ) -> Dict:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        
        metrics = {
            'query_tokens': count_tokens(query),
            'context_tokens': count_tokens(context),
            'response_tokens': count_tokens(response),
            'total_tokens': count_tokens(query + context + response),
            'latency_ms': (datetime.now() - start_time).total_seconds() * 1000
        }
        
        # Context Rot ëª¨ë‹ˆí„°ë§
        if self.config['enable_monitoring']:
            self.monitor.monitor_session(
                context_size=metrics['context_tokens'],
                response_quality=0.8,  # TODO: ì‹¤ì œ í’ˆì§ˆ í‰ê°€
                latency=metrics['latency_ms']
            )
        
        return metrics
    
    def _update_memory(self, query: str, response: str):
        """ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸"""
        
        # ì¤‘ìš”í•œ ì •ë³´ë§Œ ë©”ëª¨ë¦¬ì— ì €ì¥
        if self._is_important_exchange(query, response):
            self.memory.add(
                f"Q: {query}\nA: {response[:500]}...",
                importance='high'
            )
    
    def _is_important_exchange(self, query: str, response: str) -> bool:
        """ì¤‘ìš”í•œ êµí™˜ì¸ì§€ íŒë‹¨"""
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: ì¶©ë¶„íˆ ê¸´ ì‘ë‹µ
        return len(response) > 200
    
    def _emergency_compression(self, context: str, max_tokens: int) -> str:
        """ê¸´ê¸‰ ì••ì¶•"""
        compressor = ContextCompactor()
        return compressor.compress_to_fit(context, max_tokens)


# ============================================
# ì‚¬ìš© ì˜ˆì‹œ
# ============================================

async def main():
    """ì‹¤ì „ ì‚¬ìš© ì˜ˆì œ"""
    
    # ì—”ì§„ ì´ˆê¸°í™”
    engine = ProductionContextEngine(api_key="your-api-key")
    
    # ì‹œë‚˜ë¦¬ì˜¤: ê¸°ìˆ  ë¬¸ì„œ ë¶„ì„
    documents = [
        "Python asyncioëŠ” ë¹„ë™ê¸° I/O í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤...",
        "FastAPIëŠ” Python ì›¹ í”„ë ˆì„ì›Œí¬ë¡œ...",
        # ... 100ê°œì˜ ë¬¸ì„œ
    ]
    
    # ì¿¼ë¦¬ ì²˜ë¦¬
    result = await engine.process_query(
        user_query="Pythonìœ¼ë¡œ ë¹„ë™ê¸° ì›¹ APIë¥¼ ë§Œë“œëŠ” ë°©ë²•ì€?",
        documents=documents,
        task_complexity='moderate'
    )
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ¯ Response:")
    print("="*60)
    print(result['response'])
    
    print("\n" + "="*60)
    print("ğŸ“Š Metrics:")
    print("="*60)
    for key, value in result['metrics'].items():
        print(f"  {key}: {value:,}" if isinstance(value, int) else f"  {key}: {value}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

---

## ì»¨í…ìŠ¤íŠ¸ ìµœì í™” íŒ¨í„´

### íŒ¨í„´ 1: Progressive Disclosure (ì ì§„ì  ë…¸ì¶œ)

```python
class ProgressiveDisclosure:
    """í•„ìš”í•  ë•Œë§Œ ì •ë³´ ë¡œë“œ"""
    
    def __init__(self):
        self.skill_metadata = self.load_all_metadata()
        self.loaded_skills = {}
    
    def load_all_metadata(self):
        """ì‹œì‘ì‹œ ë©”íƒ€ë°ì´í„°ë§Œ ë¡œë“œ"""
        # ê° ìŠ¤í‚¬ë‹¹ ìˆ˜ì‹­ í† í°ë§Œ
        return {
            'pptx': {
                'name': 'PowerPoint Creation',
                'description': 'Create professional presentations',
                'tokens': 45
            },
            'xlsx': {
                'name': 'Excel Manipulation',
                'description': 'Work with spreadsheets',
                'tokens': 38
            },
            # ... 100ê°œ ìŠ¤í‚¬
        }
    
    def activate_skill(self, skill_name):
        """í•„ìš”í•  ë•Œ ì „ì²´ ìŠ¤í‚¬ ë¡œë“œ"""
        if skill_name not in self.loaded_skills:
            # ì „ì²´ ë‚´ìš© ë¡œë“œ (ìˆ˜ì²œ~ìˆ˜ë§Œ í† í°)
            self.loaded_skills[skill_name] = self.load_full_skill(skill_name)
        
        return self.loaded_skills[skill_name]

# íš¨ê³¼:
# 100ê°œ ìŠ¤í‚¬ Ã— 45 í† í° = 4,500 í† í° (ë©”íƒ€ë°ì´í„°)
# vs
# 100ê°œ ìŠ¤í‚¬ Ã— 5,000 í† í° = 500,000 í† í° (ì „ì²´)
# â†’ 99% í† í° ì ˆê°!
```

### íŒ¨í„´ 2: Semantic Chunking (ì˜ë¯¸ë¡ ì  ì²­í‚¹)

```python
def semantic_chunking(document, max_chunk_tokens=1000):
    """ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í¬ ë¶„í• """
    
    # 1. ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í• 
    paragraphs = document.split('\n\n')
    
    # 2. ì˜ë¯¸ì ìœ¼ë¡œ ì—°ê´€ëœ ë¬¸ë‹¨ ê·¸ë£¹í™”
    chunks = []
    current_chunk = []
    current_tokens = 0
    
    for para in paragraphs:
        para_tokens = count_tokens(para)
        
        if current_tokens + para_tokens > max_chunk_tokens:
            # í˜„ì¬ ì²­í¬ ì™„ì„±
            chunks.append('\n\n'.join(current_chunk))
            current_chunk = [para]
            current_tokens = para_tokens
        else:
            # í˜„ì¬ ì²­í¬ì— ì¶”ê°€
            current_chunk.append(para)
            current_tokens += para_tokens
    
    # ë§ˆì§€ë§‰ ì²­í¬
    if current_chunk:
        chunks.append('\n\n'.join(current_chunk))
    
    return chunks
```

### íŒ¨í„´ 3: Dynamic Context Pruning (ë™ì  ê°€ì§€ì¹˜ê¸°)

```python
class DynamicPruner:
    """ì‹¤ì‹œê°„ ì»¨í…ìŠ¤íŠ¸ ê°€ì§€ì¹˜ê¸°"""
    
    def prune_during_generation(self, context, current_response):
        """
        ì‘ë‹µ ìƒì„± ì¤‘ ë¶ˆí•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ ì œê±°
        """
        
        # ì´ë¯¸ ì‚¬ìš©ëœ ì •ë³´ ì‹ë³„
        used_info = self.extract_used_information(context, current_response)
        
        # ì‚¬ìš©ë˜ì§€ ì•Šì€ ë¶€ë¶„ ì œê±°
        pruned = self.remove_unused_sections(context, used_info)
        
        # í† í° ì˜ˆì‚°ì— ë§ê²Œ ì¡°ì •
        return self.fit_to_budget(pruned, max_tokens=10000)
```

---

## í”„ë¡œë•ì…˜ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### ğŸ¯ ì²´í¬ë¦¬ìŠ¤íŠ¸

```markdown
âœ… ì„¤ê³„ ë‹¨ê³„
[ ] ì‘ì—… ë³µì¡ë„ë³„ ì»¨í…ìŠ¤íŠ¸ ì „ëµ ìˆ˜ë¦½
[ ] ë©”ëª¨ë¦¬ ê³„ì¸µ êµ¬ì¡° ì„¤ê³„
[ ] ìºì‹± ì „ëµ ê³„íš
[ ] ëª¨ë‹ˆí„°ë§ ì§€í‘œ ì •ì˜

âœ… êµ¬í˜„ ë‹¨ê³„
[ ] Progressive Disclosure ì ìš©
[ ] Adaptive Window Sizing êµ¬í˜„
[ ] Distractor Filtering ì ìš©
[ ] Strategic Placement êµ¬í˜„
[ ] Emergency Compression ì¤€ë¹„

âœ… í…ŒìŠ¤íŠ¸ ë‹¨ê³„
[ ] ë‹¤ì–‘í•œ ì»¨í…ìŠ¤íŠ¸ í¬ê¸° í…ŒìŠ¤íŠ¸
[ ] Context Rot ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
[ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìˆ˜í–‰
[ ] ë¹„ìš© ë¶„ì„

âœ… ëª¨ë‹ˆí„°ë§ ë‹¨ê³„
[ ] í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
[ ] í’ˆì§ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
[ ] Context Rot ê°ì§€ ì‹œìŠ¤í…œ
[ ] ì•Œë¦¼ ì„¤ì •

âœ… ìµœì í™” ë‹¨ê³„
[ ] A/B í…ŒìŠ¤íŠ¸
[ ] ì»¨í…ìŠ¤íŠ¸ ì „ëµ ì¡°ì •
[ ] ìºì‹œ íˆíŠ¸ìœ¨ ê°œì„ 
[ ] ë¹„ìš© ìµœì í™”
```

### ğŸ“Š ì„±ëŠ¥ ëª©í‘œ

```
ê¸°ì¤€ ë©”íŠ¸ë¦­:

í† í° íš¨ìœ¨ì„±:
â”œâ”€ ëª©í‘œ: ì´ í† í° < 20K
â”œâ”€ ìš°ìˆ˜: < 10K
â””â”€ íƒì›”: < 5K

ì‘ë‹µ í’ˆì§ˆ:
â”œâ”€ ëª©í‘œ: ì •í™•ë„ > 80%
â”œâ”€ ìš°ìˆ˜: > 90%
â””â”€ íƒì›”: > 95%

ì‘ë‹µ ì†ë„:
â”œâ”€ ëª©í‘œ: < 3ì´ˆ
â”œâ”€ ìš°ìˆ˜: < 2ì´ˆ
â””â”€ íƒì›”: < 1ì´ˆ

ë¹„ìš© íš¨ìœ¨:
â”œâ”€ ëª©í‘œ: ì›” ë¹„ìš© < $1,000
â”œâ”€ ìš°ìˆ˜: < $500
â””â”€ íƒì›”: < $200
```

### ğŸ”§ ë””ë²„ê¹… ê°€ì´ë“œ

```python
class ContextDebugger:
    """ì»¨í…ìŠ¤íŠ¸ ë¬¸ì œ ë””ë²„ê¹…"""
    
    def diagnose(self, context, response):
        """ì¢…í•© ì§„ë‹¨"""
        
        issues = []
        
        # 1. í¬ê¸° í™•ì¸
        tokens = count_tokens(context)
        if tokens > 50000:
            issues.append({
                'type': 'SIZE_WARNING',
                'severity': 'high',
                'message': f'Context too large: {tokens:,} tokens',
                'suggestion': 'Apply compression or filtering'
            })
        
        # 2. ì •ë³´ ë°€ë„ í™•ì¸
        density = self.calculate_information_density(context, response)
        if density < 0.3:
            issues.append({
                'type': 'LOW_DENSITY',
                'severity': 'medium',
                'message': f'Low information density: {density:.2%}',
                'suggestion': 'Remove irrelevant context'
            })
        
        # 3. ì¤‘ë³µ í™•ì¸
        duplicates = self.find_duplicates(context)
        if duplicates:
            issues.append({
                'type': 'DUPLICATES',
                'severity': 'low',
                'message': f'Found {len(duplicates)} duplicate sections',
                'suggestion': 'Deduplicate content'
            })
        
        return issues
```

---

## ì°¸ê³  ìë£Œ

### ğŸ”— GitHub ë¦¬í¬ì§€í† ë¦¬

1. **Agent-Skills for Context Engineering**
   - https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering
   - 7ê°€ì§€ í•µì‹¬ ìŠ¤í‚¬ + ì‹¤ì „ ì˜ˆì œ

2. **Anthropic Skills (ê³µì‹)**
   - https://github.com/anthropics/skills
   - ê³µì‹ ìŠ¤í‚¬ ì˜ˆì œ ë° ë¬¸ì„œ

3. **LangChain Context Engineering**
   - https://github.com/langchain-ai/context_engineering
   - 4ëŒ€ ì „ëµ ë…¸íŠ¸ë¶

### ğŸ“š ì—°êµ¬ ë…¼ë¬¸

1. **Context Rot (Chroma Research, 2025)**
   - https://research.trychroma.com/context-rot
   - 18ê°œ ëª¨ë¸ í‰ê°€, ì‹¤í—˜ ì½”ë“œ ê³µê°œ

2. **Lost in the Middle (Liu et al., 2023)**
   - ìœ„ì¹˜ í¸í–¥ í˜„ìƒ ìµœì´ˆ ë³´ê³ 
   - U-Shaped Attention Curve

3. **Large Language Models Can Be Easily Distracted (Shi et al., 2023)**
   - Distractor íš¨ê³¼ ì—°êµ¬

### ğŸ“ ê³µì‹ ë¬¸ì„œ

1. **Anthropic Claude Documentation**
   - https://docs.anthropic.com/
   - Agent Skills ê°€ì´ë“œ

2. **Anthropic Prompt Engineering**
   - https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview

3. **Agent Skills í‘œì¤€ (Open Standard)**
   - https://agentskills.io/
   - í¬ë¡œìŠ¤ í”Œë«í¼ í˜¸í™˜ ëª…ì„¸

### ğŸ› ï¸ ë„êµ¬ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬

```python
# í† í° ì¹´ìš´íŒ…
pip install tiktoken

# ì„ë² ë”© & ë²¡í„° ê²€ìƒ‰
pip install chromadb
pip install pinecone-client

# LangChain
pip install langchain
pip install langchain-anthropic

# ë°ì´í„° ì²˜ë¦¬
pip install numpy pandas scikit-learn

# ëª¨ë‹ˆí„°ë§
pip install prometheus-client
```

### ğŸ“– ì¶”ì²œ í•™ìŠµ ê²½ë¡œ

```
ğŸŒ± ì´ˆê¸‰ (1-2ì£¼)
â”œâ”€ Agent-Skills README ì½ê¸°
â”œâ”€ Context Fundamentals í•™ìŠµ
â”œâ”€ Anthropic ê³µì‹ ë¬¸ì„œ í›‘ê¸°
â””â”€ ê°„ë‹¨í•œ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ êµ¬í˜„

ğŸŒ¿ ì¤‘ê¸‰ (2-4ì£¼)
â”œâ”€ Context Rot ë…¼ë¬¸ ì½ê¸°
â”œâ”€ 4ëŒ€ ì „ëµ (Write/Select/Compress/Isolate) ì ìš©
â”œâ”€ ê³„ì¸µì  ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ êµ¬í˜„
â””â”€ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìˆ˜í–‰

ğŸŒ³ ê³ ê¸‰ (1-3ê°œì›”)
â”œâ”€ Knowledge Graph í†µí•©
â”œâ”€ ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ êµ¬ì¶•
â”œâ”€ í”„ë¡œë•ì…˜ ìµœì í™”
â””â”€ ì»¤ìŠ¤í…€ í‰ê°€ ì‹œìŠ¤í…œ ê°œë°œ
```

### ğŸ’¬ ì»¤ë®¤ë‹ˆí‹°

- **Reddit**: r/ClaudeAI
- **Discord**: Anthropic Community
- **Twitter/X**: #ContextEngineering
- **Hacker News**: agent skills ê²€ìƒ‰

### ğŸ“º ë™ì˜ìƒ ìë£Œ

- Andrej Karpathy: "Software Is Changing" (Y Combinator)
- Anthropic: "Agent Skills Introduction"
- LangChain: "Context Engineering Strategies"

---

## ë§ˆì¹˜ë©°

### ğŸ¯ í•µì‹¬ ìš”ì•½

**1. ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ = AI ì—ì´ì „íŠ¸ì˜ í•µì‹¬ ì—­ëŸ‰**

```
"Context engineering is effectively the #1 job 
of engineers building AI agents."
```

**2. Agent-Skills â‰  Skill-Creator**

```
Agent-Skills:     ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ì›ì¹™ê³¼ íŒ¨í„´
Skill-Creator:    Claudeìš© ìŠ¤í‚¬ ìƒì„± í”„ë ˆì„ì›Œí¬

â†’ ìƒí˜¸ ë³´ì™„ì , í•¨ê»˜ ì‚¬ìš© ê¶Œì¥
```

**3. Context RotëŠ” ì‹¤ì¬í•˜ëŠ” ë¬¸ì œ**

```
ì—°êµ¬ ê²°ê³¼ (2025):
- 18ê°œ SOTA ëª¨ë¸ ëª¨ë‘ ì˜í–¥ ë°›ìŒ
- ê°„ë‹¨í•œ ì‘ì—…ì—ì„œë„ ì„±ëŠ¥ ì €í•˜
- ì»¨í…ìŠ¤íŠ¸ í¬ê¸° â†‘ â‰  ì„±ëŠ¥ â†‘
```

**4. 4ëŒ€ ì „ëµ (Write, Select, Compress, Isolate)**

```
Write:      ì»¨í…ìŠ¤íŠ¸ ì™¸ë¶€ ì €ì¥
Select:     ê´€ë ¨ ì •ë³´ë§Œ ë¡œë“œ
Compress:   í† í° íš¨ìœ¨ ê·¹ëŒ€í™”
Isolate:    ì»¨í…ìŠ¤íŠ¸ ë¶„ë¦¬
```

**5. ì‹¤ì „ ì ìš©ì´ í•µì‹¬**

```
ì´ë¡ ë§Œìœ¼ë¡œëŠ” ë¶€ì¡±
â†’ ë²¤ì¹˜ë§ˆí¬, ì¸¡ì •, ìµœì í™”
â†’ ì§€ì†ì  ëª¨ë‹ˆí„°ë§
â†’ í”¼ë“œë°± ê¸°ë°˜ ê°œì„ 
```

### ğŸš€ ë‹¤ìŒ ë‹¨ê³„

```markdown
ğŸ“… 1ì£¼ì°¨: ê¸°ì´ˆ ë‹¤ì§€ê¸°
- [ ] context-fundamentals ìŠ¤í‚¬ ì •ë…
- [ ] ê°„ë‹¨í•œ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì êµ¬í˜„
- [ ] í† í° ì¹´ìš´íŒ… ì‹œìŠ¤í…œ êµ¬ì¶•

ğŸ“… 2ì£¼ì°¨: ìµœì í™” ì ìš©
- [ ] Adaptive Window Sizing êµ¬í˜„
- [ ] Distractor Filtering í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìˆ˜í–‰

ğŸ“… 3ì£¼ì°¨: ê³ ê¸‰ ê¸°ëŠ¥
- [ ] Hierarchical Memory êµ¬í˜„
- [ ] Multi-Agent íŒ¨í„´ ì‹¤í—˜
- [ ] Context Rot ëª¨ë‹ˆí„°ë§

ğŸ“… 4ì£¼ì°¨: í”„ë¡œë•ì…˜ ì¤€ë¹„
- [ ] ì „ì²´ ì‹œìŠ¤í…œ í†µí•©
- [ ] A/B í…ŒìŠ¤íŠ¸
- [ ] ë¹„ìš© ìµœì í™”
- [ ] ë¬¸ì„œí™”
```

### ğŸ’¡ ë§ˆì§€ë§‰ ì¡°ì–¸

> "ë” í° ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ê°€ í•´ê²°ì±…ì´ ì•„ë‹™ë‹ˆë‹¤.  
> ë” ìŠ¤ë§ˆíŠ¸í•œ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ê°€ í•´ê²°ì±…ì…ë‹ˆë‹¤."

**ì„±ê³µì˜ ì—´ì‡ :**
1. ì¸¡ì •í•˜ì§€ ì•Šìœ¼ë©´ ê°œì„ í•  ìˆ˜ ì—†ë‹¤
2. ì‘ê²Œ ì‹œì‘í•´ì„œ ì ì§„ì ìœ¼ë¡œ í™•ì¥
3. ì‹¤í—˜í•˜ê³ , ì‹¤íŒ¨í•˜ê³ , ë°°ìš°ê³ , ë°˜ë³µ
4. ì»¤ë®¤ë‹ˆí‹°ì™€ ê³µìœ í•˜ê³  ë°°ìš°ê¸°

í–‰ìš´ì„ ë¹•ë‹ˆë‹¤! ğŸ‰

---

**ë¬¸ì„œ ì •ë³´**
- ì‘ì„±ì: Agent Skills ê°œë°œì ê°€ì´ë“œ
- ìµœì¢… ì—…ë°ì´íŠ¸: 2025-12-25
- ë²„ì „: 1.0.0
- ë¼ì´ì„ ìŠ¤: MIT

**ì°¸ê³ í•œ ì£¼ìš” ìë£Œ:**
- Agent-Skills for Context Engineering (Muratcan Koylan)
- Context Rot Research (Chroma, 2025)
- Anthropic Agent Skills Documentation
- LangChain Context Engineering Strategies

---

**ì‘ì„± ì¼ì**: 2025-12-25
