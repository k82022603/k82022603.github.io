---
title: "SKILL.md êµ¬ì¡°"
date: 2025-12-25 09:00:00 +0900
categories: [AI,  Context Engineering]
mermaid: [True]
tags: [AI,  context-engineering,  agent-skills,  anthropic-skills,  AIOptimization,  prompt-engineering,  Claude.write]
---

---
name: Brand Guidelines
description: Apply Acme Corp brand guidelines to documents
version: 1.0.0
dependencies:
  - python-docx
  - Pillow
---

# Overview
[ìŠ¤í‚¬ ì„¤ëª…]

# Instructions
[êµ¬ì²´ì  ì§€ì‹œì‚¬í•­]

# Examples
[ì˜ˆì œë“¤]
```

**4. ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€**

Anthropicì˜ ë¬¸ì„œ ìƒì„± ê¸°ëŠ¥ì€ ëª¨ë‘ ìŠ¤í‚¬ ê¸°ë°˜:
- `docx` - Word ë¬¸ì„œ ìƒì„±/í¸ì§‘
- `pptx` - PowerPoint í”„ë ˆì  í…Œì´ì…˜
- `xlsx` - Excel ìŠ¤í”„ë ˆë“œì‹œíŠ¸
- `pdf` - PDF ìƒì„±/ì–‘ì‹ ì‘ì„±

**5. API í†µí•©**

```python
import anthropic

client = anthropic.Anthropic()

response = client.beta.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=4096,
    betas=["code-execution-2025-08-25", "skills-2025-10-02"],
    container={
        "skills": [
            {
                "type": "anthropic",
                "skill_id": "pptx",
                "version": "latest"
            }
        ]
    },
    messages=[{
        "role": "user",
        "content": "Create a presentation about renewable energy"
    }],
    tools=[{
        "type": "code_execution_20250825",
        "name": "code_execution"
    }]
)
```

### Agent-Skills for Context Engineering (ì»¤ë®¤ë‹ˆí‹° í”„ë¡œì íŠ¸)

**ì¶œì²˜**: Muratcan Koylanì˜ GitHub ë¦¬í¬ì§€í† ë¦¬  
**URL**: https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering

#### í•µì‹¬ íŠ¹ì§•

**1. ì§€ì‹ ê¸°ë°˜ (Knowledge Base)**
- ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì›ì¹™ê³¼ íŒ¨í„´ ëª¨ìŒ
- "ë©”íƒ€ ì—ì´ì „íŠ¸" ì§€ì‹ ì œê³µ
- AIê°€ ìì‹ ì˜ ì¸ì§€ ìì›ì„ ê´€ë¦¬í•˜ëŠ” ë°©ë²• êµìœ¡

**2. 7ê°€ì§€ í•µì‹¬ ìŠ¤í‚¬** (ë‹¤ìŒ ì„¹ì…˜ì—ì„œ ìƒì„¸ ì„¤ëª…)

```
1. context-fundamentals     â†’ ì»¨í…ìŠ¤íŠ¸ ê¸°ì´ˆ
2. context-degradation      â†’ ì„±ëŠ¥ ì €í•˜ ì´í•´
3. context-optimization     â†’ ìµœì í™” ê¸°ë²•
4. multi-agent-patterns     â†’ ë©€í‹° ì—ì´ì „íŠ¸ íŒ¨í„´
5. memory-systems          â†’ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ
6. tool-design             â†’ ë„êµ¬ ì„¤ê³„
7. evaluation              â†’ í‰ê°€ ë° ì¸¡ì •
```

**3. ë²”ìš©ì„±**

```markdown
í”Œë«í¼ ë…ë¦½ì :
âœ… Claude Code
âœ… Cursor
âœ… ì»¤ìŠ¤í…€ ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬
âœ… ëª¨ë“  ìŠ¤í‚¬ ì§€ì› í”Œë«í¼

ì‹¤ì œë¡œ í…ŒìŠ¤íŠ¸:
$ cat context-fundamentals/SKILL.md | 
  your-llm-tool --prompt "ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬ ë°©ë²• ì„¤ëª…"
```

**4. ì‹¤ì „ ì¤‘ì‹¬ êµ¬ì„±**

ê° ìŠ¤í‚¬ì€ ë‹¤ìŒì„ í¬í•¨:
- ì´ë¡ ì  ë°°ê²½ (Why)
- ì‹¤ì „ íŒ¨í„´ (How)
- ì½”ë“œ ì˜ˆì œ (What)
- ì—°êµ¬ ì°¸ì¡° (Evidence)

### í•µì‹¬ ì°¨ì´ì  ìš”ì•½í‘œ

| ì¸¡ë©´ | Skill-Creator (Anthropic) | Agent-Skills (Community) |
|------|--------------------------|-------------------------|
| **ì„±ê²©** | ë„êµ¬ ìƒì„± í”„ë ˆì„ì›Œí¬ | ì‹¤ì „ ì§€ì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ |
| **ì¶”ìƒí™”** | ë©”íƒ€ ë ˆë²¨ (ìŠ¤í‚¬ ë§Œë“œëŠ” ë²•) | êµ¬í˜„ ë ˆë²¨ (ì‚¬ìš© ê°€ëŠ¥í•œ ì§€ì‹) |
| **ì£¼ìš” ê¸°ëŠ¥** | - SKILL.md í…œí”Œë¦¿ ì œê³µ<br>- ìŠ¤í‚¬ êµ¬ì¡°í™” ê°€ì´ë“œ<br>- API í†µí•© | - ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ì›ì¹™<br>- ì„±ëŠ¥ ìµœì í™” íŒ¨í„´<br>- ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì„¤ê³„ |
| **ì‚¬ìš© ë°©ë²•** | ìŠ¤í‚¬ íŒŒì¼ ìƒì„± â†’ ë“±ë¡ â†’ ìë™ ë¡œë“œ | ì§€ì‹ ìŠµë“ â†’ ì›ì¹™ ì ìš© â†’ ì»¤ìŠ¤í…€ êµ¬í˜„ |
| **í†µí•©** | Claude í”Œë«í¼ ë„¤ì´í‹°ë¸Œ | í”Œë«í¼ ë…ë¦½ì  |
| **í•™ìŠµ ê³¡ì„ ** | í”„ë ˆì„ì›Œí¬ êµ¬ì¡° ì´í•´ í•„ìš” | ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ì›ì¹™ |
| **ë²„ì „ ê´€ë¦¬** | API ë ˆë²¨ ë²„ì €ë‹ ì§€ì› | Git ê¸°ë°˜ ë²„ì „ ê´€ë¦¬ |
| **ë³´ì•ˆ** | ìƒŒë“œë°•ìŠ¤ ì‹¤í–‰ í™˜ê²½ | ì§€ì‹ ì „ë‹¬ (ì½”ë“œ ì‹¤í–‰ ì—†ìŒ) |

### ğŸ”§ ë¹„ìœ ë¡œ ì´í•´í•˜ê¸°

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ê³µêµ¬ ì œì‘ ë„êµ¬ vs ê³µêµ¬ ì‚¬ìš© ë§¤ë‰´ì–¼

Skill-Creator        Agent-Skills
     â†“                    â†“
  ì„ ë°˜, ë°€ë§           ì‘ì—… ì§€ì¹¨ì„œ
 (ë„êµ¬ ë§Œë“œëŠ” ê¸°ê³„)    (ë„êµ¬ ì‚¬ìš©ë²•)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

í”„ë¡œê·¸ë˜ë° ì–¸ì–´ vs ë””ìì¸ íŒ¨í„´

Skill-Creator        Agent-Skills
     â†“                    â†“
  Python ì–¸ì–´          GoF íŒ¨í„´
  (ë¬¸ë²•, êµ¬ì¡°)         (ì›ì¹™, íŒ¨í„´)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ChatGPTì™€ Geminiì˜ ê´€ê³„ì²˜ëŸ¼:
- í° ë²”ì£¼: ë‘˜ ë‹¤ "AI ìŠ¤í‚¬ ê´€ë ¨"
- ì„¸ë¶€ ê¸°ëŠ¥: ì™„ì „íˆ ë‹¤ë¥¸ ëª©ì ê³¼ ì‚¬ìš©ë²•
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### ì‹¤ë¬´ í™œìš© ì „ëµ

#### ì‹œë‚˜ë¦¬ì˜¤ 1: ê¸°ì—… ë‚´ë¶€ ì›Œí¬í”Œë¡œìš° ìë™í™”

```
ìƒí™©: ë¸Œëœë“œ ê°€ì´ë“œë¼ì¸ì— ë§ëŠ” ë¬¸ì„œ ìë™ ìƒì„±

âœ… Skill-Creator ì‚¬ìš©:
1. brand-guidelines.md ìŠ¤í‚¬ ìƒì„±
2. íšŒì‚¬ ë¡œê³ , ì»¬ëŸ¬, í°íŠ¸ í¬í•¨
3. Claudeì—ê²Œ ë“±ë¡
4. "Q3 ë¦¬í¬íŠ¸ ë§Œë“¤ì–´ì¤˜" â†’ ìë™ìœ¼ë¡œ ìŠ¤í‚¬ ì ìš©

âœ… Agent-Skills í™œìš©:
1. context-fundamentals í•™ìŠµ
2. ë¸Œëœë“œ ê°€ì´ë“œë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨í•˜ëŠ” ë°©ë²• ì´í•´
3. context-optimization íŒ¨í„´ ì ìš©
4. í† í° ì‚¬ìš©ëŸ‰ ìµœì†Œí™”í•˜ë©´ì„œ ì¼ê´€ì„± ìœ ì§€
```

#### ì‹œë‚˜ë¦¬ì˜¤ 2: RAG ì‹œìŠ¤í…œ ìµœì í™”

```
ìƒí™©: ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì„±ëŠ¥ ê°œì„ 

âœ… Skill-Creator:
- ì§ì ‘ì  í•´ê²°ì±… ì—†ìŒ (ë„êµ¬ ìƒì„± í”„ë ˆì„ì›Œí¬)

âœ… Agent-Skills:
1. context-degradation ìŠ¤í‚¬ë¡œ ë¬¸ì œ ì§„ë‹¨
2. context-optimizationìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ ì••ì¶•
3. memory-systemsë¡œ ê³„ì¸µì  ê²€ìƒ‰ êµ¬í˜„
4. evaluationìœ¼ë¡œ ì„±ëŠ¥ ì¸¡ì •
```

### ğŸ¯ ì„ íƒ ê°€ì´ë“œ

**Skill-Creatorë¥¼ ì„ íƒí•´ì•¼ í•  ë•Œ:**
- Claude í”Œë«í¼ì—ì„œ ë°˜ë³µì  ì‘ì—… ìë™í™”
- ë¬¸ì„œ ìƒì„±/í¸ì§‘ ê¸°ëŠ¥ í•„ìš”
- APIë¥¼ í†µí•œ ìŠ¤í‚¬ ê´€ë¦¬ í•„ìš”
- íŒ€ ì „ì²´ ìŠ¤í‚¬ ë°°í¬ í•„ìš”

**Agent-Skillsë¥¼ í™œìš©í•´ì•¼ í•  ë•Œ:**
- ì»¨í…ìŠ¤íŠ¸ ì„±ëŠ¥ ë¬¸ì œ í•´ê²°
- ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì„¤ê³„
- í”Œë«í¼ ë…ë¦½ì  ì†”ë£¨ì…˜ í•„ìš”
- ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì›ì¹™ í•™ìŠµ
- RAG/ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ìµœì í™”

**ë‘ ê°€ì§€ ëª¨ë‘ í™œìš©:**
```
Agent-Skillsë¡œ ì›ì¹™ í•™ìŠµ
      â†“
Skill-Creatorë¡œ êµ¬í˜„
      â†“
í”„ë¡œë•ì…˜ ë°°í¬
```

---

## Agent-Skills 7ê°€ì§€ í•µì‹¬ ìŠ¤í‚¬

GitHub ë¦¬í¬ì§€í† ë¦¬ì˜ `skills/` ë””ë ‰í† ë¦¬ êµ¬ì¡°:

```
skills/
â”œâ”€â”€ context-fundamentals/      # ì»¨í…ìŠ¤íŠ¸ ê¸°ì´ˆ
â”œâ”€â”€ context-degradation/       # ì„±ëŠ¥ ì €í•˜
â”œâ”€â”€ context-optimization/      # ìµœì í™”
â”œâ”€â”€ multi-agent-patterns/      # ë©€í‹° ì—ì´ì „íŠ¸
â”œâ”€â”€ memory-systems/           # ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ
â”œâ”€â”€ tool-design/              # ë„êµ¬ ì„¤ê³„
â””â”€â”€ evaluation/               # í‰ê°€
```

### 1ï¸âƒ£ context-fundamentals (ì»¨í…ìŠ¤íŠ¸ ê¸°ì´ˆ)

**ì–¸ì œ ì‚¬ìš©**: ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜ ì„¤ê³„, ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ ì‹¤íŒ¨ ë””ë²„ê¹…, ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© ìµœì í™”

#### í•µì‹¬ ê°œë…

**ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ìš”ì†Œ:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Complete Context Window            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. System Prompts                           â”‚
â”‚    - ì—ì´ì „íŠ¸ ì •ì²´ì„± ì •ì˜                      â”‚
â”‚    - í–‰ë™ ì œì•½ì‚¬í•­                            â”‚
â”‚    - ì„¸ì…˜ ì‹œì‘ì‹œ 1íšŒ ë¡œë“œ                     â”‚
â”‚                                             â”‚
â”‚ 2. Tool Definitions                         â”‚
â”‚    - ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ì„¤ëª…                     â”‚
â”‚    - íŒŒë¼ë¯¸í„° ìŠ¤í‚¤ë§ˆ                          â”‚
â”‚    - ì‹¤í–‰ ê°€ì´ë“œë¼ì¸                          â”‚
â”‚                                             â”‚
â”‚ 3. Retrieved Documents                      â”‚
â”‚    - RAGë¡œ ê°€ì ¸ì˜¨ ë¬¸ì„œ                        â”‚
â”‚    - ê´€ë ¨ì„± ì ìˆ˜ ê¸°ë°˜ í•„í„°ë§                   â”‚
â”‚    - ë™ì  ë¡œë“œ/ì–¸ë¡œë“œ                         â”‚
â”‚                                             â”‚
â”‚ 4. Message History                          â”‚
â”‚    - ëŒ€í™” ì´ë ¥                               â”‚
â”‚    - ìµœê·¼ Nê°œ í„´ ìœ ì§€                         â”‚
â”‚    - ìš”ì•½ or ì••ì¶•                            â”‚
â”‚                                             â”‚
â”‚ 5. Tool Outputs                             â”‚
â”‚    - ë„êµ¬ ì‹¤í–‰ ê²°ê³¼                           â”‚
â”‚    - ì—ëŸ¬ ë©”ì‹œì§€                             â”‚
â”‚    - ìƒíƒœ ì •ë³´                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**System Prompt ìµœì í™”:**

```markdown
âŒ ì˜ëª»ëœ ì˜ˆ (ë„ˆë¬´ ë†’ì€ ìˆ˜ì¤€):
"Be helpful and answer questions accurately."

âŒ ì˜ëª»ëœ ì˜ˆ (ë„ˆë¬´ ë‚®ì€ ìˆ˜ì¤€):
"When user asks about weather:
  1. Call get_weather(city, date)
  2. If error code 404, say 'City not found'
  3. If error code 500, retry 3 times
  ... (50ì¤„ì˜ ì„¸ë¶€ ë¡œì§)"

âœ… ì˜¬ë°”ë¥¸ ì˜ˆ (ì ì ˆí•œ ìˆ˜ì¤€):
"""
You are a data analysis assistant for financial teams.

Core Capabilities:
- Extract insights from financial reports
- Generate visualizations using provided tools
- Explain calculations in business terms

Guidelines:
- Always verify data sources before analysis
- Use clear, jargon-free language for non-technical users
- When uncertain, acknowledge limitations

Tools Available:
- spreadsheet_analyzer: For Excel/CSV processing
- chart_generator: For data visualization
- calculation_engine: For financial metrics
"""
```

**Progressive Disclosure ì›ì¹™:**

```python
# ë‚˜ìœ ë°©ë²•: ëª¨ë“  ë¬¸ì„œë¥¼ í•œë²ˆì— ë¡œë“œ
def bad_approach(query, all_documents):
    context = "\n".join(all_documents)  # ìˆ˜ë°± MB
    return f"Query: {query}\n\nDocuments:\n{context}"

# ì¢‹ì€ ë°©ë²•: í•„ìš”í•œ ì •ë³´ë§Œ ì ì§„ì  ë¡œë“œ
def good_approach(query, all_documents):
    # 1ë‹¨ê³„: ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ 5ê°œë§Œ ë¡œë“œ
    relevant = semantic_search(query, all_documents, top_k=5)
    
    # 2ë‹¨ê³„: ìš”ì•½ë³¸ ë¨¼ì € ì œê³µ
    summaries = [summarize(doc, max_tokens=200) for doc in relevant]
    
    # 3ë‹¨ê³„: í•„ìš”ì‹œ ì „ì²´ ë‚´ìš© ë¡œë“œ
    # (ì—ì´ì „íŠ¸ê°€ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­í•  ë•Œë§Œ)
    
    return {
        "summaries": summaries,
        "full_docs_available": [doc.id for doc in relevant]
    }
```

#### ì‹¤ì „ ì ìš©

```python
class ContextFundamentalsManager:
    """ì»¨í…ìŠ¤íŠ¸ ê¸°ì´ˆ ì›ì¹™ì„ ì ìš©í•œ ê´€ë¦¬ì"""
    
    def __init__(self, max_context_tokens=128000):
        self.max_tokens = max_context_tokens
        self.components = {
            'system_prompt': {'tokens': 0, 'content': '', 'priority': 10},
            'tool_definitions': {'tokens': 0, 'content': '', 'priority': 9},
            'recent_messages': {'tokens': 0, 'content': [], 'priority': 8},
            'retrieved_docs': {'tokens': 0, 'content': [], 'priority': 6},
            'tool_outputs': {'tokens': 0, 'content': [], 'priority': 7},
            'background_info': {'tokens': 0, 'content': '', 'priority': 3}
        }
    
    def build_context(self):
        """ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_components = sorted(
            self.components.items(),
            key=lambda x: x[1]['priority'],
            reverse=True
        )
        
        context_parts = []
        used_tokens = 0
        
        for name, component in sorted_components:
            if used_tokens + component['tokens'] > self.max_tokens:
                # ë‚¨ì€ ê³µê°„ì— ë§ê²Œ ì˜ë¼ë‚´ê¸°
                remaining = self.max_tokens - used_tokens
                truncated = self._truncate(component['content'], remaining)
                context_parts.append(truncated)
                break
            
            context_parts.append(component['content'])
            used_tokens += component['tokens']
        
        return '\n\n'.join(context_parts)
```

---

### 2ï¸âƒ£ context-degradation (ì»¨í…ìŠ¤íŠ¸ ì„±ëŠ¥ ì €í•˜)

**ì–¸ì œ ì‚¬ìš©**: ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ ì‹¤íŒ¨ ì§„ë‹¨, ì„±ëŠ¥ ì €í•˜ ë¬¸ì œ í•´ê²°

#### í•µì‹¬ ì‹¤íŒ¨ íŒ¨í„´

**1. Lost in the Middle (ì¤‘ê°„ ì •ë³´ ì†ì‹¤)**

```
ì‹¤í—˜ ê²°ê³¼ (Stanford, 2023):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 20ê°œ ë¬¸ì„œ, ì´ 4,000 í† í°               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ìœ„ì¹˜  1: ì •í™•ë„ 70-75% âœ…              â”‚
â”‚ ìœ„ì¹˜ 10: ì •í™•ë„ 55-60% âš ï¸              â”‚
â”‚ ìœ„ì¹˜ 20: ì •í™•ë„ 70-75% âœ…              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Uì í˜•íƒœ ì£¼ì˜ ê³¡ì„ :
   ë†’ìŒ â–²               
        â”‚ â–ˆ            â–ˆ
        â”‚ â–ˆ            â–ˆ
        â”‚ â–ˆ      â–„     â–ˆ
   ë‚®ìŒ â”‚ â–ˆ  â–„â–„â–„â–ˆ â–ˆâ–„â–„  â–ˆ
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶
          ì‹œì‘  ì¤‘ê°„   ë
```

**í•´ê²°ì±…:**

```python
def place_critical_info_strategically(documents, critical_info):
    """í•µì‹¬ ì •ë³´ë¥¼ ì‹œì‘ì´ë‚˜ ëì— ë°°ì¹˜"""
    return [
        critical_info,          # ì‹œì‘ì— ë°°ì¹˜
        *documents[:-1],        # ì¤‘ê°„ ë¬¸ì„œë“¤
        documents[-1],          # ë§ˆì§€ë§‰ ë¬¸ì„œ (ë‘ ë²ˆì§¸ë¡œ ì¤‘ìš”í•œ ì •ë³´)
    ]
```

**2. Context Poisoning (ì»¨í…ìŠ¤íŠ¸ ì˜¤ì—¼)**

```python
# ì˜ëª»ëœ ì •ë³´ê°€ ì»¨í…ìŠ¤íŠ¸ì— ë“¤ì–´ê°€ë©´ ì—°ì‡„ ì‹¤íŒ¨ ë°œìƒ

def prevent_context_poisoning():
    """
    ì»¨í…ìŠ¤íŠ¸ ì˜¤ì—¼ ë°©ì§€ ì „ëµ:
    1. í™˜ê° ê°ì§€ ë° ì œê±°
    2. ì‚¬ì‹¤ ê²€ì¦
    3. ì¶œì²˜ ì¶”ì 
    """
    
    # ì˜ˆì‹œ: ì´ì „ ì‘ë‹µ ê²€ì¦
    if response_contains_hallucination(prev_response):
        # ì˜¤ì—¼ëœ ì‘ë‹µì„ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì œê±°
        context.remove(prev_response)
        # ìƒˆë¡œìš´ ê²€ì¦ëœ ì •ë³´ë¡œ êµì²´
        context.add(verified_information)
```

**3. Context Confusion (ì»¨í…ìŠ¤íŠ¸ í˜¼ë€)**

```
ë¬¸ì œ: ìœ ì‚¬í•˜ì§€ë§Œ ê´€ë ¨ ì—†ëŠ” ì •ë³´ê°€ ë§ì„ ë•Œ

ì˜ˆì‹œ:
Query: "Python ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°"

Contextì— í¬í•¨ëœ ë¬¸ì„œ:
- Python ê¸°ì´ˆ ë¬¸ë²• âš ï¸ (ìœ ì‚¬í•˜ì§€ë§Œ ê´€ë ¨ì„± ë‚®ìŒ)
- JavaScript async/await âš ï¸ (ë¹„ë™ê¸°ì§€ë§Œ ë‹¤ë¥¸ ì–¸ì–´)
- Python asyncio ë¬¸ì„œ âœ… (ì •í™•íˆ ê´€ë ¨ë¨)

â†’ ëª¨ë¸ì´ í˜¼ë€ìŠ¤ëŸ¬ì›Œ Pythonê³¼ JavaScript ì„ì–´ì„œ ì„¤ëª…
```

**í•´ê²°ì±…: ì˜ë¯¸ë¡ ì  ê±°ë¦¬ ê³„ì‚°**

```python
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def filter_confusing_context(query, documents, threshold=0.5):
    """ìœ ì‚¬ë„ ê¸°ë°˜ í˜¼ë€ ë°©ì§€ í•„í„°ë§"""
    
    # ì„ë² ë”© ìƒì„±
    query_embedding = get_embedding(query)
    doc_embeddings = [get_embedding(doc) for doc in documents]
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarities = cosine_similarity(
        [query_embedding],
        doc_embeddings
    )[0]
    
    # ë„ˆë¬´ ìœ ì‚¬í•œ (ì¤‘ë³µ) ë¬¸ì„œ ì œê±°
    # ë„ˆë¬´ ë‹¤ë¥¸ (ê´€ë ¨ ì—†ëŠ”) ë¬¸ì„œ ì œê±°
    filtered = []
    for doc, sim in zip(documents, similarities):
        if threshold < sim < 0.95:  # ì ì ˆí•œ ë²”ìœ„ë§Œ
            filtered.append(doc)
    
    return filtered
```

**4. Attention Scarcity (ì£¼ì˜ë ¥ ë¶€ì¡±)**

```
ì»¨í…ìŠ¤íŠ¸ í¬ê¸°ì™€ ì£¼ì˜ë ¥ ê´€ê³„:

1K í† í°:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% ì£¼ì˜ë ¥
10K í† í°:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          60% ì£¼ì˜ë ¥
100K í† í°: â–ˆâ–ˆâ–ˆâ–ˆ                  20% ì£¼ì˜ë ¥
1M í† í°:   â–ˆ                      5% ì£¼ì˜ë ¥

â†’ í° ì»¨í…ìŠ¤íŠ¸ = ê° í† í°ì— ëŒ€í•œ ì£¼ì˜ë ¥ ê°ì†Œ
```

---

### 3ï¸âƒ£ context-optimization (ì»¨í…ìŠ¤íŠ¸ ìµœì í™”)

**ì–¸ì œ ì‚¬ìš©**: í† í° ì‚¬ìš©ëŸ‰ ê°ì†Œ, ì‘ë‹µ ì†ë„ ê°œì„ , ë¹„ìš© ì ˆê°

#### 3ê°€ì§€ í•µì‹¬ ê¸°ë²•

**1. Compaction (ì••ì¶•)**

```python
class ContextCompactor:
    """ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ê¸°ë²•"""
    
    def extractive_summarization(self, text, ratio=0.3):
        """í•µì‹¬ ë¬¸ì¥ë§Œ ì¶”ì¶œ"""
        sentences = sent_tokenize(text)
        
        # TF-IDFë¡œ ì¤‘ìš”ë„ ê³„ì‚°
        vectorizer = TfidfVectorizer()
        tfidf = vectorizer.fit_transform(sentences)
        
        # ìƒìœ„ N% ë¬¸ì¥ ì„ íƒ
        scores = tfidf.sum(axis=1).A1
        top_indices = np.argsort(scores)[-int(len(sentences) * ratio):]
        
        # ì›ë˜ ìˆœì„œëŒ€ë¡œ ì¬ì¡°ë¦½
        top_indices.sort()
        return ' '.join([sentences[i] for i in top_indices])
    
    def abstractive_summarization(self, text, max_length=200):
        """LLM ê¸°ë°˜ ìš”ì•½"""
        prompt = f"""
        ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {max_length} ë‹¨ì–´ ì´ë‚´ë¡œ ìš”ì•½:
        
        {text}
        
        í•µì‹¬ë§Œ í¬í•¨í•˜ê³  ì˜ˆì‹œëŠ” ì œì™¸í•˜ì„¸ìš”.
        """
        return call_llm(prompt)
    
    def chunk_and_compress(self, long_document, chunk_size=2000):
        """ì²­í¬ ë‹¨ìœ„ë¡œ ì••ì¶• í›„ ë³‘í•©"""
        chunks = split_into_chunks(long_document, chunk_size)
        summaries = [self.extractive_summarization(c) for c in chunks]
        
        # ìš”ì•½ë“¤ì„ ë‹¤ì‹œ ìš”ì•½ (ê³„ì¸µì  ì••ì¶•)
        combined = '\n'.join(summaries)
        if len(combined.split()) > 1000:
            return self.abstractive_summarization(combined)
        return combined
```

**2. Masking (ë§ˆìŠ¤í‚¹)**

```python
def mask_irrelevant_information(context, query):
    """
    ì¿¼ë¦¬ì™€ ê´€ë ¨ ì—†ëŠ” ì •ë³´ë¥¼ ë§ˆìŠ¤í‚¹
    (ì œê±° ëŒ€ì‹  [REDACTED]ë¡œ í‘œì‹œ)
    """
    
    doc_sections = split_into_sections(context)
    query_keywords = extract_keywords(query)
    
    masked_sections = []
    for section in doc_sections:
        section_keywords = extract_keywords(section)
        overlap = len(query_keywords & section_keywords)
        
        if overlap == 0:
            # ê´€ë ¨ì„± ì—†ìŒ â†’ ë§ˆìŠ¤í‚¹
            masked_sections.append("[SECTION OMITTED - Not relevant to query]")
        else:
            # ê´€ë ¨ì„± ìˆìŒ â†’ ìœ ì§€
            masked_sections.append(section)
    
    return '\n\n'.join(masked_sections)
```

**3. Caching (ìºì‹±)**

```python
import hashlib
import time

class SmartContextCache:
    """Anthropicì˜ Prompt Caching í™œìš©"""
    
    def __init__(self):
        self.cache = {}
        self.cache_ttl = 300  # 5ë¶„
    
    def get_cache_key(self, content):
        """ì»¨í…ì¸  í•´ì‹œë¡œ ìºì‹œ í‚¤ ìƒì„±"""
        return hashlib.sha256(content.encode()).hexdigest()
    
    def should_cache(self, content):
        """ìºì‹± ê°€ì¹˜ íŒë‹¨"""
        # 1. ì¶©ë¶„íˆ í° ì»¨í…ì¸  (1024 í† í° ì´ìƒ)
        if count_tokens(content) < 1024:
            return False
        
        # 2. ì¬ì‚¬ìš© ê°€ëŠ¥ì„± ë†’ì€ ì»¨í…ì¸ 
        # (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸, ë„êµ¬ ì •ì˜, ìì£¼ ì“°ëŠ” ë¬¸ì„œ ë“±)
        if is_static_content(content):
            return True
        
        return False
    
    def build_cached_context(self, static_parts, dynamic_parts):
        """
        Anthropic Prompt Caching í™œìš©:
        - static_parts: ìºì‹œë¨ (90% í• ì¸)
        - dynamic_parts: ë§¤ë²ˆ ì²˜ë¦¬
        """
        
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": static_parts['system_prompt'],
                        "cache_control": {"type": "ephemeral"}
                    },
                    {
                        "type": "text",
                        "text": static_parts['tool_definitions'],
                        "cache_control": {"type": "ephemeral"}
                    },
                    {
                        "type": "text",
                        "text": dynamic_parts['user_query']
                    }
                ]
            }
        ]
        
        return messages
```

#### ì‹¤ì „ ìµœì í™” íŒŒì´í”„ë¼ì¸

```python
class OptimizationPipeline:
    """ì „ì²´ ìµœì í™” íŒŒì´í”„ë¼ì¸"""
    
    def optimize(self, query, documents, max_tokens=10000):
        """
        ë‹¨ê³„ë³„ ìµœì í™”:
        1. ê´€ë ¨ì„± í•„í„°ë§
        2. ì»¨í…ìŠ¤íŠ¸ ì••ì¶•
        3. í† í° ì œí•œ ì ìš©
        4. ìºì‹± ì „ëµ
        """
        
        # Phase 1: ê´€ë ¨ì„± í•„í„° (50% ê°ì†Œ)
        relevant_docs = self.relevance_filter(query, documents)
        print(f"Phase 1: {len(documents)} â†’ {len(relevant_docs)} docs")
        
        # Phase 2: ì••ì¶• (70% ê°ì†Œ)
        compressed_docs = [
            self.compactor.extractive_summarization(doc, ratio=0.3)
            for doc in relevant_docs
        ]
        print(f"Phase 2: Compressed to 30% of original size")
        
        # Phase 3: í† í° ì œí•œ
        final_context = self.fit_to_budget(
            query, compressed_docs, max_tokens
        )
        
        # Phase 4: ìºì‹±
        if self.should_cache(final_context):
            cache_key = self.cache.get_cache_key(final_context)
            self.cache.set(cache_key, final_context)
        
        return final_context
```

---

### 4ï¸âƒ£ multi-agent-patterns (ë©€í‹° ì—ì´ì „íŠ¸ íŒ¨í„´)

**ì–¸ì œ ì‚¬ìš©**: ë³µì¡í•œ ì‘ì—…ì„ ì—¬ëŸ¬ ì—ì´ì „íŠ¸ë¡œ ë¶„ì‚°, ì „ë¬¸í™”ëœ ì—ì´ì „íŠ¸ í™œìš©

#### 3ê°€ì§€ í•µì‹¬ íŒ¨í„´

**1. Orchestrator Pattern (ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°)**

```python
class OrchestratorAgent:
    """ì¤‘ì•™ í†µì œ ì—ì´ì „íŠ¸ê°€ ì‘ì—… ë¶„ë°°"""
    
    def __init__(self):
        self.specialist_agents = {
            'researcher': ResearchAgent(),
            'coder': CodingAgent(),
            'reviewer': ReviewAgent(),
            'writer': WritingAgent()
        }
    
    def execute_task(self, task):
        """ì‘ì—…ì„ ë¶„ì„í•˜ê³  ì ì ˆí•œ ì—ì´ì „íŠ¸ì— í• ë‹¹"""
        
        # ì‘ì—… ë¶„ì„
        task_plan = self.analyze_task(task)
        
        results = []
        for subtask in task_plan['subtasks']:
            # ì ì ˆí•œ ì „ë¬¸ ì—ì´ì „íŠ¸ ì„ íƒ
            agent_type = subtask['agent_type']
            agent = self.specialist_agents[agent_type]
            
            # ìµœì†Œí•œì˜ ì»¨í…ìŠ¤íŠ¸ë§Œ ì „ë‹¬
            minimal_context = self.extract_relevant_context(
                subtask, 
                previous_results=results
            )
            
            # ì‹¤í–‰
            result = agent.execute(subtask, minimal_context)
            results.append(result)
        
        # ìµœì¢… ê²°ê³¼ í†µí•©
        return self.merge_results(results)
    
    def extract_relevant_context(self, subtask, previous_results):
        """ê° ì—ì´ì „íŠ¸ì— í•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ"""
        # ì´ì „ ê²°ê³¼ì—ì„œ ê´€ë ¨ ë¶€ë¶„ë§Œ ì„ íƒ
        relevant = [r for r in previous_results 
                   if self.is_relevant(r, subtask)]
        
        # í† í° ì˜ˆì‚° ë‚´ë¡œ ì••ì¶•
        return self.compress_to_budget(relevant, max_tokens=5000)
```

**ì»¨í…ìŠ¤íŠ¸ ê²©ë¦¬ íš¨ê³¼:**

```
ë‹¨ì¼ ì—ì´ì „íŠ¸ ë°©ì‹:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Context: 100K tokens               â”‚
â”‚ - Task description                 â”‚
â”‚ - All documents                    â”‚
â”‚ - All tool outputs                 â”‚
â”‚ - Full conversation history        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†’ ì„±ëŠ¥ ì €í•˜, ë†’ì€ ë¹„ìš©

ë©€í‹° ì—ì´ì „íŠ¸ (Orchestrator) ë°©ì‹:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Researcher   â”‚ â”‚ Coder        â”‚ â”‚ Reviewer     â”‚
â”‚ 10K context  â”‚ â”‚ 15K context  â”‚ â”‚ 8K context   â”‚
â”‚ (ì—°êµ¬ ë¬¸ì„œë§Œ) â”‚ â”‚ (ì½”ë“œ ê´€ë ¨ë§Œ) â”‚ â”‚ (ë¦¬ë·° ëŒ€ìƒë§Œ) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†’ ê° ì—ì´ì „íŠ¸ ìµœì  ì„±ëŠ¥, ë¹„ìš© ì ˆê°
```

**2. Swarm Pattern (ìŠ¤ì›œ)**

```python
class SwarmCoordinator:
    """ë…ë¦½ì  ì—ì´ì „íŠ¸ë“¤ì´ í˜‘ë ¥"""
    
    def __init__(self, num_agents=5):
        self.agents = [GeneralistAgent(id=i) for i in range(num_agents)]
        self.shared_memory = SharedMemory()
    
    def solve_problem(self, problem):
        """
        ê° ì—ì´ì „íŠ¸ê°€ ë…ë¦½ì ìœ¼ë¡œ ì‘ì—…í•˜ë˜
        ê³µìœ  ë©”ëª¨ë¦¬ë¥¼ í†µí•´ í˜‘ë ¥
        """
        
        # ê° ì—ì´ì „íŠ¸ì— ê°™ì€ ë¬¸ì œ í• ë‹¹
        futures = []
        for agent in self.agents:
            # ê³µìœ  ë©”ëª¨ë¦¬ ì ‘ê·¼ ê¶Œí•œ ë¶€ì—¬
            agent.connect_to_shared_memory(self.shared_memory)
            
            # ë¹„ë™ê¸° ì‹¤í–‰
            future = agent.solve_async(problem)
            futures.append(future)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        solutions = [f.result() for f in futures]
        
        # ìµœì„ ì˜ í•´ê²°ì±… ì„ íƒ
        return self.select_best_solution(solutions)
    
    def select_best_solution(self, solutions):
        """ì—¬ëŸ¬ í•´ê²°ì±… ì¤‘ ìµœì„  ì„ íƒ"""
        # íˆ¬í‘œ, ì ìˆ˜, ê²€ì¦ ë“± ë‹¤ì–‘í•œ ë°©ë²• ê°€ëŠ¥
        scores = [self.evaluate(s) for s in solutions]
        best_idx = max(range(len(scores)), key=lambda i: scores[i])
        return solutions[best_idx]
```

**ì»¨í…ìŠ¤íŠ¸ ê³µìœ  ì „ëµ:**

```python
class SharedMemory:
    """ì—ì´ì „íŠ¸ ê°„ íš¨ìœ¨ì  ì •ë³´ ê³µìœ """
    
    def __init__(self):
        self.storage = {
            'facts': {},           # ê²€ì¦ëœ ì‚¬ì‹¤
            'hypotheses': {},      # ê°€ì„¤ë“¤
            'observations': [],    # ê´€ì°° ê²°ê³¼
        }
    
    def add_fact(self, fact, confidence=1.0):
        """ê²€ì¦ëœ ì •ë³´ë§Œ ê³µìœ """
        if confidence > 0.8:
            self.storage['facts'][fact.id] = fact
    
    def get_relevant_context(self, query, max_tokens=5000):
        """ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ì •ë³´ë§Œ ì¶”ì¶œ"""
        # ì „ì²´ ê³µìœ  ë©”ëª¨ë¦¬ê°€ ì•„ë‹Œ ê´€ë ¨ ë¶€ë¶„ë§Œ
        relevant_facts = self.search_facts(query)
        return self.compress_to_budget(relevant_facts, max_tokens)
```

**3. Hierarchical Pattern (ê³„ì¸µì )**

```python
class HierarchicalSystem:
    """ê³„ì¸µì  ì—ì´ì „íŠ¸ êµ¬ì¡°"""
    
    def __init__(self):
        # ìƒìœ„ ë ˆë²¨: ì „ëµì  ì˜ì‚¬ê²°ì •
        self.strategic_agent = StrategicAgent(
            context_budget=50000  # í° ì»¨í…ìŠ¤íŠ¸
        )
        
        # ì¤‘ê°„ ë ˆë²¨: ì „ìˆ ì  ê³„íš
        self.tactical_agents = [
            TacticalAgent(context_budget=20000)
            for _ in range(3)
        ]
        
        # í•˜ìœ„ ë ˆë²¨: ì‹¤í–‰
        self.operational_agents = [
            OperationalAgent(context_budget=5000)
            for _ in range(10)
        ]
    
    def execute(self, mission):
        """ê³„ì¸µì  ì‹¤í–‰"""
        # 1. ì „ëµ ìˆ˜ë¦½ (ìƒìœ„)
        strategy = self.strategic_agent.plan(mission)
        
        # 2. ì „ìˆ  ê³„íš (ì¤‘ê°„)
        tactical_plans = []
        for tactical_agent in self.tactical_agents:
            # ì „ëµì˜ ì¼ë¶€ë§Œ ì „ë‹¬
            relevant_strategy = extract_relevant_part(
                strategy, 
                tactical_agent.specialty
            )
            plan = tactical_agent.create_plan(relevant_strategy)
            tactical_plans.append(plan)
        
        # 3. ì‹¤í–‰ (í•˜ìœ„)
        results = []
        for plan in tactical_plans:
            for task in plan.tasks:
                agent = self.assign_operational_agent(task)
                # ìµœì†Œ ì»¨í…ìŠ¤íŠ¸ë§Œ ì „ë‹¬
                result = agent.execute(task, minimal_context=True)
                results.append(result)
        
        return results
```

**ê³„ì¸µë³„ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬:**

```
Level 1 (Strategic): 50K tokens
â”œâ”€ Mission overview
â”œâ”€ Domain knowledge
â””â”€ High-level constraints

Level 2 (Tactical): 20K tokens
â”œâ”€ Assigned portion of strategy
â”œâ”€ Specialized domain info
â””â”€ Team coordination

Level 3 (Operational): 5K tokens
â”œâ”€ Specific task description
â”œâ”€ Immediate context only
â””â”€ Tool documentation
```

---

### 5ï¸âƒ£ memory-systems (ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ)

**ì–¸ì œ ì‚¬ìš©**: ì„¸ì…˜ ê°„ ì—°ì†ì„± ìœ ì§€, ì§€ì‹ ì¶•ì , ì¥ê¸° ì—ì´ì „íŠ¸ ìš´ì˜

#### ë©”ëª¨ë¦¬ ê³„ì¸µ êµ¬ì¡°

```python
class HierarchicalMemory:
    """
    3ê³„ì¸µ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ:
    - Working Memory (ì‘ì—… ë©”ëª¨ë¦¬)
    - Short-term Memory (ë‹¨ê¸° ë©”ëª¨ë¦¬)  
    - Long-term Memory (ì¥ê¸° ë©”ëª¨ë¦¬)
    """
    
    def __init__(self):
        # Layer 1: Working Memory (ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ë‚´)
        self.working = {
            'capacity': 8000,      # í† í°
            'latency': 0,          # ì¦‰ì‹œ ì ‘ê·¼
            'persistence': 'volatile',  # ì„¸ì…˜ ì¢…ë£Œì‹œ ì‚­ì œ
            'content': []
        }
        
        # Layer 2: Short-term Memory (ì„¸ì…˜ ë‚´ ìœ ì§€)
        self.short_term = {
            'capacity': 50000,
            'latency': 'low',      # ë¹ ë¥¸ ê²€ìƒ‰
            'persistence': 'session',
            'content': []
        }
        
        # Layer 3: Long-term Memory (ì˜êµ¬ ë³´ê´€)
        self.long_term = {
            'capacity': float('inf'),
            'latency': 'medium',   # ê²€ìƒ‰ í•„ìš”
            'persistence': 'permanent',
            'storage': VectorDatabase()
        }
    
    def add(self, information, importance='medium'):
        """ì¤‘ìš”ë„ì— ë”°ë¼ ì ì ˆí•œ ê³„ì¸µì— ì €ì¥"""
        
        if importance == 'critical':
            # ì‘ì—… ë©”ëª¨ë¦¬ì— ì¶”ê°€ (ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥)
            self.working['content'].append(information)
            self._enforce_working_limit()
        
        elif importance == 'high':
            # ë‹¨ê¸° ë©”ëª¨ë¦¬ì— ì¶”ê°€
            self.short_term['content'].append(information)
        
        else:
            # ì¥ê¸° ë©”ëª¨ë¦¬ì— ì €ì¥
            self.long_term['storage'].add(information)
    
    def _enforce_working_limit(self):
        """ì‘ì—… ë©”ëª¨ë¦¬ ìš©ëŸ‰ ì œí•œ ì ìš©"""
        while self.get_working_tokens() > self.working['capacity']:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª©ì„ ë‹¨ê¸°ë¡œ ì´ë™
            moved = self.working['content'].pop(0)
            self.short_term['content'].append(moved)
    
    def retrieve(self, query, max_items=5):
        """ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë©”ëª¨ë¦¬ ê²€ìƒ‰"""
        results = []
        
        # 1. ì‘ì—… ë©”ëª¨ë¦¬ì—ì„œ ê²€ìƒ‰ (ê°€ì¥ ë¹ ë¦„)
        for item in self.working['content']:
            if self.is_relevant(query, item):
                results.append(('working', item))
        
        # 2. ë‹¨ê¸° ë©”ëª¨ë¦¬ì—ì„œ ê²€ìƒ‰
        if len(results) < max_items:
            for item in self.short_term['content'][-20:]:  # ìµœê·¼ 20ê°œë§Œ
                if self.is_relevant(query, item):
                    results.append(('short_term', item))
        
        # 3. ì¥ê¸° ë©”ëª¨ë¦¬ì—ì„œ ê²€ìƒ‰
        if len(results) < max_items:
            long_term_results = self.long_term['storage'].search(
                query, 
                limit=max_items - len(results)
            )
            results.extend([('long_term', r) for r in long_term_results])
        
        return results[:max_items]
```

#### Vector Storeì˜ í•œê³„ì™€ í•´ê²°ì±…

**ë¬¸ì œ:**

```
Vector Store (RAG)ì˜ ì œì•½:
âŒ ê´€ê³„ ì •ë³´ ì†ì‹¤
   "ê³ ê° Xê°€ ì œí’ˆ Yë¥¼ ë‚ ì§œ Zì— êµ¬ë§¤" â†’ ë²¡í„°í™” ì‹œ ê´€ê³„ êµ¬ì¡° ì‚¬ë¼ì§

âŒ ë³µì¡í•œ ì¿¼ë¦¬ ë¶ˆê°€
   "ì œí’ˆ Yë¥¼ êµ¬ë§¤í•œ ê³ ê°ë“¤ì´ ë˜ êµ¬ë§¤í•œ ì œí’ˆì€?"
   â†’ Vector Storeë¡œ ë‹µë³€ ë¶ˆê°€ëŠ¥

âŒ ì‹œê°„ ì •ë³´ ë¶€ì¬
   ìµœì‹ ì„± íŒë‹¨ ì–´ë ¤ì›€
```

**í•´ê²°: Knowledge Graph + Temporal ì •ë³´**

```python
class TemporalKnowledgeGraph:
    """ì‹œê°„ ì •ë³´ë¥¼ í¬í•¨í•œ ì§€ì‹ ê·¸ë˜í”„"""
    
    def __init__(self):
        self.graph = nx.DiGraph()
        self.temporal_index = {}
    
    def add_fact(self, subject, predicate, object, timestamp):
        """
        ì‚¬ì‹¤ì„ ê·¸ë˜í”„ì— ì¶”ê°€
        ì˜ˆ: (Customer_X, purchased, Product_Y, 2024-01-15)
        """
        # ë…¸ë“œ ì¶”ê°€
        self.graph.add_node(subject, type='customer')
        self.graph.add_node(object, type='product')
        
        # ì—£ì§€ ì¶”ê°€ (ê´€ê³„)
        self.graph.add_edge(
            subject, object,
            relation=predicate,
            timestamp=timestamp
        )
        
        # ì‹œê°„ ì¸ë±ìŠ¤
        if timestamp not in self.temporal_index:
            self.temporal_index[timestamp] = []
        self.temporal_index[timestamp].append((subject, predicate, object))
    
    def query_relationship(self, query):
        """ê´€ê³„ ê¸°ë°˜ ì¿¼ë¦¬"""
        # "Product Yë¥¼ êµ¬ë§¤í•œ ê³ ê°ë“¤"
        customers = [
            node for node in self.graph.nodes()
            if self.graph.has_edge(node, 'Product_Y')
        ]
        
        # "ê·¸ ê³ ê°ë“¤ì´ êµ¬ë§¤í•œ ë‹¤ë¥¸ ì œí’ˆë“¤"
        other_products = set()
        for customer in customers:
            products = self.graph.successors(customer)
            other_products.update(products)
        
        other_products.discard('Product_Y')  # ì›ë˜ ì œí’ˆ ì œì™¸
        return list(other_products)
    
    def query_temporal(self, start_date, end_date):
        """ì‹œê°„ ë²”ìœ„ ì¿¼ë¦¬"""
        facts = []
        for timestamp, fact_list in self.temporal_index.items():
            if start_date <= timestamp <= end_date:
                facts.extend(fact_list)
        return facts
```

#### Hybrid ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ

```python
class HybridMemorySystem:
    """Vector Store + Knowledge Graph ê²°í•©"""
    
    def __init__(self):
        self.vector_store = ChromaDB()
        self.knowledge_graph = TemporalKnowledgeGraph()
        self.hierarchical_memory = HierarchicalMemory()
    
    def store_information(self, info):
        """ì •ë³´ë¥¼ ì ì ˆí•œ ì €ì¥ì†Œì— ë¶„ì‚°"""
        
        # 1. í…ìŠ¤íŠ¸ ì •ë³´ â†’ Vector Store (ì˜ë¯¸ë¡ ì  ê²€ìƒ‰)
        if info['type'] == 'document':
            self.vector_store.add(
                text=info['content'],
                metadata=info['metadata']
            )
        
        # 2. ê´€ê³„ ì •ë³´ â†’ Knowledge Graph (êµ¬ì¡°ì  ì¿¼ë¦¬)
        elif info['type'] == 'relationship':
            self.knowledge_graph.add_fact(
                subject=info['subject'],
                predicate=info['predicate'],
                object=info['object'],
                timestamp=info['timestamp']
            )
        
        # 3. ì¦‰ì‹œ ì‚¬ìš© ì •ë³´ â†’ Hierarchical Memory
        if info.get('immediate_use', False):
            self.hierarchical_memory.add(
                info['content'],
                importance='critical'
            )
    
    def retrieve(self, query, query_type='hybrid'):
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        results = {}
        
        # Semantic search
        if query_type in ['semantic', 'hybrid']:
            results['semantic'] = self.vector_store.search(query, top_k=5)
        
        # Structural query
        if query_type in ['structural', 'hybrid']:
            results['structural'] = self.knowledge_graph.query_relationship(query)
        
        # Working memory
        if query_type in ['immediate', 'hybrid']:
            results['immediate'] = self.hierarchical_memory.retrieve(query)
        
        # ê²°ê³¼ í†µí•© ë° ìˆœìœ„ ì§€ì •
        return self.merge_and_rank(results)
```

---

### 6ï¸âƒ£ tool-design (ë„êµ¬ ì„¤ê³„)

**ì–¸ì œ ì‚¬ìš©**: ì—ì´ì „íŠ¸ê°€ ì‚¬ìš©í•  ë„êµ¬ ì„¤ê³„, ë„êµ¬ ì„¤ëª… ìµœì í™”

#### ë„êµ¬ ì„¤ê³„ ì›ì¹™

**1. ëª…í™•í•œ ë„êµ¬ ì„¤ëª…**

```python
# âŒ ë‚˜ìœ ì˜ˆ: ëª¨í˜¸í•œ ì„¤ëª…
{
    "name": "process_data",
    "description": "Processes data",
    "parameters": {
        "data": {"type": "string"}
    }
}

# âœ… ì¢‹ì€ ì˜ˆ: êµ¬ì²´ì ì¸ ì„¤ëª…
{
    "name": "analyze_customer_sentiment",
    "description": """
    Analyzes customer feedback to determine sentiment (positive/negative/neutral).
    
    Use when:
    - You have customer reviews, support tickets, or survey responses
    - You need to understand overall customer satisfaction
    
    Don't use when:
    - Data is not customer-related
    - You need numerical analysis (use data_analyzer instead)
    
    Returns: Sentiment score (-1.0 to 1.0) and classification
    """,
    "parameters": {
        "feedback_text": {
            "type": "string",
            "description": "Customer feedback text to analyze. Max 5000 characters."
        },
        "language": {
            "type": "string",
            "enum": ["en", "ko", "ja"],
            "description": "Language of the feedback. Defaults to 'en'."
        }
    }
}
```

**2. ë„êµ¬ ê°œìˆ˜ ìµœì í™” (RAG for Tools)**

```python
class ToolRAG:
    """ë„êµ¬ ì„¤ëª…ì— RAG ì ìš©"""
    
    def __init__(self, all_tools):
        self.all_tools = all_tools
        self.tool_embeddings = self._embed_tools()
    
    def _embed_tools(self):
        """ê° ë„êµ¬ì˜ ì„¤ëª…ì„ ì„ë² ë”©"""
        embeddings = {}
        for tool in self.all_tools:
            description = f"{tool['name']}: {tool['description']}"
            embeddings[tool['name']] = get_embedding(description)
        return embeddings
    
    def select_relevant_tools(self, task_description, max_tools=5):
        """ì‘ì—…ê³¼ ê´€ë ¨ëœ ë„êµ¬ë§Œ ì„ íƒ"""
        
        task_embedding = get_embedding(task_description)
        
        # ê° ë„êµ¬ì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = {}
        for tool_name, tool_embedding in self.tool_embeddings.items():
            sim = cosine_similarity([task_embedding], [tool_embedding])[0][0]
            similarities[tool_name] = sim
        
        # ìƒìœ„ Nê°œ ì„ íƒ
        top_tools = sorted(
            similarities.items(),
            key=lambda x: x[1],
            reverse=True
        )[:max_tools]
        
        return [
            next(t for t in self.all_tools if t['name'] == name)
            for name, _ in top_tools
        ]
```

**ì„±ëŠ¥ í–¥ìƒ (ì—°êµ¬ ê²°ê³¼):**

```
ì „ì²´ ë„êµ¬ ì œê³µ:
â”œâ”€ 100ê°œ ë„êµ¬ â†’ ì»¨í…ìŠ¤íŠ¸ 20K í† í°
â”œâ”€ ë„êµ¬ ì„ íƒ ì •í™•ë„: 35%
â””â”€ ì‘ë‹µ ì‹œê°„: 8ì´ˆ

RAG ê¸°ë°˜ ë„êµ¬ ì„ íƒ:
â”œâ”€ 5ê°œ ê´€ë ¨ ë„êµ¬ë§Œ â†’ ì»¨í…ìŠ¤íŠ¸ 1K í† í°
â”œâ”€ ë„êµ¬ ì„ íƒ ì •í™•ë„: 85% (â†‘ 3ë°°)
â””â”€ ì‘ë‹µ ì‹œê°„: 2ì´ˆ (â†“ 75%)
```

---

### 7ï¸âƒ£ evaluation (í‰ê°€)

**ì–¸ì œ ì‚¬ìš©**: ì—ì´ì „íŠ¸ ì„±ëŠ¥ ì¸¡ì •, A/B í…ŒìŠ¤íŠ¸, í’ˆì§ˆ ë³´ì¥

#### LLM-as-a-Judge ê¸°ë²•

```python
class LLMJudge:
    """LLMì„ í‰ê°€ìë¡œ í™œìš©"""
    
    def direct_scoring(self, response, criteria):
        """ì§ì ‘ ì ìˆ˜ ë§¤ê¸°ê¸°"""
        judge_prompt = f"""
        ë‹¤ìŒ ì‘ë‹µì„ í‰ê°€í•˜ì„¸ìš”.
        
        ì‘ë‹µ: {response}
        
        í‰ê°€ ê¸°ì¤€:
        1. ì •í™•ì„± (0-10ì )
        2. ì™„ì„±ë„ (0-10ì )
        3. ëª…í™•ì„± (0-10ì )
        
        ê° ê¸°ì¤€ì— ëŒ€í•´ ì ìˆ˜ì™€ ì´ìœ ë¥¼ ì œì‹œí•˜ì„¸ìš”.
        """
        
        return call_llm(judge_prompt)
    
    def pairwise_comparison(self, response_a, response_b, question):
        """ìŒ ë¹„êµ"""
        judge_prompt = f"""
        ì§ˆë¬¸: {question}
        
        ì‘ë‹µ A: {response_a}
        ì‘ë‹µ B: {response_b}
        
        ì–´ë–¤ ì‘ë‹µì´ ë” ë‚˜ì€ê°€ìš”? A, B, ë˜ëŠ” Tieë¥¼ ì„ íƒí•˜ê³  ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.
        """
        
        return call_llm(judge_prompt)
    
    def rubric_based_evaluation(self, response, rubric):
        """ë£¨ë¸Œë¦­ ê¸°ë°˜ í‰ê°€"""
        judge_prompt = f"""
        ë£¨ë¸Œë¦­:
        {rubric}
        
        ì‘ë‹µ:
        {response}
        
        ë£¨ë¸Œë¦­ì˜ ê° í•­ëª©ì— ëŒ€í•´ í‰ê°€í•˜ì„¸ìš”.
        """
        
        return call_llm(judge_prompt)
```

#### ì»¨í…ìŠ¤íŠ¸ íš¨ìœ¨ì„± ë©”íŠ¸ë¦­

```python
class ContextEfficiencyMetrics:
    """ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© íš¨ìœ¨ì„± ì¸¡ì •"""
    
    def __init__(self):
        self.monitor = TokenMonitor()
    
    def calculate_metrics(self, context, response, task_success):
        """ì¢…í•© ë©”íŠ¸ë¦­ ê³„ì‚°"""
        
        # 1. í† í° íš¨ìœ¨ì„±
        context_tokens = self.monitor.count_tokens(context)
        response_tokens = self.monitor.count_tokens(response)
        total_tokens = context_tokens + response_tokens
        
        # 2. ì •ë³´ ë°€ë„
        # ì‹¤ì œ ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸ vs ì „ì²´ ì»¨í…ìŠ¤íŠ¸
        used_context = self.extract_used_context(context, response)
        used_tokens = self.monitor.count_tokens(used_context)
        information_density = used_tokens / context_tokens
        
        # 3. ì„±ê³µë¥  ëŒ€ë¹„ ë¹„ìš©
        cost_per_success = total_tokens if task_success else float('inf')
        
        return {
            'total_tokens': total_tokens,
            'context_tokens': context_tokens,
            'response_tokens': response_tokens,
            'information_density': information_density,
            'cost_per_success': cost_per_success,
            'success': task_success
        }
    
    def extract_used_context(self, context, response):
        """ì‘ë‹µì—ì„œ ì‹¤ì œ ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸ ë¶€ë¶„ ì¶”ì¶œ"""
        # ì‘ë‹µì— ë‚˜íƒ€ë‚œ ì»¨í…ìŠ¤íŠ¸ì˜ êµ¬ì ˆë“¤ì„ ì°¾ìŒ
        context_sentences = sent_tokenize(context)
        response_lower = response.lower()
        
        used = []
        for sent in context_sentences:
            # ë¬¸ì¥ì˜ ì£¼ìš” ë‹¨ì–´ë“¤ì´ ì‘ë‹µì— ë‚˜íƒ€ë‚˜ëŠ”ì§€ í™•ì¸
            keywords = extract_keywords(sent)
            if any(kw.lower() in response_lower for kw in keywords):
                used.append(sent)
        
        return ' '.join(used)
```